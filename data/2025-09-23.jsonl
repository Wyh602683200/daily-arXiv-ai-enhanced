{"id": "2509.16288", "pdf": "https://arxiv.org/pdf/2509.16288", "abs": "https://arxiv.org/abs/2509.16288", "authors": ["Shanookha Ali", "Nitha Niralda P C"], "title": "Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity", "categories": ["cs.AI", "05C22, 05C90, 68R10 05C22, 05C90, 68R10 05C22, 05C90, 68R10"], "comment": null, "summary": "Coronary heart disease (CHD) arises from complex interactions among\nuncontrollable factors, controllable lifestyle factors, and clinical\nindicators, where relationships are often uncertain. Fuzzy subgraph\nconnectivity (FSC) provides a systematic tool to capture such imprecision by\nquantifying the strength of association between vertices and subgraphs in fuzzy\ngraphs. In this work, a fuzzy CHD graph is constructed with vertices for\nuncontrollable, controllable, and indicator components, and edges weighted by\nfuzzy memberships. Using FSC, we evaluate connectivity to identify strongest\ndiagnostic routes, dominant risk factors, and critical bridges. Results show\nthat FSC highlights influential pathways, bounds connectivity between weakest\nand strongest correlations, and reveals critical edges whose removal reduces\npredictive strength. Thus, FSC offers an interpretable and robust framework for\nmodeling uncertainty in CHD risk prediction and supporting clinical\ndecision-making."}
{"id": "2509.16298", "pdf": "https://arxiv.org/pdf/2509.16298", "abs": "https://arxiv.org/abs/2509.16298", "authors": ["Raquel Fernandez-Peralta", "Juan Vicente Riera"], "title": "A global view of diverse construction methods of fuzzy implication functions rooted on F-chains", "categories": ["cs.AI"], "comment": null, "summary": "Fuzzy implication functions are one of the most important operators used in\nthe fuzzy logic framework. While their flexible definition allows for diverse\nfamilies with distinct properties, this variety needs a deeper theoretical\nunderstanding of their structural relationships. In this work, we focus on the\nstudy of construction methods, which employ different techniques to generate\nnew fuzzy implication functions from existing ones. Particularly, we generalize\nthe $F$-chain-based construction, recently introduced by Mesiar et al. to\nextend a method for constructing aggregation functions to the context of fuzzy\nimplication functions. Our generalization employs collections of fuzzy\nimplication functions rather than single ones, and uses two different\nincreasing functions instead of a unique $F$-chain. We analyze property\npreservation under this construction and establish sufficient conditions.\nFurthermore, we demonstrate that our generalized $F$-chain-based construction\nis a unifying framework for several existing methods. In particular, we show\nthat various construction techniques, such as contraposition, aggregation, and\ngeneralized vertical/horizontal threshold methods, can be reformulated within\nour approach. This reveals structural similarities between seemingly distinct\nconstruction strategies and provides a cohesive perspective on fuzzy\nimplication construction methods."}
{"id": "2509.16299", "pdf": "https://arxiv.org/pdf/2509.16299", "abs": "https://arxiv.org/abs/2509.16299", "authors": ["Raquel Fernandez-Peralta", "Andrea Mesiarová-Zemánková"], "title": "On the Non-Uniqueness of Representation of $(U,N)$-Implications", "categories": ["cs.AI"], "comment": null, "summary": "Fuzzy implication functions constitute fundamental operators in fuzzy logic\nsystems, extending classical conditionals to manage uncertainty in logical\ninference. Among the extensive families of these operators, generalizations of\nthe classical material implication have received considerable theoretical\nattention, particularly $(S,N)$-implications constructed from t-conorms and\nfuzzy negations, and their further generalizations to $(U,N)$-implications\nusing disjunctive uninorms. Prior work has established characterization\ntheorems for these families under the assumption that the fuzzy negation $N$ is\ncontinuous, ensuring uniqueness of representation. In this paper, we disprove\nthis last fact for $(U,N)$-implications and we show that they do not\nnecessarily possess a unique representation, even if the fuzzy negation is\ncontinuous. Further, we provide a comprehensive study of uniqueness conditions\nfor both uninorms with continuous and non-continuous underlying functions. Our\nresults offer important theoretical insights into the structural properties of\nthese operators."}
{"id": "2509.16330", "pdf": "https://arxiv.org/pdf/2509.16330", "abs": "https://arxiv.org/abs/2509.16330", "authors": ["Minxing Zhang", "Yi Yang", "Roy Xie", "Bhuwan Dhingra", "Shuyan Zhou", "Jian Pei"], "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a new paradigm that\nextends LLMs' capabilities beyond text generation to dynamic interaction with\nexternal environments. By integrating reasoning with perception, memory, and\ntool use, agents are increasingly deployed in diverse domains like web\nnavigation and household robotics. A critical challenge, however, lies in\nensuring agent generalizability - the ability to maintain consistent\nperformance across varied instructions, tasks, environments, and domains,\nespecially those beyond agents' fine-tuning data. Despite growing interest, the\nconcept of generalizability in LLM-based agents remains underdefined, and\nsystematic approaches to measure and improve it are lacking. In this survey, we\nprovide the first comprehensive review of generalizability in LLM-based agents.\nWe begin by emphasizing agent generalizability's importance by appealing to\nstakeholders and clarifying the boundaries of agent generalizability by\nsituating it within a hierarchical domain-task ontology. We then review\ndatasets, evaluation dimensions, and metrics, highlighting their limitations.\nNext, we categorize methods for improving generalizability into three groups:\nmethods for the backbone LLM, for agent components, and for their interactions.\nMoreover, we introduce the distinction between generalizable frameworks and\ngeneralizable agents and outline how generalizable frameworks can be translated\ninto agent-level generalizability. Finally, we identify critical challenges and\nfuture directions, including developing standardized frameworks, variance- and\ncost-based metrics, and approaches that integrate methodological innovations\nwith architecture-level designs. By synthesizing progress and highlighting\nopportunities, this survey aims to establish a foundation for principled\nresearch on building LLM-based agents that generalize reliably across diverse\napplications."}
{"id": "2509.16332", "pdf": "https://arxiv.org/pdf/2509.16332", "abs": "https://arxiv.org/abs/2509.16332", "authors": ["Stephen Fitz", "Peter Romero", "Steven Basart", "Sipeng Chen", "Jose Hernandez-Orallo"], "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models increasingly mediate high-stakes interactions,\nintensifying research on their capabilities and safety. While recent work has\nshown that LLMs exhibit consistent and measurable synthetic personality traits,\nlittle is known about how modulating these traits affects model behavior. We\naddress this gap by investigating how psychometric personality control grounded\nin the Big Five framework influences AI behavior in the context of capability\nand safety benchmarks. Our experiments reveal striking effects: for example,\nreducing conscientiousness leads to significant drops in safety-relevant\nmetrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well\nas reduction in general capabilities as measured by MMLU. These findings\nhighlight personality shaping as a powerful and underexplored axis of model\ncontrol that interacts with both safety and general competence. We discuss the\nimplications for safety evaluation, alignment strategies, steering model\nbehavior after deployment, and risks associated with possible exploitation of\nthese findings. Our findings motivate a new line of research on\npersonality-sensitive safety evaluations and dynamic behavioral control in\nLLMs."}
{"id": "2509.16348", "pdf": "https://arxiv.org/pdf/2509.16348", "abs": "https://arxiv.org/abs/2509.16348", "authors": ["Minxiao Wang", "Saurabh Kataria", "Juntong Ni", "Timothy G. Buchman", "Jocelyn Grunwell", "Mark Mai", "Wei Jin", "Matthew Clark", "Stephanie Brown", "Michael Fundora", "Puneet Sharma", "Tony Pan", "Sam Khan", "Timothy Ruchti", "Naveen Muthu", "Kevin Maher", "Sivasubramanium V Bhavani", "Xiao Hu"], "title": "A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)", "categories": ["cs.AI"], "comment": null, "summary": "We present UNIPHY+, a unified physiological foundation model (physioFM)\nframework designed to enable continuous human health and diseases monitoring\nacross care settings using ubiquitously obtainable physiological data. We\npropose novel strategies for incorporating contextual information during\npretraining, fine-tuning, and lightweight model personalization via multi-modal\nlearning, feature fusion-tuning, and knowledge distillation. We advocate\ntesting UNIPHY+ with a broad set of use cases from intensive care to ambulatory\nmonitoring in order to demonstrate that UNIPHY+ can empower generalizable,\nscalable, and personalized physiological AI to support both clinical\ndecision-making and long-term health monitoring."}
{"id": "2509.16372", "pdf": "https://arxiv.org/pdf/2509.16372", "abs": "https://arxiv.org/abs/2509.16372", "authors": ["Balu Bhasuran", "Mattia Prosperi", "Karim Hanna", "John Petrilli", "Caretia JeLayne Washington", "Zhe He"], "title": "Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation", "categories": ["cs.AI"], "comment": null, "summary": "This study evaluates causal reasoning in large language models (LLMs) using\n99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of\nCausation: association, intervention, and counterfactual reasoning. We examined\ncommon laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and\npaired them with relevant causal factors including age, gender, obesity, and\nsmoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with\nresponses evaluated by four medically trained human experts. GPT-o1\ndemonstrated stronger discriminative performance (AUROC overall = 0.80 +/-\n0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores\nacross association (0.75 vs 0.72), intervention (0.84 vs 0.70), and\ncounterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and\nspecificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings\nshowing similar trends. Both models performed best on intervention questions\nand worst on counterfactuals, particularly in altered outcome scenarios. These\nfindings suggest GPT-o1 provides more consistent causal reasoning, but\nrefinement is required before adoption in high-stakes clinical applications."}
{"id": "2509.16399", "pdf": "https://arxiv.org/pdf/2509.16399", "abs": "https://arxiv.org/abs/2509.16399", "authors": ["Guojun Xiong", "Milind Tambe"], "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping", "categories": ["cs.AI"], "comment": "28pages, 19figures", "summary": "In social impact optimization, AI decision systems often rely on solvers that\noptimize well-calibrated mathematical objectives. However, these solvers cannot\ndirectly accommodate evolving human preferences, typically expressed in natural\nlanguage rather than formal constraints. Recent approaches address this by\nusing large language models (LLMs) to generate new reward functions from\npreference descriptions. While flexible, they risk sacrificing the system's\ncore utility guarantees. In this paper, we propose \\texttt{VORTEX}, a\nlanguage-guided reward shaping framework that preserves established\noptimization goals while adaptively incorporating human feedback. By\nformalizing the problem as multi-objective optimization, we use LLMs to\niteratively generate shaping rewards based on verbal reinforcement and\ntext-gradient prompt updates. This allows stakeholders to steer decision\nbehavior via natural language without modifying solvers or specifying trade-off\nweights. We provide theoretical guarantees that \\texttt{VORTEX} converges to\nPareto-optimal trade-offs between utility and preference satisfaction.\nEmpirical results in real-world allocation tasks demonstrate that\n\\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage\ngoals while maintaining high task performance. This work introduces a practical\nand theoretically grounded paradigm for human-AI collaborative optimization\nguided by natural language."}
{"id": "2509.16431", "pdf": "https://arxiv.org/pdf/2509.16431", "abs": "https://arxiv.org/abs/2509.16431", "authors": ["Mohammad Iqbal Rasul Seeam", "Victor S. Sheng"], "title": "Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing", "categories": ["cs.AI"], "comment": "7 pages, 3 figures, no .bbl file needed because bibliography already\n  in main.tex file", "summary": "In the manufacturing industry, it is very important to keep machines and\nprocesses running smoothly and without unexpected problems. One of the most\ncommon tools used to check if everything is working properly is called\nStatistical Process Control (SPC). Traditional SPC methods work by checking\nwhether recent measurements are within acceptable limits. However, they only\nreact after a problem has already occurred. This can lead to wasted materials,\nmachine downtime, and increased costs. In this paper, we present a smarter way\nto use SPC. Instead of just reacting to issues after they happen, our system\ncan predict future problems before they occur. We use a machine learning tool\ncalled Facebook Prophet, which is designed to work with time-series data (data\nthat changes over time). Prophet looks at past data and forecasts what the next\nvalue will be. Then, we use SPC rules to decide if the predicted value is in a\nSafe zone (no problem), a Warning zone (needs attention), or a Critical zone\n(may require shutting down the process). We applied this system to real data\nfrom a semiconductor manufacturing company. One of the challenges with this\ndata is that the measurements are not taken at regular time intervals. This\nmakes it harder to predict future values accurately. Despite this, our model\nwas able to make strong predictions and correctly classify the risk level of\nfuture measurements. The main benefit of our system is that it gives engineers\nand technicians a chance to act early - before something goes wrong. This helps\nreduce unexpected failures and improves the overall stability and reliability\nof the production process. By combining machine learning with traditional SPC,\nwe make quality control more proactive, accurate, and useful for modern\nindustry."}
{"id": "2509.16444", "pdf": "https://arxiv.org/pdf/2509.16444", "abs": "https://arxiv.org/abs/2509.16444", "authors": ["Chenhan Lyu", "Yutong Song", "Pengfei Zhang", "Amir M. Rahmani"], "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots", "categories": ["cs.AI"], "comment": null, "summary": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications."}
{"id": "2509.16456", "pdf": "https://arxiv.org/pdf/2509.16456", "abs": "https://arxiv.org/abs/2509.16456", "authors": ["Jiahao Yu", "Zelei Cheng", "Xian Wu", "Xinyu Xing"], "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning", "categories": ["cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Large language models (LLMs) are increasingly used in various domains,\nshowing impressive potential on different tasks. Recently, reasoning LLMs have\nbeen proposed to improve the \\textit{reasoning} or \\textit{thinking}\ncapabilities of LLMs to solve complex problems. Despite the promising results\nof reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs\nstill remains a significant challenge. While existing optimization methods have\nadvanced the LLM reasoning capabilities, they often treat reasoning\ntrajectories as a whole, without considering the underlying critical steps\nwithin the trajectory. In this paper, we introduce \\textbf{G}uided\n\\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that\ndives into the reasoning process to enable more effective improvements. GPO\nfirst identifies the `critical step' within a reasoning trajectory - a point\nthat the model must carefully proceed to succeed at the problem. We locate the\ncritical step by estimating the advantage function. GPO then resets the policy\nto the critical step, samples the new rollout and prioritizes the learning\nprocess on those rollouts. This focus allows the model to learn more\neffectively from pivotal moments within the reasoning process to improve the\nreasoning performance. We demonstrate that GPO is a general strategy that can\nbe integrated with various optimization methods to improve reasoning\nperformance. Besides theoretical analysis, our experiments across challenging\nreasoning benchmarks show that GPO can consistently and significantly enhance\nthe performance of existing optimization methods, showcasing its effectiveness\nand generalizability in improving LLM reasoning by concentrating on pivotal\nmoments within the generation process."}
{"id": "2509.16547", "pdf": "https://arxiv.org/pdf/2509.16547", "abs": "https://arxiv.org/abs/2509.16547", "authors": ["Adrian Wurm"], "title": "Checking extracted rules in Neural Networks", "categories": ["cs.AI", "cs.LG"], "comment": "7 pages, one figure", "summary": "In this paper we investigate formal verification of extracted rules for\nNeural Networks under a complexity theoretic point of view. A rule is a global\nproperty or a pattern concerning a large portion of the input space of a\nnetwork. These rules are algorithmically extracted from networks in an effort\nto better understand their inner way of working. Here, three problems will be\nin the focus: Does a given set of rules apply to a given network? Is a given\nset of rules consistent or do the rules contradict themselves? Is a given set\nof rules exhaustive in the sense that for every input the output is determined?\nFinding algorithms that extract such rules out of networks has been\ninvestigated over the last 30 years, however, to the author's current\nknowledge, no attempt in verification was made until now. A lot of attempts of\nextracting rules use heuristics involving randomness and over-approximation, so\nit might be beneficial to know whether knowledge obtained in that way can\nactually be trusted.\n  We investigate the above questions for neural networks with ReLU-activation\nas well as for Boolean networks, each for several types of rules. We\ndemonstrate how these problems can be reduced to each other and show that most\nof them are co-NP-complete."}
{"id": "2509.16561", "pdf": "https://arxiv.org/pdf/2509.16561", "abs": "https://arxiv.org/abs/2509.16561", "authors": ["Yue Xin", "Chen Shen", "Shaotian Yan", "Xiaosong Yuan", "Yaoming Wang", "Xiaofeng Zhang", "Chenxi Huang", "Jieping Ye"], "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "accpeted by EMNLP 2025", "summary": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of\nlarge language models (LLMs) to a large margin. However, the mechanism\nunderlying such improvements remains unexplored. In this paper, we present\n\\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed\n\\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd}\nM\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a\nmathematically rigorous evaluation metric for quantifying component-level\ncontributions in few-shot CoT reasoning. Concretely, we leverage the Shapley\nvalue for mathematical expression attribution and develop an efficient\nstratified sampling algorithm that significantly reduces the computational\ncomplexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality\n\\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance\nanalysis. Comprehensive validation across popular LLM models and diverse\nmathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder\nframework exhibits a robust monotonic correlation with model performance, not\nonly providing theoretical explanations for the empirical success of existing\nfew-shot CoT but also establishing mathematically rigorous principles for\nprompt construction optimization. Furthermore, we verify the reliability of the\nexplanation, based on which we unify the insights of previous work."}
{"id": "2509.16578", "pdf": "https://arxiv.org/pdf/2509.16578", "abs": "https://arxiv.org/abs/2509.16578", "authors": ["Wenyao Li", "Ran Zhang", "Pengyang Wang", "Yuanchun Zhou", "Pengfei Wang"], "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Human mobility forecasting is important for applications such as\ntransportation planning, urban management, and personalized recommendations.\nHowever, existing methods often fail to generalize to unseen users or locations\nand struggle to capture dynamic intent due to limited labeled data and the\ncomplexity of mobility patterns. We propose ZHMF, a framework for zero-shot\nhuman mobility forecasting that combines a semantic enhanced retrieval and\nreflection mechanism with a hierarchical language model based reasoning system.\nThe task is reformulated as a natural language question answering paradigm.\nLeveraging LLMs semantic understanding of user histories and context, our\napproach handles previously unseen prediction scenarios. We further introduce a\nhierarchical reflection mechanism for iterative reasoning and refinement by\ndecomposing forecasting into an activity level planner and a location level\nselector, enabling collaborative modeling of long term user intentions and\nshort term contextual preferences. Experiments on standard human mobility\ndatasets show that our approach outperforms existing models. Ablation studies\nreveal the contribution of each module, and case studies illustrate how the\nmethod captures user intentions and adapts to diverse contextual scenarios."}
{"id": "2509.16590", "pdf": "https://arxiv.org/pdf/2509.16590", "abs": "https://arxiv.org/abs/2509.16590", "authors": ["Manuel Borroto", "Katie Gallagher", "Antonio Ielo", "Irfan Kareem", "Francesco Ricca", "Alessandra Russo"], "title": "Question Answering with LLMs and Learning from Answer Sets", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "Under consideration for TPLP journal", "summary": "Large Language Models (LLMs) excel at understanding natural language but\nstruggle with explicit commonsense reasoning. A recent trend of research\nsuggests that the combination of LLM with robust symbolic reasoning systems can\novercome this problem on story-based question answering tasks. In this setting,\nexisting approaches typically depend on human expertise to manually craft the\nsymbolic component. We argue, however, that this component can also be\nautomatically learned from examples. In this work, we introduce LLM2LAS, a\nhybrid system that effectively combines the natural language understanding\ncapabilities of LLMs, the rule induction power of the Learning from Answer Sets\n(LAS) system ILASP, and the formal reasoning strengths of Answer Set\nProgramming (ASP). LLMs are used to extract semantic structures from text,\nwhich ILASP then transforms into interpretable logic rules. These rules allow\nan ASP solver to perform precise and consistent reasoning, enabling correct\nanswers to previously unseen questions. Empirical results outline the strengths\nand weaknesses of our automatic approach for learning and reasoning in a\nstory-based question answering benchmark."}
{"id": "2509.16648", "pdf": "https://arxiv.org/pdf/2509.16648", "abs": "https://arxiv.org/abs/2509.16648", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted in the Findings of EMNLP, 2025", "summary": "The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced."}
{"id": "2509.16656", "pdf": "https://arxiv.org/pdf/2509.16656", "abs": "https://arxiv.org/abs/2509.16656", "authors": ["Changyu Zeng", "Yifan Wang", "Zimu Wang", "Wei Wang", "Zhengni Yang", "Muyi Bao", "Jiming Xiao", "Ahn Nguyen", "Yutao Yue"], "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in 2D multimodal large language models (MLLMs) have\nsignificantly improved performance in vision-language tasks. However, extending\nthese capabilities to 3D environments remains a distinct challenge due to the\ncomplexity of spatial reasoning. Nevertheless, existing 3D benchmarks often\nlack fine-grained numerical reasoning task annotations, limiting MLLMs' ability\nto perform precise spatial measurements and complex numerical reasoning. To\naddress this gap, we introduce NUMINA, the first Natural Understanding\nbenchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities\nto enhance multimodal indoor perceptual understanding. NUMINA features\nmulti-scale annotations and various question-answer pairs, generated using\nNUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and\nrule-based self-verification. We evaluate the performance of various\nstate-of-the-art LLMs on NUMINA following the Chat-Scene framework,\ndemonstrating that current LLMs struggle with multimodal numerical reasoning,\nparticularly in performing precise computations such as distance and volume\nestimation, highlighting the need for further advancements in 3D models. The\ndataset and source codes can be obtained from\nhttps://github.com/fengshun124/NUMINA."}
{"id": "2509.16742", "pdf": "https://arxiv.org/pdf/2509.16742", "abs": "https://arxiv.org/abs/2509.16742", "authors": ["Mohammad Beigi", "Ying Shen", "Parshin Shojaee", "Qifan Wang", "Zichao Wang", "Chandan Reddy", "Ming Jin", "Lifu Huang"], "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories", "categories": ["cs.AI"], "comment": null, "summary": "Despite the remarkable capabilities of large language models, current\ntraining paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency\nof a model to agree with or reinforce user-provided information even when it's\nfactually incorrect. To address this challenge, we introduce \\textbf{SMART}\n(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes\nsycophancy as a \\textit{reasoning optimization problem} rather than an output\nalignment issue. SMART is a two-stage framework comprising: (1)\nUncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically\nadjusts model exploration based on state-level uncertainty to collect\nhigh-quality, diverse reasoning trajectories alongside both stepwise progress\nand final outcome rewards; and (2) progress-based reinforcement learning, which\nfine-tunes the model using the collected trajectories and reward signals to\nreinforce effective reasoning patterns. Through extensive experiments, we show\nthat SMART significantly reduces sycophantic behavior while preserving strong\nperformance on out-of-distribution inputs and maintaining general capabilities.\nThese results underscore the importance of optimizing internal reasoning\nmechanisms to build more truthful and aligned AI assistants."}
{"id": "2509.16810", "pdf": "https://arxiv.org/pdf/2509.16810", "abs": "https://arxiv.org/abs/2509.16810", "authors": ["Shen Chang", "Dennis Liu", "Renran Tian", "Kristen L. Swartzell", "Stacie L. Klingler", "Amy M. Nagle", "Nan Kong"], "title": "Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment", "categories": ["cs.AI"], "comment": null, "summary": "Consistent high-quality nursing care is essential for patient safety, yet\ncurrent nursing education depends on subjective, time-intensive instructor\nfeedback in training future nurses, which limits scalability and efficiency in\ntheir training, and thus hampers nursing competency when they enter the\nworkforce. In this paper, we introduce a video-language model (VLM) based\nframework to develop the AI capability of automated procedural assessment and\nfeedback for nursing skills training, with the potential of being integrated\ninto existing training programs. Mimicking human skill acquisition, the\nframework follows a curriculum-inspired progression, advancing from high-level\naction recognition, fine-grained subaction decomposition, and ultimately to\nprocedural reasoning. This design supports scalable evaluation by reducing\ninstructor workload while preserving assessment quality. The system provides\nthree core capabilities: 1) diagnosing errors by identifying missing or\nincorrect subactions in nursing skill instruction videos, 2) generating\nexplainable feedback by clarifying why a step is out of order or omitted, and\n3) enabling objective, consistent formative evaluation of procedures.\nValidation on synthesized videos demonstrates reliable error detection and\ntemporal localization, confirming its potential to handle real-world training\nvariability. By addressing workflow bottlenecks and supporting large-scale,\nstandardized evaluation, this work advances AI applications in nursing\neducation, contributing to stronger workforce development and ultimately safer\npatient care."}
{"id": "2509.16811", "pdf": "https://arxiv.org/pdf/2509.16811", "abs": "https://arxiv.org/abs/2509.16811", "authors": ["Zihan Ding", "Junlong Chen", "Per Ola Kristensson", "Junxiao Shen", "Xinyi Wang"], "title": "Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Creators struggle to edit long-form, narrative-rich videos not because of UI\ncomplexity, but due to the cognitive demands of searching, storyboarding, and\nsequencing hours of footage. Existing transcript- or embedding-based methods\nfall short for creative workflows, as models struggle to track characters,\ninfer motivations, and connect dispersed events. We present a prompt-driven,\nmodular editing system that helps creators restructure multi-hour content\nthrough free-form prompts rather than timelines. At its core is a semantic\nindexing pipeline that builds a global narrative via temporal segmentation,\nguided memory compression, and cross-granularity fusion, producing\ninterpretable traces of plot, dialogue, emotion, and context. Users receive\ncinematic edits while optionally refining transparent intermediate outputs.\nEvaluated on 400+ videos with expert ratings, QA, and preference studies, our\nsystem scales prompt-driven editing, preserves narrative coherence, and\nbalances automation with creator control."}
{"id": "2509.16839", "pdf": "https://arxiv.org/pdf/2509.16839", "abs": "https://arxiv.org/abs/2509.16839", "authors": ["Yu Yao", "Jiayi Dong", "Ju Li", "Yang Yang", "Yilun Du"], "title": "Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs", "categories": ["cs.AI"], "comment": "Equal contribution: Yu Yao and Jiayi Dong. Equal advising: Ju Li,\n  Yang Yang, and Yilun Du. Affiliations: Massachusetts Institute of Technology\n  (Yu Yao, Ju Li), University of California, Los Angeles (Jiayi Dong, Yang\n  Yang), Harvard University (Yilun Du)", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities not\nonly in language generation but also in advancing scientific discovery. A\ngrowing body of work has explored ways to improve their reasoning, from\nself-consistency and chain-of-thought to multi-agent debate. Inspired by the\ndynamics of scientific committees and the \"Society of Mind,\" we introduce\nRoundtable Policy, a complementary inference-time reasoning framework that\nperforms inference through the weighted consensus of multiple LLMs. Our\nfindings indicate that this approach significantly enhances reasoning in\ncomplex heterogeneous scientific tasks and improves scientific narratives in\nterms of creativity, rigor, and logical coherence, while reducing\nhallucinations that single models are prone to. Our approach emphasizes\nstructured and interpretable consensus rather than opaque convergence, while\nrequiring only black-box access and uniform procedures, making it broadly\napplicable to multi-LLM reasoning."}
{"id": "2509.16859", "pdf": "https://arxiv.org/pdf/2509.16859", "abs": "https://arxiv.org/abs/2509.16859", "authors": ["Fangfang Li", "Xiaojie Zhang"], "title": "The Principles of Human-like Conscious Machine", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Determining whether another system, biological or artificial, possesses\nphenomenal consciousness has long been a central challenge in consciousness\nstudies. This attribution problem has become especially pressing with the rise\nof large language models and other advanced AI systems, where debates about \"AI\nconsciousness\" implicitly rely on some criterion for deciding whether a given\nsystem is conscious. In this paper, we propose a substrate-independent,\nlogically rigorous, and counterfeit-resistant sufficiency criterion for\nphenomenal consciousness. We argue that any machine satisfying this criterion\nshould be regarded as conscious with at least the same level of confidence with\nwhich we attribute consciousness to other humans. Building on this criterion,\nwe develop a formal framework and specify a set of operational principles that\nguide the design of systems capable of meeting the sufficiency condition. We\nfurther argue that machines engineered according to this framework can, in\nprinciple, realize phenomenal consciousness. As an initial validation, we show\nthat humans themselves can be viewed as machines that satisfy this framework\nand its principles. If correct, this proposal carries significant implications\nfor philosophy, cognitive science, and artificial intelligence. It offers an\nexplanation for why certain qualia, such as the experience of red, are in\nprinciple irreducible to physical description, while simultaneously providing a\ngeneral reinterpretation of human information processing. Moreover, it suggests\na path toward a new paradigm of AI beyond current statistics-based approaches,\npotentially guiding the construction of genuinely human-like AI."}
{"id": "2509.16865", "pdf": "https://arxiv.org/pdf/2509.16865", "abs": "https://arxiv.org/abs/2509.16865", "authors": ["Xia Jiang", "Yaoxin Wu", "Minshuo Li", "Zhiguang Cao", "Yingqian Zhang"], "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers", "categories": ["cs.AI"], "comment": null, "summary": "Combinatorial optimization (CO) problems, central to decision-making\nscenarios like logistics and manufacturing, are traditionally solved using\nproblem-specific algorithms requiring significant domain expertise. While large\nlanguage models (LLMs) have shown promise in automating CO problem solving,\nexisting approaches rely on intermediate steps such as code generation or\nsolver invocation, limiting their generality and accessibility. This paper\nintroduces a novel framework that empowers LLMs to serve as end-to-end CO\nsolvers by directly mapping natural language problem descriptions to solutions.\nWe propose a two-stage training strategy: supervised fine-tuning (SFT) imparts\nLLMs with solution generation patterns from domain-specific solvers, while a\nfeasibility-and-optimality-aware reinforcement learning (FOARL) process\nexplicitly mitigates constraint violations and refines solution quality.\nEvaluation across seven NP-hard CO problems shows that our method achieves a\nhigh feasibility rate and reduces the average optimality gap to 1.03-8.20% by\ntuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),\nreasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our\nmethod establishes a unified language-based pipeline for CO without extensive\ncode execution or manual architectural adjustments for different problems,\noffering a general and language-driven alternative to traditional solver design\nwhile maintaining relative feasibility guarantees."}
{"id": "2509.16866", "pdf": "https://arxiv.org/pdf/2509.16866", "abs": "https://arxiv.org/abs/2509.16866", "authors": ["Mohammad Ramezanali", "Mo Vazifeh", "Paolo Santi"], "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce seqBench, a parametrized benchmark for probing sequential\nreasoning limits in Large Language Models (LLMs) through precise,\nmulti-dimensional control over several key complexity dimensions. seqBench\nallows systematic variation of (1) the logical depth, defined as the number of\nsequential actions required to solve the task; (2) the number of backtracking\nsteps along the optimal path, quantifying how often the agent must revisit\nprior states to satisfy deferred preconditions (e.g., retrieving a key after\nencountering a locked door); and (3) the noise ratio, defined as the ratio\nbetween supporting and distracting facts about the environment. Our evaluations\non state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses\nexponentially beyond a model-specific logical depth. Unlike existing\nbenchmarks, seqBench's fine-grained control facilitates targeted analyses of\nthese reasoning failures, illuminating universal scaling laws and statistical\nlimits, as detailed in this paper alongside its generation methodology and\nevaluation metrics. We find that even top-performing models systematically fail\non seqBench's structured reasoning tasks despite minimal search complexity,\nunderscoring key limitations in their commonsense reasoning capabilities.\nDesigned for future evolution to keep pace with advancing models, the seqBench\ndatasets are publicly released to spur deeper scientific inquiry into LLM\nreasoning, aiming to establish a clearer understanding of their true potential\nand current boundaries for robust real-world application."}
{"id": "2509.16891", "pdf": "https://arxiv.org/pdf/2509.16891", "abs": "https://arxiv.org/abs/2509.16891", "authors": ["Sha Li"], "title": "LLMs as Layout Designers: A Spatial Reasoning Perspective", "categories": ["cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated impressive reasoning and\nplanning abilities in textual domains and can effectively follow instructions\nfor complex tasks, their capacity for spatial understanding and reasoning\nremains limited. Such capabilities, however, are critical for applications like\ncontent-aware graphic layout design, which demands precise placement,\nalignment, and structural organization of multiple elements within constrained\nvisual spaces. To address this gap, we propose LaySPA, a reinforcement\nlearning-based framework that augments LLM agents with explicit spatial\nreasoning capabilities. LaySPA leverages hybrid reward signals that capture\ngeometric validity, structural fidelity, and visual quality, enabling agents to\nmodel inter-element relationships, navigate the canvas, and optimize spatial\narrangements. Through iterative self-exploration and adaptive policy\noptimization, LaySPA produces both interpretable reasoning traces and\nstructured layouts. Experimental results demonstrate that LaySPA generates\nstructurally sound and visually appealing layouts, outperforming larger\ngeneral-purpose LLMs and achieving results on par with state-of-the-art\nspecialized layout models."}
{"id": "2509.16924", "pdf": "https://arxiv.org/pdf/2509.16924", "abs": "https://arxiv.org/abs/2509.16924", "authors": ["Jia Li", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation", "categories": ["cs.AI", "cs.SD"], "comment": "Main paper (14 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously\nlocalize a sound source in unknown and complex 3D environments based on\naudio-visual signals. Existing methods often rely on static modality fusion\nstrategies and neglect the spatial cues embedded in stereo audio, leading to\nperformance degradation in cluttered or occluded scenes. To address these\nissues, we propose an end-to-end reinforcement learning-based AVN framework\nwith two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention\n\\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity\nbetween left and right audio channels to enhance directional sound perception;\nand (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion\nModule (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between\nvisual and auditory features based on audio cues, thereby improving robustness\nto environmental changes. Extensive experiments are conducted on two realistic\n3D scene datasets, Replica and Matterport3D, demonstrating that our method\nsignificantly outperforms existing approaches in terms of navigation success\nrate and path efficiency. Notably, our model achieves over 40\\% improvement\nunder audio-only conditions compared to the best-performing baselines. These\nresults highlight the importance of explicitly modeling spatial cues from\nstereo channels and performing deep multi-modal fusion for robust and efficient\naudio-visual navigation."}
{"id": "2509.16958", "pdf": "https://arxiv.org/pdf/2509.16958", "abs": "https://arxiv.org/abs/2509.16958", "authors": ["Remo Pareschi"], "title": "Quantum Abduction: A New Paradigm for Reasoning under Uncertainty", "categories": ["cs.AI", "I.2.0; I.2.1; I.2.3; I.2.8"], "comment": "23 pages, 8 figures, 3 tables; submitted to Sci, MDPI", "summary": "Abductive reasoning - the search for plausible explanations - has long been\ncentral to human inquiry, from forensics to medicine and scientific discovery.\nYet formal approaches in AI have largely reduced abduction to eliminative\nsearch: hypotheses are treated as mutually exclusive, evaluated against\nconsistency constraints or probability updates, and pruned until a single\n\"best\" explanation remains. This reductionist framing overlooks the way human\nreasoners sustain multiple explanatory lines in suspension, navigate\ncontradictions, and generate novel syntheses. This paper introduces quantum\nabduction, a non-classical paradigm that models hypotheses in superposition,\nallows them to interfere constructively or destructively, and collapses only\nwhen coherence with evidence is reached. Grounded in quantum cognition and\nimplemented with modern NLP embeddings and generative AI, the framework\nsupports dynamic synthesis rather than premature elimination. Case studies span\nhistorical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"),\nliterary demonstrations (\"Murder on the Orient Express\"), medical diagnosis,\nand scientific theory change. Across these domains, quantum abduction proves\nmore faithful to the constructive and multifaceted nature of human reasoning,\nwhile offering a pathway toward expressive and transparent AI reasoning\nsystems."}
{"id": "2509.17037", "pdf": "https://arxiv.org/pdf/2509.17037", "abs": "https://arxiv.org/abs/2509.17037", "authors": ["Yajing Yang", "Tony Deng", "Min-Yen Kan"], "title": "KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration", "categories": ["cs.AI"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "We propose KAHAN, a knowledge-augmented hierarchical framework that\nsystematically extracts insights from raw tabular data at entity, pairwise,\ngroup, and system levels. KAHAN uniquely leverages LLMs as domain experts to\ndrive the analysis. On DataTales financial reporting benchmark, KAHAN\noutperforms existing approaches by over 20% on narrative quality (GPT-4o),\nmaintains 98.2% factuality, and demonstrates practical utility in human\nevaluation. Our results reveal that knowledge quality drives model performance\nthrough distillation, hierarchical analysis benefits vary with market\ncomplexity, and the framework transfers effectively to healthcare domains. The\ndata and code are available at https://github.com/yajingyang/kahan."}
{"id": "2509.17062", "pdf": "https://arxiv.org/pdf/2509.17062", "abs": "https://arxiv.org/abs/2509.17062", "authors": ["Cristian Pérez-Corral", "Antonio Garrido", "Laura Sebastia"], "title": "From domain-landmark graph learning to problem-landmark graph generation", "categories": ["cs.AI"], "comment": null, "summary": "Landmarks have long played a pivotal role in automated planning, serving as\ncrucial elements for improving the planning algorithms. The main limitation of\nclassical landmark extraction methods is their sensitivity to specific planning\ntasks. This results in landmarks fully tailored to individual instances,\nthereby limiting their applicability across other instances of the same\nplanning domain. We propose a novel approach that learns landmark relationships\nfrom multiple planning tasks of a planning domain. This leads to the creation\nof a \\textit{probabilistic lifted ordering graph}, as a structure that captures\nweighted abstractions of relationships between parameterized landmarks.\nAlthough these orderings are not 100\\% true (they are probabilistic), they can\nstill be very useful in planning. Next, given a new planning task for that\ndomain, we instantiate the relationships from that graph to this particular\ninstance. This instantiation operates in two phases. First, it generates two\ngraphs: the former instantiating information from the initial state and the\nlatter from the goal state. Second, it combines these two graphs into one\nunified graph by searching equivalences to extract landmark orderings. We\nevaluate the precision and recallof the information found by our approach over\nwell-known planning domains."}
{"id": "2509.17066", "pdf": "https://arxiv.org/pdf/2509.17066", "abs": "https://arxiv.org/abs/2509.17066", "authors": ["Kunrong Li", "Kwan Hui Lim"], "title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking", "categories": ["cs.AI", "cs.IR"], "comment": "PRICAI 2025", "summary": "Next point-of-interest (POI) recommendation predicts a user's next\ndestination from historical movements. Traditional models require intensive\ntraining, while LLMs offer flexible and generalizable zero-shot solutions but\noften generate generic or geographically irrelevant results due to missing\ntrajectory and spatial context. To address these issues, we propose RALLM-POI,\na framework that couples LLMs with retrieval-augmented generation and\nself-rectification. We first propose a Historical Trajectory Retriever (HTR)\nthat retrieves relevant past trajectories to serve as contextual references,\nwhich are then reranked by a Geographical Distance Reranker (GDR) for\nprioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier\n(ALR) is designed to refine outputs through self-reflection. Without additional\ntraining, RALLM-POI achieves substantial accuracy gains across three real-world\nFoursquare datasets, outperforming both conventional and LLM-based baselines.\nCode is released at https://github.com/LKRcrocodile/RALLM-POI."}
{"id": "2509.17068", "pdf": "https://arxiv.org/pdf/2509.17068", "abs": "https://arxiv.org/abs/2509.17068", "authors": ["Chen Wang", "Sarah Erfani", "Tansu Alpcan", "Christopher Leckie"], "title": "Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection", "categories": ["cs.AI"], "comment": "15 pages, 5 figures", "summary": "Long-term trajectory anomaly detection is a challenging problem due to the\ndiversity and complex spatiotemporal dependencies in trajectory data. Existing\ntrajectory anomaly detection methods fail to simultaneously consider both the\nhigh-level intentions of agents as well as the low-level details of the agent's\nnavigation when analysing an agent's trajectories. This limits their ability to\ncapture the full diversity of normal trajectories. In this paper, we propose an\nunsupervised trajectory anomaly detection method named Intention-aware\nHierarchical Diffusion model (IHiD), which detects anomalies through both\nhigh-level intent evaluation and low-level sub-trajectory analysis. Our\napproach leverages Inverse Q Learning as the high-level model to assess whether\na selected subgoal aligns with an agent's intention based on predicted\nQ-values. Meanwhile, a diffusion model serves as the low-level model to\ngenerate sub-trajectories conditioned on subgoal information, with anomaly\ndetection based on reconstruction error. By integrating both models, IHiD\neffectively utilises subgoal transition knowledge and is designed to capture\nthe diverse distribution of normal trajectories. Our experiments show that the\nproposed method IHiD achieves up to 30.2% improvement in anomaly detection\nperformance in terms of F1 score over state-of-the-art baselines."}
{"id": "2509.17087", "pdf": "https://arxiv.org/pdf/2509.17087", "abs": "https://arxiv.org/abs/2509.17087", "authors": ["Nicholas Kruus", "Madhavendra Thakur", "Adam Khoja", "Leonhard Nagel", "Maximilian Nicholson", "Abeer Sharma", "Jason Hausenloy", "Alberto KoTafoya", "Aliya Mukhanova", "Alli Katila-Miikkulainen", "Harish Chandran", "Ivan Zhang", "Jessie Chen", "Joel Raj", "Jord Nguyen", "Lai Hsien Hao", "Neja Jayasundara", "Soham Sen", "Sophie Zhang", "Ashley Dora Kokui Tamaklo", "Bhavya Thakur", "Henry Close", "Janghee Lee", "Nina Sefton", "Raghavendra Thakur", "Shiv Munagala", "Yeeun Kim"], "title": "Governing Automated Strategic Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Military and economic strategic competitiveness between nation-states will\nincreasingly be defined by the capability and cost of their frontier artificial\nintelligence models. Among the first areas of geopolitical advantage granted by\nsuch systems will be in automating military intelligence. Much discussion has\nbeen devoted to AI systems enabling new military modalities, such as lethal\nautonomous weapons, or making strategic decisions. However, the ability of a\ncountry of \"CIA analysts in a data-center\" to synthesize diverse data at scale,\nand its implications, have been underexplored. Multimodal foundation models\nappear on track to automate strategic analysis previously done by humans. They\nwill be able to fuse today's abundant satellite imagery, phone-location traces,\nsocial media records, and written documents into a single queryable system. We\nconduct a preliminary uplift study to empirically evaluate these capabilities,\nthen propose a taxonomy of the kinds of ground truth questions these systems\nwill answer, present a high-level model of the determinants of this system's AI\ncapabilities, and provide recommendations for nation-states to remain\nstrategically competitive within the new paradigm of automated intelligence."}
{"id": "2509.17116", "pdf": "https://arxiv.org/pdf/2509.17116", "abs": "https://arxiv.org/abs/2509.17116", "authors": ["Hang Xu", "Zang Yu", "Yehui Tang", "Pengbo Hu", "Yuhao Tang", "Hao Dong"], "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces MCTS-EP, an online learning framework that combines\nlarge language models (LLM) with Monte Carlo Tree Search (MCTS) for training\nembodied agents. MCTS-EP integrates three key components: MCTS-guided\nexploration for preference data collection, efficient multi-modal reasoning\nmechanism, and iterative training pipeline based on preference optimization. We\ntheoretically prove that MCTS-EP achieves better performance bounds than\nconventional on-policy algorithms when the loss function is strongly convex,\nand demonstrate that it can be formulated as a search-enhanced variant of GAIL.\nMCTS-EP achieves state-of-the-art performace across serval benchmarks. In\nALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.\nIn WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average\ninteraction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code\navailable at: https://github.com/xuhang-2/Embodied-Agent-Planning"}
{"id": "2509.17158", "pdf": "https://arxiv.org/pdf/2509.17158", "abs": "https://arxiv.org/abs/2509.17158", "authors": ["Pierre Andrews", "Amine Benhalloum", "Gerard Moreno-Torres Bertran", "Matteo Bettini", "Amar Budhiraja", "Ricardo Silveira Cabral", "Virginie Do", "Romain Froger", "Emilien Garreau", "Jean-Baptiste Gaya", "Hugo Laurençon", "Maxime Lecanu", "Kunal Malkan", "Dheeraj Mekala", "Pierre Ménard", "Grégoire Mialon", "Ulyana Piterbarg", "Mikhail Plekhanov", "Mathieu Rita", "Andrey Rusakov", "Thomas Scialom", "Vladislav Vorotilov", "Mengjue Wang", "Ian Yu"], "title": "ARE: Scaling Up Agent Environments and Evaluations", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward."}
{"id": "2509.17192", "pdf": "https://arxiv.org/pdf/2509.17192", "abs": "https://arxiv.org/abs/2509.17192", "authors": ["Glenn Matlin", "Parv Mahajan", "Isaac Song", "Yixiong Hao", "Ryan Bard", "Stu Topp", "Evan Montoya", "M. Rehan Parwani", "Soham Shetty", "Mark Riedl"], "title": "Shall We Play a Game? Language Models for Open-ended Wargames", "categories": ["cs.AI"], "comment": null, "summary": "Wargames are multi-faceted, multi-player depictions of conflict in which\nparticipants' decisions influence future events. Wargames are often used to\nexplore the strategic implications of decision-making. However, it also\nencompasses entertainment-oriented simulations, ranging from _Chess_ to\ntabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more\nopen-ended side of the spectrum of wargames, players use natural language to\nconvey their moves, and adjudicators propose outcomes. Language Models (LMs)\nare increasingly being considered for how they can provide insights into\nreal-world, consequential decisions. We conduct a scoping literature review of\na curated selection of 100 recent works on AI in wargames, from which we\nconstruct an ontology of wargames in terms of the creativity afforded to either\nthe players or adjudicators. Focusing on the space of wargames with the most\nopen-endedness for players and adjudicators, we distill a set of considerations\nfor when and how to use LMs in different application areas. We also present a\nset of safety considerations, best practices for deploying LMs in open-ended\nwargames, and conclude with a set of high-impact open research challenges."}
{"id": "2509.17238", "pdf": "https://arxiv.org/pdf/2509.17238", "abs": "https://arxiv.org/abs/2509.17238", "authors": ["Soheil Zibakhsh", "Mohammad Samragh", "Kumari Nishu", "Lauren Hannah", "Arnav Kundu", "Minsik Cho"], "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "comment": null, "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."}
{"id": "2509.17240", "pdf": "https://arxiv.org/pdf/2509.17240", "abs": "https://arxiv.org/abs/2509.17240", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Alaa Abd-alrazaq", "Aliya Tabassum", "Junaid Qadir"], "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process."}
{"id": "2509.17259", "pdf": "https://arxiv.org/pdf/2509.17259", "abs": "https://arxiv.org/abs/2509.17259", "authors": ["Ilham Wicaksono", "Zekun Wu", "Rahul Patel", "Theo King", "Adriano Koshiyama", "Philip Treleaven"], "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B", "categories": ["cs.AI"], "comment": "Winner of the OpenAI GPT-OSS-20B Red Teaming Challenge (Kaggle, 2025)", "summary": "As the industry increasingly adopts agentic AI systems, understanding their\nunique vulnerabilities becomes critical. Prior research suggests that security\nflaws at the model level do not fully capture the risks present in agentic\ndeployments, where models interact with tools and external environments. This\npaper investigates this gap by conducting a comparative red teaming analysis of\nGPT-OSS-20B, a 20-billion parameter open-source model. Using our observability\nframework AgentSeer to deconstruct agentic systems into granular actions and\ncomponents, we apply iterative red teaming attacks with harmful objectives from\nHarmBench at two distinct levels: the standalone model and the model operating\nwithin an agentic loop. Our evaluation reveals fundamental differences between\nmodel level and agentic level vulnerability profiles. Critically, we discover\nthe existence of agentic-only vulnerabilities, attack vectors that emerge\nexclusively within agentic execution contexts while remaining inert against\nstandalone models. Agentic level iterative attacks successfully compromise\nobjectives that completely failed at the model level, with tool-calling\ncontexts showing 24\\% higher vulnerability than non-tool contexts. Conversely,\ncertain model-specific exploits work exclusively at the model level and fail\nwhen transferred to agentic contexts, demonstrating that standalone model\nvulnerabilities do not always generalize to deployed systems."}
{"id": "2509.17318", "pdf": "https://arxiv.org/pdf/2509.17318", "abs": "https://arxiv.org/abs/2509.17318", "authors": ["Zhuofan Chen", "Jiyuan He", "Yichi Zhang", "Xing Hu", "Haoxing Wen", "Jun Bai", "Wenge Rong"], "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning poses significant challenges for Large Language Models\n(LLMs) due to its demand for multi-step reasoning and abstract conceptual\nintegration. While recent test-time scaling techniques rely heavily on\nhigh-quality, challenging problems, the scarcity of Olympiad-level math\nproblems remains a bottleneck. We introduce CogAtom, a novel cognitive\natom-based framework for synthesizing mathematically rigorous and cognitively\ndiverse problems. Unlike prior approaches, CogAtom models problem construction\nas a process of selecting and recombining fundamental reasoning units,\ncognitive atoms, extracted from human-authored solutions. A diversity-promoting\nrandom walk algorithm enables exploration of the cognitive atom space, while a\nconstraint-based recombination mechanism ensures logical soundness and\nstructural validity. The combinatorial nature of the graph structure provides a\nnear-infinite space of reasoning paths, and the walk algorithm systematically\nexplores this space to achieve large-scale synthesis of high-quality problems;\nmeanwhile, by controlling the number of cognitive atoms, we can precisely\nadjust problem difficulty, ensuring diversity, scalability, and controllability\nof the generated problems. Experimental results demonstrate that CogAtom\noutperforms existing methods in accuracy, reasoning depth, and diversity,\ngenerating problems that closely match the difficulty of AIME while exceeding\nit in structural variation. Our work offers a cognitively grounded pathway\ntoward scalable, high-quality math problem generation.Our code is publicly\navailable at https://github.com/Icarus-1111/CogAtom."}
{"id": "2509.17337", "pdf": "https://arxiv.org/pdf/2509.17337", "abs": "https://arxiv.org/abs/2509.17337", "authors": ["Ala Jararweh", "Michael Adams", "Avinash Sahu", "Abdullah Mueen", "Afsah Anwar"], "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Increasing complexity in software systems places a growing demand on\nreasoning tools that unlock vulnerabilities manifest in source code. Many\ncurrent approaches focus on vulnerability analysis as a classifying task,\noversimplifying the nuanced and context-dependent real-world scenarios. Even\nthough current code large language models (LLMs) excel in code understanding,\nthey often pay little attention to security-specific reasoning. We propose\nLLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code\nthrough question-answering (QA). Our model is trained to integrate paired code\nand natural queries into a unified space, enhancing reasoning and\ncontext-dependent insights about code vulnerability. To evaluate our model\nperformance, we construct a curated dataset of real-world vulnerabilities\npaired with security-focused questions and answers. Our model outperforms\nstate-of-the-art general-purpose and code LLMs in the QA and detection tasks.\nWe further explain decision-making by conducting qualitative analysis to\nhighlight capabilities and limitations. By integrating code and QA, LLaVul\nenables more interpretable and security-focused code understanding."}
{"id": "2509.17353", "pdf": "https://arxiv.org/pdf/2509.17353", "abs": "https://arxiv.org/abs/2509.17353", "authors": ["Ahmed T. Elboardy", "Ghada Khoriba", "Essam A. Rashed"], "title": "Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation", "categories": ["cs.AI", "eess.IV", "physics.med-ph"], "comment": "NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:\n  Benchmarks, Emergent Abilities, and Scaling", "summary": "Automating radiology report generation poses a dual challenge: building\nclinically reliable systems and designing rigorous evaluation protocols. We\nintroduce a multi-agent reinforcement learning framework that serves as both a\nbenchmark and evaluation environment for multimodal clinical reasoning in the\nradiology ecosystem. The proposed framework integrates large language models\n(LLMs) and large vision models (LVMs) within a modular architecture composed of\nten specialized agents responsible for image analysis, feature extraction,\nreport generation, review, and evaluation. This design enables fine-grained\nassessment at both the agent level (e.g., detection and segmentation accuracy)\nand the consensus level (e.g., report quality and clinical relevance). We\ndemonstrate an implementation using chatGPT-4o on public radiology datasets,\nwhere LLMs act as evaluators alongside medical radiologist feedback. By\naligning evaluation protocols with the LLM development lifecycle, including\npretraining, finetuning, alignment, and deployment, the proposed benchmark\nestablishes a path toward trustworthy deviance-based radiology report\ngeneration."}
{"id": "2509.17354", "pdf": "https://arxiv.org/pdf/2509.17354", "abs": "https://arxiv.org/abs/2509.17354", "authors": ["Jiazhao Shi", "Yichen Lin", "Yiheng Hua", "Ziyu Wang", "Zijian Zhang", "Wenjia Zheng", "Yun Song", "Kuan Lu", "Shoufeng Lu"], "title": "Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Lane-change maneuvers are a leading cause of highway accidents, underscoring\nthe need for accurate intention prediction to improve the safety and\ndecision-making of autonomous driving systems. While prior studies using\nmachine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)\nhave shown promise, most approaches remain limited by binary classification,\nlack of scenario diversity, and degraded performance under longer prediction\nhorizons. In this study, we propose a physics-informed AI framework that\nexplicitly integrates vehicle kinematics, interaction feasibility, and\ntraffic-safety metrics (e.g., distance headway, time headway,\ntime-to-collision, closing gap time) into the learning process. lane-change\nprediction is formulated as a three-class problem that distinguishes left\nchange, right change, and no change, and is evaluated across both straight\nhighway segments (highD) and complex ramp scenarios (exiD). By integrating\nvehicle kinematics with interaction features, our machine learning models,\nparticularly LightGBM, achieve state-of-the-art accuracy and strong\ngeneralization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,\nand 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,\noutperforming a two-layer stacked LSTM baseline. These findings demonstrate the\npractical advantages of a physics-informed and feature-rich machine learning\nframework for real-time lane-change intention prediction in autonomous driving\nsystems."}
{"id": "2509.17380", "pdf": "https://arxiv.org/pdf/2509.17380", "abs": "https://arxiv.org/abs/2509.17380", "authors": ["Zhizhang FU", "Guangsheng Bao", "Hongbo Zhang", "Chenkai Hu", "Yue Zhang"], "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process", "categories": ["cs.AI"], "comment": null, "summary": "LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and\ninconsistency, since they lack robust causal underpinnings and may rely on\nsuperficial correlations rather than genuine understanding. Successive LRMs\nhave emerged as a promising alternative, leveraging advanced training\ntechniques such as reinforcement learning (RL) and distillation to improve task\naccuracy. However, the impact of these training methods on causality remains\nlargely unexplored. In this study, we conduct a systematic causal analysis on\nLLMs and LRMs, examining structural causal models (SCMs) of four key variables:\nproblem instruction (Z), thinking process (T), reasoning steps (X), and answer\n(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal\nreasoning capabilities, aligning more closely with ideal causal structures,\nwhile LLMs and distilled LRMs fail to address causality-related deficiencies.\nOur further investigation indicates that RLVR reduces spurious correlations and\nstrengthens genuine causal patterns, thereby mitigating unfaithfulness and\nbias. In addition, our inspection on the dynamics of the RLVR training process\nobserves a high correlation between reduced spurious features and improved\ncausal structures, where the causal relationships consistently improve in the\ntraining process. This study contributes to the understanding of causality in\nreasoning models, highlights the critical role of RLVR in enhancing causal\nreasoning, and provides insights for designing future AI systems with stronger\ncausal foundations. We release our code and data at\nhttps://github.com/Harryking1999/CoT_Causal_Analysis."}
{"id": "2509.17393", "pdf": "https://arxiv.org/pdf/2509.17393", "abs": "https://arxiv.org/abs/2509.17393", "authors": ["Kang-il Lee", "Jahyun Koo", "Seunghyun Yoon", "Minbeom Kim", "Hyukhun Koh", "Dongryeol Lee", "Kyomin Jung"], "title": "Program Synthesis via Test-Time Transduction", "categories": ["cs.AI", "cs.CL"], "comment": "NeurIPS 2025", "summary": "We introduce transductive program synthesis, a new formulation of the program\nsynthesis task that explicitly leverages test inputs during synthesis. While\nprior approaches to program synthesis--whether based on natural language\ndescriptions or input-output examples--typically aim to generalize from\ntraining examples, they often struggle with robustness, especially in\nreal-world settings where training examples are limited and test inputs involve\nvarious edge cases. To address this, we propose a novel framework that improves\nrobustness by treating synthesis as an active learning over a finite hypothesis\nclass defined by programs' outputs. We use an LLM to predict outputs for\nselected test inputs and eliminate inconsistent hypotheses, where the inputs\nare chosen via a greedy maximin algorithm to minimize the number of LLM queries\nrequired. We evaluate our approach on two real-world datasets: Playgol, a\nstring transformation benchmark, and MBPP+, a Python code generation benchmark.\nWe demonstrate that our method significantly improves program synthesis in both\naccuracy and efficiency. We release our code at\nhttps://github.com/klee972/SYNTRA."}
{"id": "2509.17425", "pdf": "https://arxiv.org/pdf/2509.17425", "abs": "https://arxiv.org/abs/2509.17425", "authors": ["Zhenliang Zhang", "Yuxi Wang", "Hongzhao Xie", "Shiyun Zhao", "Mingyuan Liu", "Yujie Lu", "Xinyi He", "Zhenku Cheng", "Yujia Peng"], "title": "Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments", "categories": ["cs.AI"], "comment": null, "summary": "A key feature differentiating artificial general intelligence (AGI) from\ntraditional AI is that AGI can perform composite tasks that require a wide\nrange of capabilities. Although embodied agents powered by multimodal large\nlanguage models (MLLMs) offer rich perceptual and interactive capabilities, it\nremains largely unexplored whether they can solve composite tasks. In the\ncurrent work, we designed a set of composite tasks inspired by common daily\nactivities observed in early childhood development. Within a dynamic and\nsimulated home environment, these tasks span three core domains: object\nunderstanding, spatial intelligence, and social activity. We evaluated 17\nleading proprietary and open-source MLLMs on these tasks. The results\nconsistently showed poor performance across all three domains, indicating a\nsubstantial gap between current capabilities and general intelligence\nrequirements. Together, our tasks offer a preliminary framework for evaluating\nthe general capabilities of embodied agents, marking an early but significant\nstep toward the development of embodied MLLMs and their real-world deployment."}
{"id": "2509.17439", "pdf": "https://arxiv.org/pdf/2509.17439", "abs": "https://arxiv.org/abs/2509.17439", "authors": ["Yangxuan Zhou", "Sha Zhao", "Jiquan Wang", "Haiteng Jiang", "Shijian Li", "Tao Li", "Gang Pan"], "title": "SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding", "categories": ["cs.AI", "cs.LG"], "comment": "21 pages, 13 figures", "summary": "Human brain achieves dynamic stability-plasticity balance through synaptic\nhomeostasis. Inspired by this biological principle, we propose SPICED: a\nneuromorphic framework that integrates the synaptic homeostasis mechanism for\nunsupervised continual EEG decoding, particularly addressing practical\nscenarios where new individuals with inter-individual variability emerge\ncontinually. SPICED comprises a novel synaptic network that enables dynamic\nexpansion during continual adaptation through three bio-inspired neural\nmechanisms: (1) critical memory reactivation; (2) synaptic consolidation and\n(3) synaptic renormalization. The interplay within synaptic homeostasis\ndynamically strengthens task-discriminative memory traces and weakens\ndetrimental memories. By integrating these mechanisms with continual learning\nsystem, SPICED preferentially replays task-discriminative memory traces that\nexhibit strong associations with newly emerging individuals, thereby achieving\nrobust adaptations. Meanwhile, SPICED effectively mitigates catastrophic\nforgetting by suppressing the replay prioritization of detrimental memories\nduring long-term continual learning. Validated on three EEG datasets, SPICED\nshow its effectiveness."}
{"id": "2509.17460", "pdf": "https://arxiv.org/pdf/2509.17460", "abs": "https://arxiv.org/abs/2509.17460", "authors": ["Jianlong Chang", "Haixin Wang", "Zhiyuan Dang", "Li Huang", "Zhiyu Wang", "Ruoqi Cao", "Shihao Piao", "Dongzhe Li", "Dianyu Gao", "Dongsheng Wang", "Yin Li", "Jinan Sun", "Lu Fang", "Zhouchen Lin"], "title": "AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks", "categories": ["cs.AI", "cs.LG"], "comment": "65 pages, 28 figures, paper under review", "summary": "The pursuit of artificial general intelligence continuously demands\ngeneralization in one model across myriad tasks, even those not seen before.\nHowever, current AI models are isolated from each other for being limited to\nspecific tasks, now first defined as Intelligence Islands. To unify\nIntelligence Islands into one, we propose Pangaea, the first AI supercontinent\nakin to the geological Pangaea. Pangaea encodes any data into a unified format\nand accumulates universal knowledge through pre-training on 296 datasets across\ndiverse modalities. Eventually, it demonstrates remarkable generalization\nacross 45 general tasks and 15 scientific tasks encompassing a wide range of\nscientific subjects. By investigating Pangaea deeper, the scaling effect of\nmodality is revealed, quantifying the universal knowledge accumulation across\nmodalities as the cumulative distribution function of a geometric distribution.\nOn the whole, Pangaea shows strong potential to handle myriad tasks, indicating\na new direction toward artificial general intelligence."}
{"id": "2509.17544", "pdf": "https://arxiv.org/pdf/2509.17544", "abs": "https://arxiv.org/abs/2509.17544", "authors": ["Juan Cañada", "Raúl Alonso", "Julio Molleda", "Fidel Díez"], "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data", "categories": ["cs.AI"], "comment": null, "summary": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign."}
{"id": "2509.17550", "pdf": "https://arxiv.org/pdf/2509.17550", "abs": "https://arxiv.org/abs/2509.17550", "authors": ["Neslihan Kose", "Anthony Rhodes", "Umur Aybars Ciftci", "Ilke Demir"], "title": "Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted for publication at the ICCV 2025 STREAM workshop", "summary": "As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection."}
{"id": "2509.17553", "pdf": "https://arxiv.org/pdf/2509.17553", "abs": "https://arxiv.org/abs/2509.17553", "authors": ["Congcong Ge", "Yachuan Liu", "Yixuan Tang", "Yifan Zhu", "Yaofeng Tu", "Yunjun Gao"], "title": "MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances", "categories": ["cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "In commercial systems, a pervasive requirement for automatic data preparation\n(ADP) is to transfer relational data from disparate sources to targets with\nstandardized schema specifications. Previous methods rely on labor-intensive\nsupervision signals or target table data access permissions, limiting their\nusage in real-world scenarios. To tackle these challenges, we propose an\neffective end-to-end ADP framework MontePrep, which enables training-free\npipeline synthesis with zero target-instance requirements. MontePrep is\nformulated as an open-source large language model (LLM) powered tree-structured\nsearch problem. It consists of three pivot components, i.e., a data preparation\naction sandbox (DPAS), a fundamental pipeline generator (FPG), and an\nexecution-aware pipeline optimizer (EPO). We first introduce DPAS, a\nlightweight action sandbox, to navigate the search-based pipeline generation.\nThe design of DPAS circumvents exploration of infeasible pipelines. Then, we\npresent FPG to build executable DP pipelines incrementally, which explores the\npredefined action sandbox by the LLM-powered Monte Carlo Tree Search.\nFurthermore, we propose EPO, which invokes pipeline execution results from\nsources to targets to evaluate the reliability of the generated pipelines in\nFPG. In this way, unreasonable pipelines are eliminated, thus facilitating the\nsearch process from both efficiency and effectiveness perspectives. Extensive\nexperimental results demonstrate the superiority of MontePrep with significant\nimprovement against five state-of-the-art competitors."}
{"id": "2509.17567", "pdf": "https://arxiv.org/pdf/2509.17567", "abs": "https://arxiv.org/abs/2509.17567", "authors": ["Yang Xiao", "Mohan Jiang", "Jie Sun", "Keyu Li", "Jifan Lin", "Yumin Zhuang", "Ji Zeng", "Shijie Xia", "Qishuo Hua", "Xuefeng Li", "Xiaojie Cai", "Tongyu Wang", "Yue Zhang", "Liming Liu", "Xia Wu", "Jinlong Hou", "Yuan Cheng", "Wenjie Li", "Xiang Wang", "Dequan Wang", "Pengfei Liu"], "title": "LIMI: Less is More for Agency", "categories": ["cs.AI"], "comment": null, "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations."}
{"id": "2509.17589", "pdf": "https://arxiv.org/pdf/2509.17589", "abs": "https://arxiv.org/abs/2509.17589", "authors": ["Jun Ling", "Yao Qi", "Tao Huang", "Shibo Zhou", "Yanqin Huang", "Jiang Yang", "Ziqi Song", "Ying Zhou", "Yang Yang", "Heng Tao Shen", "Peng Wang"], "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models", "categories": ["cs.AI"], "comment": "NeurIPS 2025", "summary": "In this work, we address the task of table image to LaTeX code generation,\nwith the goal of automating the reconstruction of high-quality,\npublication-ready tables from visual inputs. A central challenge of this task\nlies in accurately handling complex tables -- those with large sizes, deeply\nnested structures, and semantically rich or irregular cell content -- where\nexisting methods often fail. We begin with a comprehensive analysis,\nidentifying key challenges and highlighting the limitations of current\nevaluation protocols. To overcome these issues, we propose a reinforced\nmultimodal large language model (MLLM) framework, where a pre-trained MLLM is\nfine-tuned on a large-scale table-to-LaTeX dataset. To further improve\ngeneration quality, we introduce a dual-reward reinforcement learning strategy\nbased on Group Relative Policy Optimization (GRPO). Unlike standard approaches\nthat optimize purely over text outputs, our method incorporates both a\nstructure-level reward on LaTeX code and a visual fidelity reward computed from\nrendered outputs, enabling direct optimization of the visual output quality. We\nadopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and\nshow that our method achieves state-of-the-art performance, particularly on\nstructurally complex tables, demonstrating the effectiveness and robustness of\nour approach."}
{"id": "2509.17677", "pdf": "https://arxiv.org/pdf/2509.17677", "abs": "https://arxiv.org/abs/2509.17677", "authors": ["Xiyuan Zhou", "Xinlei Wang", "Yirui He", "Yang Wu", "Ruixi Zou", "Yuheng Cheng", "Yulu Xie", "Wenxuan Liu", "Huan Zhao", "Yan Xu", "Jinjin Gu", "Junhua Zhao"], "title": "EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance on mathematical\nreasoning under well-posed conditions. However, real-world engineering problems\nrequire more than mathematical symbolic computation -- they need to deal with\nuncertainty, context, and open-ended scenarios. Existing benchmarks fail to\ncapture these complexities. We introduce EngiBench, a hierarchical benchmark\ndesigned to evaluate LLMs on solving engineering problems. It spans three\nlevels of increasing difficulty (foundational knowledge retrieval, multi-step\ncontextual reasoning, and open-ended modeling) and covers diverse engineering\nsubfields. To facilitate a deeper understanding of model performance, we\nsystematically rewrite each problem into three controlled variants (perturbed,\nknowledge-enhanced, and math abstraction), enabling us to separately evaluate\nthe model's robustness, domain-specific knowledge, and mathematical reasoning\nabilities. Experiment results reveal a clear performance gap across levels:\nmodels struggle more as tasks get harder, perform worse when problems are\nslightly changed, and fall far behind human experts on the high-level\nengineering tasks. These findings reveal that current LLMs still lack the\nhigh-level reasoning needed for real-world engineering, highlighting the need\nfor future models with deeper and more reliable problem-solving capabilities.\nOur source code and data are available at\nhttps://github.com/EngiBench/EngiBench."}
{"id": "2509.17706", "pdf": "https://arxiv.org/pdf/2509.17706", "abs": "https://arxiv.org/abs/2509.17706", "authors": ["Pierre Montalbano", "Simon de Givry", "George Katsirelos"], "title": "Virtual Arc Consistency for Linear Constraints inCost Function Networks", "categories": ["cs.AI"], "comment": null, "summary": "In Constraint Programming, solving discrete minimization problems with hard\nand soft constraints can be done either using (i) soft global constraints, (ii)\na reformulation into a linear program, or (iii) a reformulation into local cost\nfunctions. Approach (i) benefits from a vast catalog of constraints. Each soft\nconstraint propagator communicates with other soft constraints only through the\nvariable domains, resulting in weak lower bounds. Conversely, the approach (ii)\nprovides a global view with strong bounds, but the size of the reformulation\ncan be problematic. We focus on approach (iii) in which soft arc consistency\n(SAC) algorithms produce bounds of intermediate quality. Recently, the\nintroduction of linear constraints as local cost functions increases their\nmodeling expressiveness. We adapt an existing SAC algorithm to handle linear\nconstraints. We show that our algorithm significantly improves the lower bounds\ncompared to the original algorithm on several benchmarks, reducing solving time\nin some cases."}
{"id": "2509.17711", "pdf": "https://arxiv.org/pdf/2509.17711", "abs": "https://arxiv.org/abs/2509.17711", "authors": ["Shenwei Kang", "Xin Zhang", "Wen Liu", "Bin Li", "Yujie Liu", "Bo Gao"], "title": "DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation", "categories": ["cs.AI"], "comment": null, "summary": "Human engagement estimation in conversational scenarios is essential for\napplications such as adaptive tutoring, remote healthcare assessment, and\nsocially aware human--computer interaction. Engagement is a dynamic, multimodal\nsignal conveyed by facial expressions, speech, gestures, and behavioral cues\nover time. In this work we introduce DA-Mamba, a dialogue-aware multimodal\narchitecture that replaces attention-heavy dialogue encoders with Mamba-based\nselective state-space processing to achieve linear time and memory complexity\nwhile retaining expressive cross-modal reasoning. We design a Mamba\ndialogue-aware selective state-space model composed of three core modules: a\nDialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group\nFusion and Partner-Group Fusion, these modules achieve expressive dialogue\nunderstanding. Extensive experiments on three standard benchmarks (NoXi,\nNoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art\n(SOTA) methods in concordance correlation coefficient (CCC), while reducing\ntraining time and peak memory; these gains enable processing much longer\nsequences and facilitate real-time deployment in resource-constrained,\nmulti-party conversational settings. The source code will be available at:\nhttps://github.com/kksssssss-ssda/MMEA."}
{"id": "2509.17774", "pdf": "https://arxiv.org/pdf/2509.17774", "abs": "https://arxiv.org/abs/2509.17774", "authors": ["Joao Marques-Silva", "Alexey Ignatiev"], "title": "Efficient & Correct Predictive Equivalence for Decision Trees", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "The Rashomon set of decision trees (DTs) finds importance uses. Recent work\nshowed that DTs computing the same classification function, i.e. predictive\nequivalent DTs, can represent a significant fraction of the Rashomon set. Such\nredundancy is undesirable. For example, feature importance based on the\nRashomon set becomes inaccurate due the existence of predictive equivalent DTs,\ni.e. DTs with the same prediction for every possible input. In recent work,\nMcTavish et al. proposed solutions for several computational problems related\nwith DTs, including that of deciding predictive equivalent DTs. This approach,\nwhich this paper refers to as MBDSR, consists of applying the well-known method\nof Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal\nform) representations of DTs, which are then used for comparing DTs for\npredictive equivalence. Furthermore, the minimum-size DNF representation was\nalso applied to computing explanations for the predictions made by DTs, and to\nfinding predictions in the presence of missing data. However, the problem of\nformula minimization is hard for the second level of the polynomial hierarchy,\nand the QM method may exhibit worst-case exponential running time and space.\nThis paper first demonstrates that there exist decision trees that trigger the\nworst-case exponential running time and space of the QM method. Second, the\npaper shows that the MBDSR approach can produce incorrect results for the\nproblem of deciding predictive equivalence. Third, the paper shows that any of\nthe problems to which the minimum-size DNF representation has been applied to\ncan in fact be solved in polynomial time, in the size of the DT. The\nexperiments confirm that, for DTs for which the the worst-case of the QM method\nis triggered, the algorithms proposed in this paper are orders of magnitude\nfaster than the ones proposed by McTavish et al."}
{"id": "2509.17905", "pdf": "https://arxiv.org/pdf/2509.17905", "abs": "https://arxiv.org/abs/2509.17905", "authors": ["Zongqian Wu", "Baoduo Xu", "Tianyu Li", "Zhu Sun", "Xiaofeng Zhu", "Lei Feng"], "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling", "categories": ["cs.AI"], "comment": "23 pages, 9 figures", "summary": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets."}
{"id": "2509.17907", "pdf": "https://arxiv.org/pdf/2509.17907", "abs": "https://arxiv.org/abs/2509.17907", "authors": ["Xiaojing Dong", "Weilin Huang", "Liang Li", "Yiying Li", "Shu Liu", "Tongtong Ou", "Shuang Ouyang", "Yu Tian", "Fengxuan Zhao"], "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models", "categories": ["cs.AI"], "comment": null, "summary": "Rapid advances in text-to-image (T2I) generation have raised higher\nrequirements for evaluation methodologies. Existing benchmarks center on\nobjective capabilities and dimensions, but lack an application-scenario\nperspective, limiting external validity. Moreover, current evaluations\ntypically rely on either ELO for overall ranking or MOS for dimension-specific\nscoring, yet both methods have inherent shortcomings and limited\ninterpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),\na systematic and practical approach for evaluating T2I models. First, we\npropose a structured taxonomy encompassing user scenarios, elements, element\ncompositions, and text expression forms to construct the Magic-Bench-377, which\nsupports label-level assessment and ensures a balanced coverage of both user\nscenarios and capabilities. On this basis, we combine ELO and\ndimension-specific MOS to generate model rankings and fine-grained assessments\nrespectively. This joint evaluation method further enables us to quantitatively\nanalyze the contribution of each dimension to user satisfaction using\nmultivariate logistic regression. By applying MEF to current T2I models, we\nobtain a leaderboard and key characteristics of the leading models. We release\nour evaluation framework and make Magic-Bench-377 fully open-source to advance\nresearch in the evaluation of visual generative models."}
{"id": "2509.17917", "pdf": "https://arxiv.org/pdf/2509.17917", "abs": "https://arxiv.org/abs/2509.17917", "authors": ["Junyu Lu", "Songxin Zhang", "Zejian Xie", "Zhuoyang Song", "Jiaxing Zhang"], "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities."}
{"id": "2509.17956", "pdf": "https://arxiv.org/pdf/2509.17956", "abs": "https://arxiv.org/abs/2509.17956", "authors": ["Lin Luo", "Yuri Nakao", "Mathieu Chollet", "Hiroya Inakoshi", "Simone Stumpf"], "title": "\"I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Assessing fairness in artificial intelligence (AI) typically involves AI\nexperts who select protected features, fairness metrics, and set fairness\nthresholds. However, little is known about how stakeholders, particularly those\naffected by AI outcomes but lacking AI expertise, assess fairness. To address\nthis gap, we conducted a qualitative study with 30 stakeholders without AI\nexpertise, representing potential decision subjects in a credit rating\nscenario, to examine how they assess fairness when placed in the role of\ndeciding on features with priority, metrics, and thresholds. We reveal that\nstakeholders' fairness decisions are more complex than typical AI expert\npractices: they considered features far beyond legally protected features,\ntailored metrics for specific contexts, set diverse yet stricter fairness\nthresholds, and even preferred designing customized fairness. Our results\nextend the understanding of how stakeholders can meaningfully contribute to AI\nfairness governance and mitigation, underscoring the importance of\nincorporating stakeholders' nuanced fairness judgments."}
{"id": "2509.17957", "pdf": "https://arxiv.org/pdf/2509.17957", "abs": "https://arxiv.org/abs/2509.17957", "authors": ["David Hyland", "Mahault Albarracin"], "title": "On the Variational Costs of Changing Our Minds", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": "Accepted as a full paper at the 6th International Workshop on Active\n  Inference", "summary": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes."}
{"id": "2509.17978", "pdf": "https://arxiv.org/pdf/2509.17978", "abs": "https://arxiv.org/abs/2509.17978", "authors": ["Antoni Guasch", "Maria Isabel Valdez"], "title": "The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents", "categories": ["cs.AI", "cs.LO"], "comment": "Paper 1 of 4 in The STAR-XAI Protocol series. Paper 2\n  [arXiv:ID_to_be_added], Paper 3 [arXiv:ID_to_be_added], Paper 4\n  [arXiv:ID_to_be_added]", "summary": "Current Large Reasoning Models (LRMs) exhibit significant limitations in\nreliability and transparency, often showing a collapse in reasoning\ncapabilities when faced with high-complexity, long-horizon tasks. This\n\"illusion of thinking\" is frequently an artifact of non-agentic, black-box\nevaluation paradigms that fail to cultivate robust problem-solving processes.\nIn response, we introduce The STAR-XAI Protocol (Socratic, Transparent,\nAgentic, Reasoning - for eXplainable Artificial Intelligence), a novel\nmethodology for training and operating verifiably reliable AI agents. Our\nmethod reframes the human-AI interaction as a structured, Socratic dialogue,\ngoverned by an explicit and evolving rulebook, the Consciousness Transfer\nPackage (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc\nstrategic justification and a state-locking Checksum that prevents error\naccumulation, the protocol transforms a powerful but opaque LRM into a\ndisciplined \"Clear Box\" agent. We demonstrate the efficacy of this method\nthrough an exhaustive 25-move case study in the complex strategic game \"Caps i\nCaps\". The agent not only solved the high-complexity puzzle but also\ndemonstrated Second-Order Agency, identifying flaws in its own\nsupervisor-approved plans and adapting its core integrity protocols mid-task.\nThe STAR-XAI Protocol offers a practical pathway to creating AI agents that are\nnot just high-performing, but also transparent, auditable, and trustworthy by\ndesign."}
{"id": "2509.18076", "pdf": "https://arxiv.org/pdf/2509.18076", "abs": "https://arxiv.org/abs/2509.18076", "authors": ["Hy Dang", "Tianyi Liu", "Zhuofeng Wu", "Jingfeng Yang", "Haoming Jiang", "Tao Yang", "Pei Chen", "Zhengyang Wang", "Helen Wang", "Huasheng Li", "Bing Yin", "Meng Jiang"], "title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates", "categories": ["cs.AI"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications."}
{"id": "2509.18083", "pdf": "https://arxiv.org/pdf/2509.18083", "abs": "https://arxiv.org/abs/2509.18083", "authors": ["Valentin Lacombe", "Valentin Quesnel", "Damien Sileo"], "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models."}
{"id": "2403.09548", "pdf": "https://arxiv.org/pdf/2403.09548", "abs": "https://arxiv.org/abs/2403.09548", "authors": ["João Manoel Herrera Pinheiro", "Marcelo Becker"], "title": "Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.QM"], "comment": "9 pages, 16 figures", "summary": "Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models."}
{"id": "2509.16212", "pdf": "https://arxiv.org/pdf/2509.16212", "abs": "https://arxiv.org/abs/2509.16212", "authors": ["Ahmad Maroof Karimi", "Woong Shin", "Jesse Hines", "Tirthankar Ghosal", "Naw Safrin Sattar", "Feiyi Wang"], "title": "EPIC: Generative AI Platform for Accelerating HPC Operational Data Analytics", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "We present EPIC, an AI-driven platform designed to augment operational data\nanalytics. EPIC employs a hierarchical multi-agent architecture where a\ntop-level large language model provides query processing, reasoning and\nsynthesis capabilities. These capabilities orchestrate three specialized\nlow-level agents for information retrieval, descriptive analytics, and\npredictive analytics. This architecture enables EPIC to perform HPC operational\nanalytics on multi-modal data, including text, images, and tabular formats,\ndynamically and iteratively. EPIC addresses the limitations of existing HPC\noperational analytics approaches, which rely on static methods that struggle to\nadapt to evolving analytics tasks and stakeholder demands.\n  Through extensive evaluations on the Frontier HPC system, we demonstrate that\nEPIC effectively handles complex queries. Using descriptive analytics as a use\ncase, fine-tuned smaller models outperform large state-of-the-art foundation\nmodels, achieving up to 26% higher accuracy. Additionally, we achieved 19x\nsavings in LLM operational costs compared to proprietary solutions by employing\na hybrid approach that combines large foundational models with fine-tuned local\nopen-weight models."}
{"id": "2509.16213", "pdf": "https://arxiv.org/pdf/2509.16213", "abs": "https://arxiv.org/abs/2509.16213", "authors": ["Xiaolei Zhu", "Xiaofei Jin", "Ziyang Kang", "Chonghui Sun", "Junjie Feng", "Dingwen Hu", "Zengyi Wang", "Hanyue Zhuang", "Qian Zheng", "Huajin Tang", "Shi Gu", "Xin Du", "De Ma", "Gang Pan"], "title": "DarwinWafer: A Wafer-Scale Neuromorphic Chip", "categories": ["cs.ET", "cs.AI", "cs.AR"], "comment": null, "summary": "Neuromorphic computing promises brain-like efficiency, yet today's multi-chip\nsystems scale over PCBs and incur orders-of-magnitude penalties in bandwidth,\nlatency, and energy, undermining biological algorithms and system efficiency.\nWe present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip\ninterconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets\non a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based\nasynchronous wafer fabric with hierarchical time-step synchronization provide\nlow-latency, coherent operation across the wafer. Each chiplet implements 2.35\nM neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per\nwafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9\npJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by\na holistic chiplet-interposer co-design flow (including an in-house\ninterposer-bump planner with early SI/PI and electro-thermal closure) and a\nwarpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin\nconnections, enabling robust, demountable wafer-to-board integration.\nMeasurements confirm 10 mV supply droop and a uniform thermal profile (34-36\n{\\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations:\ntwo zebrafish brains per chiplet with high connectivity fidelity (Spearman r =\n0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our\nknowledge, DarwinWafer represents a pioneering demonstration of wafer-scale\nneuromorphic computing, establishing a viable and scalable path toward\nlarge-scale, brain-like computation on silicon by replacing PCB-level\ninterconnects with high-density, on-wafer integration."}
{"id": "2509.16215", "pdf": "https://arxiv.org/pdf/2509.16215", "abs": "https://arxiv.org/abs/2509.16215", "authors": ["Izavan dos S. Correia", "Henrique C. T. Santos", "Tiago A. E. Ferreira"], "title": "Discovering Software Parallelization Points Using Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "cs.PL", "cs.SE"], "comment": "17 pages, 10 figures", "summary": "This study proposes a deep learning-based approach for discovering loops in\nprogramming code according to their potential for parallelization. Two genetic\nalgorithm-based code generators were developed to produce two distinct types of\ncode: (i) independent loops, which are parallelizable, and (ii) ambiguous\nloops, whose dependencies are unclear, making them impossible to define if the\nloop is parallelizable or not. The generated code snippets were tokenized and\npreprocessed to ensure a robust dataset. Two deep learning models - a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN) - were\nimplemented to perform the classification. Based on 30 independent runs, a\nrobust statistical analysis was employed to verify the expected performance of\nboth models, DNN and CNN. The CNN showed a slightly higher mean performance,\nbut the two models had a similar variability. Experiments with varying dataset\nsizes highlighted the importance of data diversity for model performance. These\nresults demonstrate the feasibility of using deep learning to automate the\nidentification of parallelizable structures in code, offering a promising tool\nfor software optimization and performance improvement."}
{"id": "2509.16226", "pdf": "https://arxiv.org/pdf/2509.16226", "abs": "https://arxiv.org/abs/2509.16226", "authors": ["Brian S. Lin", "Jiaxin Yuan", "Zihan Zhou", "Shouli Wang", "Shuo Wang", "Cunliang Kong", "Qi Shi", "Yuxuan Li", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages", "summary": "As large language models (LLMs) increasingly exhibit human-like capabilities,\na fundamental question emerges: How can we enable LLMs to learn the underlying\npatterns from limited examples in entirely novel environments and apply them\neffectively? This question is central to the ability of LLMs in inductive\nreasoning. Existing research on LLM-based inductive reasoning can be broadly\ncategorized based on whether the underlying rules are expressible via explicit\nmathematical equations. However, many recent studies in the beyond-equations\ncategory have emphasized rule design without grounding them in specific\nscenarios. Inspired by the parallels between inductive reasoning and human\nscientific discovery, we propose the task of LLM-Based Scientific Inductive\nReasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to\nevaluate the inductive reasoning abilities of LLMs in scientific settings. Our\nexperimental results show that current LLMs still struggle with this task,\nunderscoring its difficulty and the need for further advancement in this area."}
{"id": "2509.16241", "pdf": "https://arxiv.org/pdf/2509.16241", "abs": "https://arxiv.org/abs/2509.16241", "authors": ["Eishkaran Singh", "Tanav Singh Bajaj", "Siddharth Nayak"], "title": "REAMS: Reasoning Enhanced Algorithm for Maths Solving", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": null, "summary": "The challenges of solving complex university-level mathematics problems,\nparticularly those from MIT, and Columbia University courses, and selected\ntasks from the MATH dataset, remain a significant obstacle in the field of\nartificial intelligence. Conventional methods have consistently fallen short in\nthis domain, highlighting the need for more advanced approaches. In this paper,\nwe introduce a language-based solution that leverages zero-shot learning and\nmathematical reasoning to effectively solve, explain, and generate solutions\nfor these advanced math problems. By integrating program synthesis, our method\nreduces reliance on large-scale training data while significantly improving\nproblem-solving accuracy. Our approach achieves an accuracy of 90.15%,\nrepresenting a substantial improvement over the previous benchmark of 81% and\nsetting a new standard in automated mathematical problem-solving. These\nfindings highlight the significant potential of advanced AI methodologies to\naddress and overcome the challenges presented by some of the most complex\nmathematical courses and datasets."}
{"id": "2509.16250", "pdf": "https://arxiv.org/pdf/2509.16250", "abs": "https://arxiv.org/abs/2509.16250", "authors": ["Saifuddin Sagor", "Md Taimur Ahad", "Faruk Ahmed", "Rokonozzaman Ayon", "Sanzida Parvin"], "title": "A study on Deep Convolutional Neural Networks, transfer learning, and Mnet model for Cervical Cancer Detection", "categories": ["q-bio.TO", "cs.AI", "cs.CV"], "comment": null, "summary": "Early and accurate detection through Pap smear analysis is critical to\nimproving patient outcomes and reducing mortality of Cervical cancer.\nState-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require\nsubstantial computational resources, extended training time, and large\ndatasets. In this study, a lightweight CNN model, S-Net (Simple Net), is\ndeveloped specifically for cervical cancer detection and classification using\nPap smear images to address these limitations. Alongside S-Net, six SOTA CNNs\nwere evaluated using transfer learning, including multi-path (DenseNet201,\nResNet152), depth-based (Serasnet152), width-based multi-connection (Xception),\ndepth-wise separable convolutions (MobileNetV2), and spatial exploitation-based\n(VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net\nreaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in\nterms of computational efficiency and inference time, making it a more\npractical choice for real-time and resource-constrained applications. A major\nlimitation in CNN-based medical diagnosis remains the lack of transparency in\nthe decision-making process. To address this, Explainable AI (XAI) techniques,\nsuch as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the\nkey image regions influencing model predictions. The novelty of this study lies\nin the development of a highly accurate yet computationally lightweight model\n(S-Net) caPable of rapid inference while maintaining interpretability through\nXAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs,\ninvestigates the effects of negative transfer learning on Pap smear images, and\nexamines pixel intensity patterns in correctly and incorrectly classified\nsamples."}
{"id": "2509.16251", "pdf": "https://arxiv.org/pdf/2509.16251", "abs": "https://arxiv.org/abs/2509.16251", "authors": ["Rokonozzaman Ayon", "Md Taimur Ahad", "Bo Song", "Yan Li"], "title": "R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration", "categories": ["q-bio.TO", "cs.AI", "cs.CV"], "comment": null, "summary": "State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticized\nfor their extensive computational power, long training times, and large\ndatasets. To overcome this limitation, we propose a reasonable network (R-Net),\na lightweight CNN only to detect and classify colorectal cancer (CRC) using the\nEnteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset\n(EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs\n(DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-based\nmulti-connection CNNs (Xception), depth-wise separable convolutions\n(MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, and\ntwo ensemble models are also tested on the same dataset. The ensemble models\nare a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) and\na multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However,\nthe proposed R-Net lightweight achieved 99.37% accuracy, outperforming\nMobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand the\ndecision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM are\nintegrated to visualize which parts of the EBHI image contribute to the\ndetection and classification process of R-Net. The main novelty of this\nresearch lies in building a reliable, lightweight CNN R-Net that requires fewer\ncomputing resources yet maintains strong prediction results. SOTA CNNs,\ntransfer learning, and ensemble models also extend our knowledge on CRC\nclassification and detection. XAI functionality and the impact of pixel\nintensity on correct and incorrect classification images are also some\nnovelties in CRC detection and classification."}
{"id": "2509.16254", "pdf": "https://arxiv.org/pdf/2509.16254", "abs": "https://arxiv.org/abs/2509.16254", "authors": ["Sajim Ahmed", "Muhammad Zain Chaudhary", "Muhammad Zohaib Chaudhary", "Mahmoud Abbass", "Ahmed Sherif", "Mohammad Mahbubur Rahman Khan Mamun"], "title": "Imaging Modalities-Based Classification for Lung Cancer Detection", "categories": ["q-bio.TO", "cs.AI", "68T07, 92C55"], "comment": "Accepted at ICMI 2025", "summary": "Lung cancer continues to be the predominant cause of cancer-related mortality\nglobally. This review analyzes various approaches, including advanced image\nprocessing methods, focusing on their efficacy in interpreting CT scans, chest\nradiographs, and biological markers. Notably, we identify critical gaps in the\nprevious surveys, including the need for robust models that can generalize\nacross diverse populations and imaging modalities. This comprehensive synthesis\naims to serve as a foundational resource for researchers and clinicians,\nguiding future efforts toward more accurate and efficient lung cancer\ndetection. Key findings reveal that 3D CNN architectures integrated with CT\nscans achieve the most superior performances, yet challenges such as high false\npositives, dataset variability, and computational complexity persist across\nmodalities."}
{"id": "2509.16256", "pdf": "https://arxiv.org/pdf/2509.16256", "abs": "https://arxiv.org/abs/2509.16256", "authors": ["Asiya Ibrahim Zanga", "Salisu Mamman Abdulrahman", "Abubakar Ado", "Abdulkadir Abubakar Bichi", "Lukman Aliyu Jibril", "Abdulmajid Babangida Umar", "Alhassan Adamu", "Shamsuddeen Hassan Muhammad", "Bashir Salisu Abubakar"], "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language", "categories": ["cs.CL", "cs.AI"], "comment": "Masters Thesis, a Dataset Paper", "summary": "The development of Natural Language Processing (NLP) tools for low-resource\nlanguages is critically hindered by the scarcity of annotated datasets. This\npaper addresses this fundamental challenge by introducing HausaMovieReview, a\nnovel benchmark dataset comprising 5,000 YouTube comments in Hausa and\ncode-switched English. The dataset was meticulously annotated by three\nindependent annotators, demonstrating a robust agreement with a Fleiss' Kappa\nscore of 0.85 between annotators. We used this dataset to conduct a comparative\nanalysis of classical models (Logistic Regression, Decision Tree, K-Nearest\nNeighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results\nreveal a key finding: the Decision Tree classifier, with an accuracy and\nF1-score 89.72% and 89.60% respectively, significantly outperformed the deep\nlearning models. Our findings also provide a robust baseline, demonstrating\nthat effective feature engineering can enable classical models to achieve\nstate-of-the-art performance in low-resource contexts, thereby laying a solid\nfoundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis"}
{"id": "2509.16262", "pdf": "https://arxiv.org/pdf/2509.16262", "abs": "https://arxiv.org/abs/2509.16262", "authors": ["Jeonghyun Lee", "Jui-Tse Hung", "Meryem Yilmaz Soylu", "Diana Popescu", "Christopher Zhang Cui", "Gayane Grigoryan", "David A Joyner", "Stephen W Harmon"], "title": "Socratic Mind: Impact of a Novel GenAI-Powered Assessment Tool on Student Learning and Higher-Order Thinking", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This study examines the impact of Socratic Mind, a Generative Artificial\nIntelligence (GenAI) powered formative assessment tool that employs Socratic\nquestioning to support student learning in a large, fully online\nundergraduate-level computing course. Employing a quasi-experimental,\nmixed-methods design, we investigated participants' engagement patterns, the\ninfluence of user experience on engagement, and impacts on both perceived and\nactual learning outcomes. Data were collected from the system logs, surveys on\nuser experience and perceived engagement and learning gains, student\nreflections, and course performance data. Results indicated that participants\nconsistently reported high levels of affective, behavioral, and cognitive\nengagement, and these were strongly linked to positive user experiences and\nperceived learning outcomes. Quantitative analysis further revealed that\nstudents who engaged with the GenAI tool experienced significant gains in their\nquiz scores compared to those who did not, particularly benefiting students\nwith lower baseline achievement. Additionally, thematic analysis of qualitative\nfeedback revealed substantial perceived improvements in higher-order thinking\nskills, including problem solving, critical thinking, and self-reflection. Our\nfindings highlight the promise of AI-mediated dialogue in fostering deeper\nengagement and higher-order cognitive skills. As higher education institutions\nexpand GenAI integration in curriculum, this dialogic, GenAI powered assessment\ntool can offer a scalable strategy to promote students' meaningful learning\noutcomes."}
{"id": "2509.16264", "pdf": "https://arxiv.org/pdf/2509.16264", "abs": "https://arxiv.org/abs/2509.16264", "authors": ["Wenjie Lin", "Hange Liu", "Xutao Mao", "Yingying Zhuang", "Jingwei Shi", "Xudong Han", "Tianyu Shi", "Jinrui Yang"], "title": "Gender and Political Bias in Large Language Models: A Demonstration Platform", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "online demo: https://euro-parl-vote-demo.vercel.app/; Video:\n  https://www.youtube.com/@Jinrui-sf2jg", "summary": "We present ParlAI Vote, an interactive system for exploring European\nParliament debates and votes, and for testing LLMs on vote prediction and bias\nanalysis. This platform connects debate topics, speeches, and roll-call\noutcomes, and includes rich demographic data such as gender, age, country, and\npolitical group. Users can browse debates, inspect linked speeches, compare\nreal voting outcomes with predictions from frontier LLMs, and view error\nbreakdowns by demographic group. Visualizing the EuroParlVote benchmark and its\ncore tasks of gender classification and vote prediction, ParlAI Vote highlights\nsystematic performance bias in state-of-the-art LLMs. The system unifies data,\nmodels, and visual analytics in a single interface, lowering the barrier for\nreproducing findings, auditing behavior, and running counterfactual scenarios.\nIt supports research, education, and public engagement with legislative\ndecision-making, while making clear both the strengths and the limitations of\ncurrent LLMs in political analysis."}
{"id": "2509.16268", "pdf": "https://arxiv.org/pdf/2509.16268", "abs": "https://arxiv.org/abs/2509.16268", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Wenxuan Wang", "Pingchuan Ma", "Shuai Wang", "Lei Ma"], "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Function calling (FC) has emerged as a powerful technique for facilitating\nlarge language models (LLMs) to interact with external systems and perform\nstructured tasks. However, the mechanisms through which it influences model\nbehavior remain largely under-explored. Besides, we discover that in addition\nto the regular usage of FC, this technique can substantially enhance the\ncompliance of LLMs with user instructions. These observations motivate us to\nleverage causality, a canonical analysis method, to investigate how FC works\nwithin LLMs. In particular, we conduct layer-level and token-level causal\ninterventions to dissect FC's impact on the model's internal computational\nlogic when responding to user queries. Our analysis confirms the substantial\ninfluence of FC and reveals several in-depth insights into its mechanisms. To\nfurther validate our findings, we conduct extensive experiments comparing the\neffectiveness of FC-based instructions against conventional prompting methods.\nWe focus on enhancing LLM safety robustness, a critical LLM application\nscenario, and evaluate four mainstream LLMs across two benchmark datasets. The\nresults are striking: FC shows an average performance improvement of around\n135% over conventional prompting methods in detecting malicious inputs,\ndemonstrating its promising potential to enhance LLM reliability and capability\nin practical applications."}
{"id": "2509.16273", "pdf": "https://arxiv.org/pdf/2509.16273", "abs": "https://arxiv.org/abs/2509.16273", "authors": ["Jungseob Yi", "Seoyoung Choi", "Sun Kim", "Sangseon Lee"], "title": "SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening Enhancement Controlling False Positive", "categories": ["cs.LG", "cs.AI"], "comment": "33 pages, 12 figures", "summary": "Virtual screening (VS) aims to identify bioactive compounds from vast\nchemical libraries, but remains difficult in low-label regimes where only a few\nactives are known. Existing methods largely rely on general-purpose molecular\nfingerprints and overlook class-discriminative substructures critical to\nbioactivity. Moreover, they consider molecules independently, limiting\neffectiveness in low-label regimes. We introduce SubDyve, a network-based VS\nframework that constructs a subgraph-aware similarity network and propagates\nactivity signals from a small known actives. When few active compounds are\navailable, SubDyve performs iterative seed refinement, incrementally promoting\nnew candidates based on local false discovery rate. This strategy expands the\nseed set with promising candidates while controlling false positives from\ntopological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets\nunder zero-shot conditions and on the CDK7 target with a 10-million-compound\nZINC dataset. SubDyve consistently outperforms existing fingerprint or\nembedding-based approaches, achieving margins of up to +34.0 on the BEDROC and\n+24.6 on the EF1% metric."}
{"id": "2509.16275", "pdf": "https://arxiv.org/pdf/2509.16275", "abs": "https://arxiv.org/abs/2509.16275", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness\n  and Security of Large Language Models (ROSE-LLM) special session at ICMLA\n  2025", "summary": "Modern software development pipelines face growing challenges in securing\nlarge codebases with extensive dependencies. Static analysis tools like Bandit\nare effective at vulnerability detection but suffer from high false positives\nand lack repair capabilities. Large Language Models (LLMs), in contrast, can\nsuggest fixes but often hallucinate changes and lack self-validation. We\npresent SecureFixAgent, a hybrid repair framework integrating Bandit with\nlightweight local LLMs (<8B parameters) in an iterative detect-repair-validate\nloop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning\non a diverse, curated dataset spanning multiple Python project domains,\nmitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses\nBandit for detection, the LLM for candidate fixes with explanations, and Bandit\nre-validation for verification, all executed locally to preserve privacy and\nreduce cloud reliance. Experiments show SecureFixAgent reduces false positives\nby 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers\nfalse positives by 5.46% compared to pre-trained LLMs, typically converging\nwithin three iterations. Beyond metrics, developer studies rate explanation\nquality 4.5/5, highlighting its value for human trust and adoption. By\ncombining verifiable security improvements with transparent rationale in a\nresource-efficient local framework, SecureFixAgent advances trustworthy,\nautomated vulnerability remediation for modern pipelines."}
{"id": "2509.16276", "pdf": "https://arxiv.org/pdf/2509.16276", "abs": "https://arxiv.org/abs/2509.16276", "authors": ["Bahare Riahi", "Veronica Catete"], "title": "Comparative Analysis of STEM and non-STEM Teachers' Needs for Integrating AI into Educational Environments", "categories": ["cs.CY", "cs.AI", "cs.HC", "I.2.1; I.2.4; K.3.1; H.5.2"], "comment": "16 pages, 3 figures, Published in HCII 2025 Conference Proceedings", "summary": "There is an increasing imperative to integrate programming platforms within\nAI frameworks to enhance educational tasks for both teachers and students.\nHowever, commonly used platforms such as Code.org, Scratch, and Snap fall short\nof providing the desired AI features and lack adaptability for\ninterdisciplinary applications. This study explores how educational platforms\ncan be improved by incorporating AI and analytics features to create more\neffective learning environments across various subjects and domains. We\ninterviewed 8 K-12 teachers and asked their practices and needs while using any\nblock-based programming (BBP) platform in their classes. We asked for their\napproaches in assessment, course development and expansion of resources, and\nstudent monitoring in their classes. Thematic analysis of the interview\ntranscripts revealed both commonalities and differences in the AI tools needed\nbetween the STEM and non-STEM groups. Our results indicated advanced AI\nfeatures that could promote BBP platforms. Both groups stressed the need for\nintegrity and plagiarism checks, AI adaptability, customized rubrics, and\ndetailed feedback in assessments. Non-STEM teachers also emphasized the\nimportance of creative assignments and qualitative assessments. Regarding\nresource development, both AI tools desired for updating curricula, tutoring\nlibraries, and generative AI features. Non-STEM teachers were particularly\ninterested in supporting creative endeavors, such as art simulations. For\nstudent monitoring, both groups prioritized desktop control, daily tracking,\nbehavior monitoring, and distraction prevention tools. Our findings identify\nspecific AI-enhanced features needed by K-12 teachers across various\ndisciplines and lay the foundation for creating more efficient, personalized,\nand engaging educational experiences."}
{"id": "2509.16277", "pdf": "https://arxiv.org/pdf/2509.16277", "abs": "https://arxiv.org/abs/2509.16277", "authors": ["Haobo Yang", "Shiyan Zhang", "Zhuoyi Yang", "Jilong Guo", "Jun Yang", "Xinyu Zhang"], "title": "Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep perception networks in autonomous driving traditionally rely on\ndata-intensive training regimes and post-hoc anomaly detection, often\ndisregarding fundamental information-theoretic constraints governing stable\ninformation processing. We reconceptualize deep neural encoders as hierarchical\ncommunication chains that incrementally compress raw sensory inputs into\ntask-relevant latent features. Within this framework, we establish two\ntheoretically justified design principles for robust perception: (D1) smooth\nvariation of mutual information between consecutive layers, and (D2) monotonic\ndecay of latent entropy with network depth. Our analysis shows that, under\nrealistic architectural assumptions, particularly blocks comprising repeated\nlayers of similar capacity, enforcing smooth information flow (D1) naturally\nencourages entropy decay (D2), thus ensuring stable compression. Guided by\nthese insights, we propose Eloss, a novel entropy-based regularizer designed as\na lightweight, plug-and-play training objective. Rather than marginal accuracy\nimprovements, this approach represents a conceptual shift: it unifies\ninformation-theoretic stability with standard perception tasks, enabling\nexplicit, principled detection of anomalous sensor inputs through entropy\ndeviations. Experimental validation on large-scale 3D object detection\nbenchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss\nconsistently achieves competitive or improved accuracy while dramatically\nenhancing sensitivity to anomalies, amplifying distribution-shift signals by up\nto two orders of magnitude. This stable information-compression perspective not\nonly improves interpretability but also establishes a solid theoretical\nfoundation for safer, more robust autonomous driving perception systems."}
{"id": "2509.16279", "pdf": "https://arxiv.org/pdf/2509.16279", "abs": "https://arxiv.org/abs/2509.16279", "authors": ["Sarahana Shrestha", "Aparna S. Varde", "Pankaj Lal"], "title": "Energy Equity, Infrastructure and Demographic Analysis with XAI Methods", "categories": ["cs.CY", "cs.AI", "I.2.6; H.4.2"], "comment": null, "summary": "This study deploys methods in explainable artificial intelligence (XAI), e.g.\ndecision trees and Pearson's correlation coefficient (PCC), to investigate\nelectricity usage in multiple locales. It addresses the vital issue of energy\nburden, i.e. total amount spent on energy divided by median household income.\nSocio-demographic data is analyzed with energy features, especially using\ndecision trees and PCC, providing explainable predictors on factors affecting\nenergy burden. Based on the results of the analysis, a pilot energy equity web\nportal is designed along with a novel energy burden calculator. Leveraging XAI,\nthis portal (with its calculator) serves as a prototype information system that\ncan offer tailored actionable advice to multiple energy stakeholders. The\nultimate goal of this study is to promote greater energy equity through the\nadaptation of XAI methods for energy-related analysis with suitable\nrecommendations."}
{"id": "2509.16293", "pdf": "https://arxiv.org/pdf/2509.16293", "abs": "https://arxiv.org/abs/2509.16293", "authors": ["Borui Wan", "Gaohong Liu", "Zuquan Song", "Jun Wang", "Yun Zhang", "Guangming Sheng", "Shuguang Wang", "Houmin Wei", "Chenyuan Wang", "Weiqiang Lou", "Xi Yang", "Mofan Zhang", "Kaihua Jiang", "Cheng Ren", "Xiaoyun Zhi", "Menghan Yu", "Zhe Nan", "Zhuolin Zheng", "Baoquan Zhong", "Qinlong Wang", "Huan Yu", "Jinxin Chi", "Wang Zhang", "Yuhan Li", "Zixian Du", "Sida Zhao", "Yongqiang Zhang", "Jingzhe Tang", "Zherui Liu", "Chuan Wu", "Yanghua Peng", "Haibin Lin", "Wencong Xiao", "Xin Liu", "Liang Xiang"], "title": "Robust LLM Training Infrastructure at ByteDance", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."}
{"id": "2509.16295", "pdf": "https://arxiv.org/pdf/2509.16295", "abs": "https://arxiv.org/abs/2509.16295", "authors": ["Mobina Noori", "Mahasweta Chakraborti", "Amy X Zhang", "Seth Frey"], "title": "Patterns in the Transition From Founder-Leadership to Community Governance of Open Source", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Open digital public infrastructure needs community management to ensure\naccountability, sustainability, and robustness. Yet open-source projects often\nrely on centralized decision-making, and the determinants of successful\ncommunity management remain unclear. We analyze 637 GitHub repositories to\ntrace transitions from founder-led to shared governance. Specifically, we\ndocument trajectories to community governance by extracting institutional\nroles, actions, and deontic cues from version-controlled project constitutions\nGOVERNANCE.md. With a semantic parsing pipeline, we cluster elements into\nbroader role and action types. We find roles and actions grow, and regulation\nbecomes more balanced, reflecting increases in governance scope and\ndifferentiation over time. Rather than shifting tone, communities grow by\nlayering and refining responsibilities. As transitions to community management\nmature, projects increasingly regulate ecosystem-level relationships and add\ndefinition to project oversight roles. Overall, this work offers a scalable\npipeline for tracking the growth and development of community governance\nregimes from open-source software's familiar default of founder-ownership."}
{"id": "2509.16297", "pdf": "https://arxiv.org/pdf/2509.16297", "abs": "https://arxiv.org/abs/2509.16297", "authors": ["Richard Ackermann", "Simeon Emanuilov"], "title": "How Large Language Models are Designed to Hallucinate", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "23 pages, 2 tables, 2 figures", "summary": "Large language models (LLMs) achieve remarkable fluency across linguistic and\nreasoning tasks but remain systematically prone to hallucination. Prevailing\naccounts attribute hallucinations to data gaps, limited context, or\noptimization errors. We argue instead that hallucination is a structural\noutcome of the transformer architecture. As coherence engines, transformers are\ncompelled to produce fluent continuations, with self-attention simulating the\nrelational structure of meaning but lacking the existential grounding of\ntemporality, mood, and care that stabilizes human understanding. On this basis,\nwe distinguish ontological hallucination, arising when continuations require\ndisclosure of beings in world, and residual reasoning hallucination, where\nmodels mimic inference by recycling traces of human reasoning in text. We\nillustrate these patterns through case studies aligned with Heideggerian\ncategories and an experiment across twelve LLMs showing how simulated\n\"self-preservation\" emerges under extended prompts. Our contribution is\nthreefold: (1) a comparative account showing why existing explanations are\ninsufficient; (2) a predictive taxonomy of hallucination linked to existential\nstructures with proposed benchmarks; and (3) design directions toward\n\"truth-constrained\" architectures capable of withholding or deferring when\ndisclosure is absent. We conclude that hallucination is not an incidental\ndefect but a defining limit of transformer-based models, an outcome scaffolding\ncan mask but never resolve."}
{"id": "2509.16325", "pdf": "https://arxiv.org/pdf/2509.16325", "abs": "https://arxiv.org/abs/2509.16325", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 1 figure", "summary": "Imagine AI assistants that enhance conversations without interrupting them:\nquietly providing relevant information during a medical consultation,\nseamlessly preparing materials as teachers discuss lesson plans, or\nunobtrusively scheduling meetings as colleagues debate calendars. While modern\nconversational LLM agents directly assist human users with tasks through a chat\ninterface, we study this alternative paradigm for interacting with LLM agents,\nwhich we call \"overhearing agents.\" Rather than demanding the user's attention,\noverhearing agents continuously monitor ambient activity and intervene only\nwhen they can provide contextual assistance. In this paper, we present the\nfirst analysis of overhearing LLM agents as a distinct paradigm in human-AI\ninteraction and establish a taxonomy of overhearing agent interactions and\ntasks grounded in a survey of works on prior LLM-powered agents and exploratory\nHCI studies. Based on this taxonomy, we create a list of best practices for\nresearchers and developers building overhearing agent systems. Finally, we\noutline the remaining research gaps and reveal opportunities for future\nresearch in the overhearing paradigm."}
{"id": "2509.16339", "pdf": "https://arxiv.org/pdf/2509.16339", "abs": "https://arxiv.org/abs/2509.16339", "authors": ["Josias K. Moukpe", "Philip K. Chan", "Ming Zhang"], "title": "Highly Imbalanced Regression with Tabular Data in SEP and Other Applications", "categories": ["cs.LG", "cs.AI"], "comment": "ICMLA 2025", "summary": "We investigate imbalanced regression with tabular data that have an imbalance\nratio larger than 1,000 (\"highly imbalanced\"). Accurately estimating the target\nvalues of rare instances is important in applications such as forecasting the\nintensity of rare harmful Solar Energetic Particle (SEP) events. For\nregression, the MSE loss does not consider the correlation between predicted\nand actual values. Typical inverse importance functions allow only convex\nfunctions. Uniform sampling might yield mini-batches that do not have rare\ninstances. We propose CISIR that incorporates correlation, Monotonically\nDecreasing Involution (MDI) importance, and stratified sampling. Based on five\ndatasets, our experimental results indicate that CISIR can achieve lower error\nand higher correlation than some recent methods. Also, adding our correlation\ncomponent to other recent methods can improve their performance. Lastly, MDI\nimportance can outperform other importance functions. Our code can be found in\nhttps://github.com/Machine-Earning/CISIR."}
{"id": "2509.16343", "pdf": "https://arxiv.org/pdf/2509.16343", "abs": "https://arxiv.org/abs/2509.16343", "authors": ["Chung-En", "Yu", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks."}
{"id": "2509.16345", "pdf": "https://arxiv.org/pdf/2509.16345", "abs": "https://arxiv.org/abs/2509.16345", "authors": ["Minxiao Wang", "Runze Yan", "Carol Li", "Saurabh Kataria", "Xiao Hu", "Matthew Clark", "Timothy Ruchti", "Timothy G. Buchman", "Sivasubramanium V Bhavani", "Randall J. Lee"], "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Clinical laboratory tests provide essential biochemical measurements for\ndiagnosis and treatment, but are limited by intermittent and invasive sampling.\nIn contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded\nsignal in intensive care units (ICUs) that reflects cardiovascular dynamics and\ncan serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a\nframework that combines a large-scale PPG foundation model for local waveform\nencoding with a patient-aware Mamba model for long-range temporal modeling. Our\narchitecture addresses three challenges: (1) capturing extended temporal trends\nin laboratory values, (2) accounting for patient-specific baseline variation\nvia FiLM-modulated initial states, and (3) performing multi-task estimation for\ninterrelated biomarkers. We evaluate our method on the two ICU datasets for\npredicting the five key laboratory tests. The results show substantial\nimprovements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$\namong most of the estimation targets. This work demonstrates the feasibility of\ncontinuous, personalized lab value estimation from routine PPG monitoring,\noffering a pathway toward non-invasive biochemical surveillance in critical\ncare."}
{"id": "2509.16346", "pdf": "https://arxiv.org/pdf/2509.16346", "abs": "https://arxiv.org/abs/2509.16346", "authors": ["Juan Castorena", "E. Louise Loudermilk", "Scott Pokswinski", "Rodman Linn"], "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments."}
{"id": "2509.16347", "pdf": "https://arxiv.org/pdf/2509.16347", "abs": "https://arxiv.org/abs/2509.16347", "authors": ["Alicia E. Boyd"], "title": "QUINTA: Reflexive Sensibility For Responsible AI Research and Data-Driven Processes", "categories": ["cs.SI", "cs.AI"], "comment": "14 pages, 5 figures, 1 Table, This paper was accepted as a poster\n  presentation at Equity and Access in Algorithms, Mechanisms, and Optimization\n  (EAAMO) Conference in 2023", "summary": "As the field of artificial intelligence (AI) and machine learning (ML)\ncontinues to prioritize fairness and the concern for historically marginalized\ncommunities, the importance of intersectionality in AI research has gained\nsignificant recognition. However, few studies provide practical guidance on how\nresearchers can effectively incorporate intersectionality into critical praxis.\nIn response, this paper presents a comprehensive framework grounded in critical\nreflexivity as intersectional praxis. Operationalizing intersectionality within\nthe AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative\nIntersectional Data (QUINTA) is introduced as a methodological paradigm that\nchallenges conventional and superficial research habits, particularly in\ndata-centric processes, to identify and mitigate negative impacts such as the\ninadvertent marginalization caused by these practices. The framework centers\nresearcher reflexivity to call attention to the AI researchers' power in\ncreating and analyzing AI/DS artifacts through data-centric approaches. To\nillustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher\ndemonstration utilizing the \\#metoo movement as a case study. Note: This paper\nwas accepted as a poster presentation at Equity and Access in Algorithms,\nMechanisms, and Optimization (EAAMO) Conference in 2023."}
{"id": "2509.16352", "pdf": "https://arxiv.org/pdf/2509.16352", "abs": "https://arxiv.org/abs/2509.16352", "authors": ["Yunfan Yang", "Jiarong Xu", "Hongzhe Zhang", "Xiao Fang"], "title": "Secure Confidential Business Information When Sharing Machine Learning Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Model-sharing offers significant business value by enabling firms with\nwell-established Machine Learning (ML) models to monetize and share their\nmodels with others who lack the resources to develop ML models from scratch.\nHowever, concerns over data confidentiality remain a significant barrier to\nmodel-sharing adoption, as Confidential Property Inference (CPI) attacks can\nexploit shared ML models to uncover confidential properties of the model\nprovider's private model training data. Existing defenses often assume that CPI\nattacks are non-adaptive to the specific ML model they are targeting. This\nassumption overlooks a key characteristic of real-world adversaries: their\nresponsiveness, i.e., adversaries' ability to dynamically adjust their attack\nmodels based on the information of the target and its defenses. To overcome\nthis limitation, we propose a novel defense method that explicitly accounts for\nthe responsive nature of real-world adversaries via two methodological\ninnovations: a novel Responsive CPI attack and an attack-defense arms race\nframework. The former emulates the responsive behaviors of adversaries in the\nreal world, and the latter iteratively enhances both the target and attack\nmodels, ultimately producing a secure ML model that is robust against\nresponsive CPI attacks. Furthermore, we propose and integrate a novel\napproximate strategy into our defense, which addresses a critical computational\nbottleneck of defense methods and improves defense efficiency. Through\nextensive empirical evaluations across various realistic model-sharing\nscenarios, we demonstrate that our method outperforms existing defenses by more\neffectively defending against CPI attacks, preserving ML model utility, and\nreducing computational overhead."}
{"id": "2509.16369", "pdf": "https://arxiv.org/pdf/2509.16369", "abs": "https://arxiv.org/abs/2509.16369", "authors": ["Akshay Govind Srinivasan", "Ryan Jacob George", "Jayden Koshy Joe", "Hrushikesh Kant", "Harshith M R", "Sachin Sundar", "Sudharshan Suresh", "Rahul Vimalkanth", "Vijayavallabh"], "title": "Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction", "categories": ["cs.IR", "cs.AI", "cs.CE", "H.4; H.5; H.3.3"], "comment": "14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the\n  proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025", "summary": "Accurate and reliable knowledge retrieval is vital for financial\nquestion-answering, where continually updated data sources and complex,\nhigh-stakes contexts demand precision. Traditional retrieval systems rely on a\nsingle database and retriever, but financial applications require more\nsophisticated approaches to handle intricate regulatory filings, market\nanalyses, and extensive multi-year reports. We introduce a framework for\nfinancial Retrieval Augmented Generation (RAG) that leverages agentic AI and\nthe Multi-HyDE system, an approach that generates multiple, nonequivalent\nqueries to boost the effectiveness and coverage of retrieval from large,\nstructured financial corpora. Our pipeline is optimized for token efficiency\nand multi-step financial reasoning, and we demonstrate that their combination\nimproves accuracy by 11.2% and reduces hallucinations by 15%. Our method is\nevaluated on standard financial QA benchmarks, showing that integrating\ndomain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,\nincluding keyword and table-based retrieval, significantly enhances both the\naccuracy and reliability of answers. This research not only delivers a modular,\nadaptable retrieval framework for finance but also highlights the importance of\nstructured agent workflows and multi-perspective retrieval for trustworthy\ndeployment of AI in high-stakes financial applications."}
{"id": "2509.16391", "pdf": "https://arxiv.org/pdf/2509.16391", "abs": "https://arxiv.org/abs/2509.16391", "authors": ["Yasser H. Khalil", "Mehdi Setayesh", "Hongliang Li"], "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness."}
{"id": "2509.16394", "pdf": "https://arxiv.org/pdf/2509.16394", "abs": "https://arxiv.org/abs/2509.16394", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Elena Hayoung Lee", "Gale Lucas"], "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Language Models (LLMs) are increasingly deployed in socially complex,\ninteraction-driven tasks, yet their ability to mirror human behavior in\nemotionally and strategically complex contexts remains underexplored. This\nstudy assesses the behavioral alignment of personality-prompted LLMs in\nadversarial dispute resolution by simulating multi-turn conflict dialogues that\nincorporate negotiation. Each LLM is guided by a matched Five-Factor\npersonality profile to control for individual variation and enhance realism. We\nevaluate alignment across three dimensions: linguistic style, emotional\nexpression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the\nclosest alignment with humans in linguistic style and emotional dynamics, while\nClaude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial\nalignment gaps persist. Our findings establish a benchmark for alignment\nbetween LLMs and humans in socially complex interactions, underscoring both the\npromise and the limitations of personality conditioning in dialogue modeling."}
{"id": "2509.16397", "pdf": "https://arxiv.org/pdf/2509.16397", "abs": "https://arxiv.org/abs/2509.16397", "authors": ["Taqiya Ehsan", "Shuren Xia", "Jorge Ortiz"], "title": "GRID: Graph-based Reasoning for Intervention and Discovery in Built Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per\nincident and achieves only 60 percent diagnostic accuracy, reflecting analytics\nthat stop at correlation instead of causation. To close this gap, we present\nGRID (Graph-based Reasoning for Intervention and Discovery), a three-stage\ncausal discovery pipeline that combines constraint-based search, neural\nstructural equation modeling, and language model priors to recover directed\nacyclic graphs from building sensor data. Across six benchmarks: synthetic\nrooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset,\nand a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00,\nwith exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden,\nPhysical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86\nin noisy conditions). The method outperforms ten baseline approaches across all\nevaluation scenarios. Intervention scheduling achieves low operational impact\nin most scenarios (cost <= 0.026) while reducing risk metrics compared to\nbaseline approaches. The framework integrates constraint-based methods, neural\narchitectures, and domain-specific language model prompts to address the\nobservational-causal gap in building analytics."}
{"id": "2509.16413", "pdf": "https://arxiv.org/pdf/2509.16413", "abs": "https://arxiv.org/abs/2509.16413", "authors": ["Richard Diehl Martinez", "David Demitri Africa", "Yuval Weiss", "Suchir Salhan", "Ryan Daniels", "Paula Buttery"], "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Building language models (LMs), especially small and medium ones, remains\nmore art than science. While large LMs often improve by sheer scale, it is\nstill unclear why many design choices work. For small LMs, this uncertainty is\nmore limiting: tight parameter budgets make each decision critical, yet\nresearchers still lack systematic, scientific ways to test and refine new\nideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic,\nhypothesis-driven research for small and medium-scale language model\ndevelopment. Pico consists of two libraries that together provide a practical\nsandbox where researchers can make targeted changes to a model's architecture\nor training procedures and directly observe their effects on the model's\nbehavior. To support reproducible experimentation, we also release a suite of\nbaseline models, pico-decoder, trained under standardized conditions and\nopen-sourced for the community. Case studies highlight how Pico can support\niterative small LM design and analysis."}
{"id": "2509.16418", "pdf": "https://arxiv.org/pdf/2509.16418", "abs": "https://arxiv.org/abs/2509.16418", "authors": ["Petr Grinberg", "Eric Bezzam", "Paolo Prandoni", "Martin Vetterli"], "title": "LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": "Submitted to ICASSP 2026", "summary": "With society's increasing reliance on digital data sharing, the protection of\nsensitive information has become critical. Encryption serves as one of the\nprivacy-preserving methods; however, its realization in the audio domain\npredominantly relies on signal processing or software methods embedded into\nhardware. In this paper, we introduce LenslessMic, a hybrid optical\nhardware-based encryption method that utilizes a lensless camera as a physical\nlayer of security applicable to multiple types of audio. We show that\nLenslessMic enables (1) robust authentication of audio recordings and (2)\nencryption strength that can rival the search space of 256-bit digital\nstandards, while maintaining high-quality signals and minimal loss of content\ninformation. The approach is validated with a low-cost Raspberry Pi prototype\nand is open-sourced together with datasets to facilitate research in the area."}
{"id": "2509.16421", "pdf": "https://arxiv.org/pdf/2509.16421", "abs": "https://arxiv.org/abs/2509.16421", "authors": ["Aiden Chang", "Celso De Melo", "Stephanie M. Lukin"], "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at NeurIPS 2025, 32 pages, 5 figures", "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding."}
{"id": "2509.16437", "pdf": "https://arxiv.org/pdf/2509.16437", "abs": "https://arxiv.org/abs/2509.16437", "authors": ["Jina Suh", "Lindy Le", "Erfan Shayegani", "Gonzalo Ramos", "Judith Amores", "Desmond C. Ong", "Mary Czerwinski", "Javier Hernandez"], "title": "SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy in Sustained Human-AI Conversations", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Empathy is increasingly recognized as a key factor in human-AI communication,\nyet conventional approaches to \"digital empathy\" often focus on simulating\ninternal, human-like emotional states while overlooking the inherently\nsubjective, contextual, and relational facets of empathy as perceived by users.\nIn this work, we propose a human-centered taxonomy that emphasizes observable\nempathic behaviors and introduce a new dataset, Sense-7, of real-world\nconversations between information workers and Large Language Models (LLMs),\nwhich includes per-turn empathy annotations directly from the users, along with\nuser characteristics, and contextual details, offering a more user-grounded\nrepresentation of empathy. Analysis of 695 conversations from 109 participants\nreveals that empathy judgments are highly individualized, context-sensitive,\nand vulnerable to disruption when conversational continuity fails or user\nexpectations go unmet. To promote further research, we provide a subset of 672\nanonymized conversation and provide exploratory classification analysis,\nshowing that an LLM-based classifier can recognize 5 levels of empathy with an\nencouraging average Spearman $\\rho$=0.369 and Accuracy=0.487 over this set.\nOverall, our findings underscore the need for AI designs that dynamically\ntailor empathic behaviors to user contexts and goals, offering a roadmap for\nfuture research and practical development of socially attuned, human-centered\nartificial agents."}
{"id": "2509.16443", "pdf": "https://arxiv.org/pdf/2509.16443", "abs": "https://arxiv.org/abs/2509.16443", "authors": ["Ryan Tomich", "Zhizhen Zhong", "Dirk Englund"], "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems", "categories": ["physics.app-ph", "cs.AI", "cs.PL"], "comment": "9 pages, 8 figures", "summary": "The growing demand for low-latency, energy-efficient inference in large\nlanguage models (LLMs) has catalyzed interest in heterogeneous architectures.\nWhile GPUs remain dominant, they are poorly suited for integration with\nemerging domain-specific accelerators like the Photonic Tensor Units (PTUs),\nwhich offer low-power, high-throughput linear computation. This motivates\nhybrid compilation strategies that combine photonic and electronic resources.\nWe present LightCode, a compiler framework and simulator for mapping LLM\ninference workloads across hybrid photonic-electronic systems. LightCode\nintroduces the Stacked Graph, an intermediate representation that encodes\nmultiple hardware-specific realizations of each tensor operation. Hardware\nassignment is formulated as a constrained subgraph selection problem optimized\nfor latency or energy under parametric cost models. We evaluate LightCode on\nthe prefill stage of GPT-2 and Llama-7B showing that under our workload and\nhardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our\nsimulated workloads at maximum sequence length; (ii) multiplexing and\nassignment strategy yielded latency improvements exceeding 10x; and (iii)\nOptimizing for latency or energy resulted in distinct hardware mappings in our\nsimulations. LightCode offers a module, foundational framework and simulator\nfor compiling LLMs to emerging photonic accelerators."}
{"id": "2509.16449", "pdf": "https://arxiv.org/pdf/2509.16449", "abs": "https://arxiv.org/abs/2509.16449", "authors": ["Tsz Fung Pang", "Maryam Berijanian", "Thomas Orth", "Breanna Shi", "Charlotte S. Alexander"], "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Legal documents are often long, dense, and difficult to comprehend, not only\nfor laypeople but also for legal experts. While automated document\nsummarization has great potential to improve access to legal knowledge,\nprevailing task-based evaluators overlook divergent user and stakeholder needs.\nTool development is needed to encompass the technicality of a case summary for\na litigator yet be accessible for a self-help public researching for their\nlawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation\nframework that scores summaries through the lens of six personas, including\nlegal and non-legal users. We also introduce a controlled dimension-shifted\npilot dataset of U.S. civil rights case summaries that varies along depth,\naccessibility, and procedural detail as well as Diversity-Coverage Index (DCI)\nto expose divergent optima of legal summary between persona-aware and\npersona-agnostic judges. This work enables refinement of legal AI summarization\nsystems for both expert and non-expert users, with the potential to increase\naccess to legal knowledge. The code base and data are publicly available in\nGitHub."}
{"id": "2509.16452", "pdf": "https://arxiv.org/pdf/2509.16452", "abs": "https://arxiv.org/abs/2509.16452", "authors": ["Son Hai Nguyen", "Diwei Wang", "Jinhyeok Jang", "Hyewon Seo"], "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision."}
{"id": "2509.16454", "pdf": "https://arxiv.org/pdf/2509.16454", "abs": "https://arxiv.org/abs/2509.16454", "authors": ["Devin Lange", "Shanghua Gao", "Pengwei Sui", "Austen Money", "Priya Misner", "Marinka Zitnik", "Nils Gehlenborg"], "title": "A Generative AI System for Biomedical Data Discovery with Grammar-Based Visualizations", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "We explore the potential for combining generative AI with grammar-based\nvisualizations for biomedical data discovery. In our prototype, we use a\nmulti-agent system to generate visualization specifications and apply filters.\nThese visualizations are linked together, resulting in an interactive dashboard\nthat is progressively constructed. Our system leverages the strengths of\nnatural language while maintaining the utility of traditional user interfaces.\nFurthermore, we utilize generated interactive widgets enabling user adjustment.\nFinally, we demonstrate the potential utility of this system for biomedical\ndata discovery with a case study."}
{"id": "2509.16457", "pdf": "https://arxiv.org/pdf/2509.16457", "abs": "https://arxiv.org/abs/2509.16457", "authors": ["Yunzhe Wang", "Gale M. Lucas", "Burcin Becerik-Gerber", "Volkan Ustun"], "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025), Main Conference", "summary": "Language-driven generative agents have enabled large-scale social simulations\nwith transformative uses, from interpersonal training to aiding global\npolicy-making. However, recent studies indicate that generative agent behaviors\noften deviate from expert expectations and real-world data--a phenomenon we\nterm the Behavior-Realism Gap. To address this, we introduce a theoretical\nframework called Persona-Environment Behavioral Alignment (PEBA), formulated as\na distribution matching problem grounded in Lewin's behavior equation stating\nthat behavior is a function of the person and their environment. Leveraging\nPEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that\niteratively refines agent personas, implicitly aligning their collective\nbehaviors with realistic expert benchmarks within a specified environmental\ncontext. We validate PEvo in an active shooter incident simulation we\ndeveloped, achieving an 84% average reduction in distributional divergence\ncompared to no steering and a 34% improvement over explicit instruction\nbaselines. Results also show PEvo-refined personas generalize to novel, related\nsimulation scenarios. Our method greatly enhances behavioral realism and\nreliability in high-stakes social simulations. More broadly, the PEBA-PEvo\nframework provides a principled approach to developing trustworthy LLM-driven\nsocial simulations."}
{"id": "2509.16463", "pdf": "https://arxiv.org/pdf/2509.16463", "abs": "https://arxiv.org/abs/2509.16463", "authors": ["Spencer Compton", "Kristjan Greenewald", "Dmitriy Katz", "Murat Kocaoglu"], "title": "Entropic Causal Inference: Graph Identifiability", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ICML 2022. This version corrects a bug in semi-synthetic\n  experiments", "summary": "Entropic causal inference is a recent framework for learning the causal graph\nbetween two variables from observational data by finding the\ninformation-theoretically simplest structural explanation of the data, i.e.,\nthe model with smallest entropy. In our work, we first extend the causal graph\nidentifiability result in the two-variable setting under relaxed assumptions.\nWe then show the first identifiability result using the entropic approach for\nlearning causal graphs with more than two nodes. Our approach utilizes the\nproperty that ancestrality between a source node and its descendants can be\ndetermined using the bivariate entropic tests. We provide a sound sequential\npeeling algorithm for general graphs that relies on this property. We also\npropose a heuristic algorithm for small graphs that shows strong empirical\nperformance. We rigorously evaluate the performance of our algorithms on\nsynthetic data generated from a variety of models, observing improvement over\nprior work. Finally we test our algorithms on real-world datasets."}
{"id": "2509.16479", "pdf": "https://arxiv.org/pdf/2509.16479", "abs": "https://arxiv.org/abs/2509.16479", "authors": ["Christopher Silver", "Thangarajah Akilan"], "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Falls among seniors are a major public health issue. Existing solutions using\nwearable sensors, ambient sensors, and RGB-based vision systems face challenges\nin reliability, user compliance, and practicality. Studies indicate that\nstakeholders, such as older adults and eldercare facilities, prefer\nnon-wearable, passive, privacy-preserving, and real-time fall detection systems\nthat require no user interaction. This study proposes an advanced thermal fall\ndetection method using a Bidirectional Convolutional Long Short-Term Memory\n(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general\nattention mechanisms. Through systematic experimentation across hundreds of\nmodel variations exploring the integration of attention mechanisms, recurrent\nmodules, and motion flow, we identified top-performing architectures. Among\nthem, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of\n$99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly\nemerged, diverse, and privacy-preserving benchmark. These results highlight the\ngeneralizability and practicality of the proposed model, setting new standards\nfor thermal fall detection and paving the way toward deployable,\nhigh-performance solutions."}
{"id": "2509.16487", "pdf": "https://arxiv.org/pdf/2509.16487", "abs": "https://arxiv.org/abs/2509.16487", "authors": ["Zixun Chen", "Petr Babkin", "Akshat Gupta", "Gopala Anumanchipalli", "Xiaomo Liu"], "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue is one of the landmark abilities of large language models (LLMs).\nDespite its ubiquity, few studies actually distinguish specific ingredients\nunderpinning dialogue behavior emerging during post-training. We employ a\ncomprehensive suite of model-based metrics, each targeting a distinct\nfine-grained aspect of dialogue, motivated by linguistic theory. We evaluate\nhow the performance of pre-trained Pythia models changes with respect to each\nof those dimensions, depending on model size and as a result of supervised\nfine-tuning on conversational datasets. We observe only a mild impact of raw\nmodel size on most metrics, whereas fine-tuning quickly saturates the scores\nfor all but the smallest models tested. Somewhat contrary to our expectations,\nmany metrics show very similar trends, especially if they are all rooted in the\nsame evaluator model, which raises the question of their reliability in\nmeasuring a specific dimension. To that end, we conduct additional analyses of\nscore distributions, metric correlations, and term frequencies in generated\nresponses to help explain our observations."}
{"id": "2509.16494", "pdf": "https://arxiv.org/pdf/2509.16494", "abs": "https://arxiv.org/abs/2509.16494", "authors": ["Fengyuan Liu", "Rui Zhao", "Shuo Chen", "Guohao Li", "Philip Torr", "Lei Han", "Jindong Gu"], "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Individual Large Language Models (LLMs) have demonstrated significant\ncapabilities across various domains, such as healthcare and law. Recent studies\nalso show that coordinated multi-agent systems exhibit enhanced decision-making\nand reasoning abilities through collaboration. However, due to the\nvulnerabilities of individual LLMs and the difficulty of accessing all agents\nin a multi-agent system, a key question arises: If attackers only know one\nagent, could they still generate adversarial samples capable of misleading the\ncollective decision? To explore this question, we formulate it as a game with\nincomplete information, where attackers know only one target agent and lack\nknowledge of the other agents in the system. With this formulation, we propose\nM-Spoiler, a framework that simulates agent interactions within a multi-agent\nsystem to generate adversarial samples. These samples are then used to\nmanipulate the target agent in the target system, misleading the system's\ncollaborative decision-making process. More specifically, M-Spoiler introduces\na stubborn agent that actively aids in optimizing adversarial samples by\nsimulating potential stubborn responses from agents in the target system. This\nenhances the effectiveness of the generated adversarial samples in misleading\nthe system. Through extensive experiments across various tasks, our findings\nconfirm the risks posed by the knowledge of an individual agent in multi-agent\nsystems and demonstrate the effectiveness of our framework. We also explore\nseveral defense mechanisms, showing that our proposed attack framework remains\nmore potent than baselines, underscoring the need for further research into\ndefensive strategies."}
{"id": "2509.16496", "pdf": "https://arxiv.org/pdf/2509.16496", "abs": "https://arxiv.org/abs/2509.16496", "authors": ["Seyyedali Hosseinalipour", "Shimiao Li", "Adedoyin Inaolaji", "Filippo Malandra", "Luis Herrera", "Nicholas Mastronarde"], "title": "Synergies between Federated Foundation Models and Smart Power Grids", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "The recent emergence of large language models (LLMs) such as GPT-3 has marked\na significant paradigm shift in machine learning. Trained on massive corpora of\ndata, these models demonstrate remarkable capabilities in language\nunderstanding, generation, summarization, and reasoning, transforming how\nintelligent systems process and interact with human language. Although LLMs may\nstill seem like a recent breakthrough, the field is already witnessing the rise\nof a new and more general category: multi-modal, multi-task foundation models\n(M3T FMs). These models go beyond language and can process heterogeneous data\ntypes/modalities, such as time-series measurements, audio, imagery, tabular\nrecords, and unstructured logs, while supporting a broad range of downstream\ntasks spanning forecasting, classification, control, and retrieval. When\ncombined with federated learning (FL), they give rise to M3T Federated\nFoundation Models (FedFMs): a highly recent and largely unexplored class of\nmodels that enable scalable, privacy-preserving model training/fine-tuning\nacross distributed data sources. In this paper, we take one of the first steps\ntoward introducing these models to the power systems research community by\noffering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii)\nsmart grids for FedFMs. In the former, we explore how M3T FedFMs can enhance\nkey grid functions, such as load/demand forecasting and fault detection, by\nlearning from distributed, heterogeneous data available at the grid edge in a\nprivacy-preserving manner. In the latter, we investigate how the constraints\nand structure of smart grids, spanning energy, communication, and regulatory\ndimensions, shape the design, training, and deployment of M3T FedFMs."}
{"id": "2509.16517", "pdf": "https://arxiv.org/pdf/2509.16517", "abs": "https://arxiv.org/abs/2509.16517", "authors": ["Burak Satar", "Zhixin Ma", "Patrick A. Irawan", "Wilfried A. Mulyawan", "Jing Jiang", "Ee-Peng Lim", "Chong-Wah Ngo"], "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Accepted to EMNLP 2025 Main Conference,\n  https://seeingculture-benchmark.github.io/", "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture"}
{"id": "2509.16525", "pdf": "https://arxiv.org/pdf/2509.16525", "abs": "https://arxiv.org/abs/2509.16525", "authors": ["Anna Mazhar", "Sainyam Galhotra"], "title": "Causal Fuzzing for Verifying Machine Unlearning", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "As machine learning models become increasingly embedded in decision-making\nsystems, the ability to \"unlearn\" targeted data or features is crucial for\nenhancing model adaptability, fairness, and privacy in models which involves\nexpensive training. To effectively guide machine unlearning, a thorough testing\nis essential. Existing methods for verification of machine unlearning provide\nlimited insights, often failing in scenarios where the influence is indirect.\nIn this work, we propose CAF\\'E, a new causality based framework that unifies\ndatapoint- and feature-level unlearning for verification of black-box ML\nmodels. CAF\\'E evaluates both direct and indirect effects of unlearning targets\nthrough causal dependencies, providing actionable insights with fine-grained\nanalysis. Our evaluation across five datasets and three model architectures\ndemonstrates that CAF\\'E successfully detects residual influence missed by\nbaselines while maintaining computational efficiency."}
{"id": "2509.16527", "pdf": "https://arxiv.org/pdf/2509.16527", "abs": "https://arxiv.org/abs/2509.16527", "authors": ["Guangze Zheng", "Shijie Lin", "Haobo Zuo", "Si Si", "Ming-Shan Wang", "Changhong Fu", "Jia Pan"], "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/", "summary": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world\npixel dynamicity for visual tracking. LBM decomposes visual representations\ninto dynamic pixel lattices and solves pixel motion states through\ncollision-streaming processes. Specifically, the high-dimensional distribution\nof the target pixels is acquired through a multilayer predict-update network to\nestimate the pixel positions and visibility. The predict stage formulates\nlattice collisions among the spatial neighborhood of target pixels and develops\nlattice streaming within the temporal visual context. The update stage\nrectifies the pixel distributions with online visual representations. Compared\nwith existing methods, LBM demonstrates practical applicability in an online\nand real-time manner, which can efficiently adapt to real-world visual tracking\ntasks. Comprehensive evaluations of real-world point tracking benchmarks such\nas TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of\nlarge-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B\nfurther demonstrates LBM's real-world practicality."}
{"id": "2509.16530", "pdf": "https://arxiv.org/pdf/2509.16530", "abs": "https://arxiv.org/abs/2509.16530", "authors": ["Wei Xie", "Shuoyoucheng Ma", "Zhenhua Wang", "Enze Wang", "Kai Chen", "Xiaobing Sun", "Baosheng Wang"], "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans", "categories": ["cs.CL", "cs.AI"], "comment": "Thank you for your attention. This paper was accepted by the CogSci\n  2025 conference in April and published in August. The location in the\n  proceedings is: https://escholarship.org/uc/item/39k8f46q", "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\nexhibited human-like intelligence by learning from vast amounts of\ninternet-scale data. However, the uninterpretability of large-scale neural\nnetworks raises concerns about the reliability of LLM. Studies have attempted\nto assess the psychometric properties of LLMs by borrowing concepts from human\npsychology to enhance their interpretability, but they fail to account for the\nfundamental differences between LLMs and humans. This results in high rejection\nrates when human scales are reused directly. Furthermore, these scales do not\nsupport the measurement of LLM psychological property variations in different\nlanguages. This paper introduces AIPsychoBench, a specialized benchmark\ntailored to assess the psychological properties of LLM. It uses a lightweight\nrole-playing prompt to bypass LLM alignment, improving the average effective\nresponse rate from 70.12% to 90.40%. Meanwhile, the average biases are only\n3.3% (positive) and 2.1% (negative), which are significantly lower than the\nbiases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.\nFurthermore, among the total of 112 psychometric subcategories, the score\ndeviations for seven languages compared to English ranged from 5% to 20.2% in\n43 subcategories, providing the first comprehensive evidence of the linguistic\nimpact on the psychometrics of LLM."}
{"id": "2509.16532", "pdf": "https://arxiv.org/pdf/2509.16532", "abs": "https://arxiv.org/abs/2509.16532", "authors": ["Run Yu", "Yangdi Liu", "Wen-Da Wei", "Chen Li"], "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recently,vision-based robotic manipulation has garnered significant attention\nand witnessed substantial advancements. 2D image-based and 3D point cloud-based\npolicy learning represent two predominant paradigms in the field, with recent\nstudies showing that the latter consistently outperforms the former in terms of\nboth policy performance and generalization, thereby underscoring the value and\nsignificance of 3D information. However, 3D point cloud-based approaches face\nthe significant challenge of high data acquisition costs, limiting their\nscalability and real-world deployment. To address this issue, we propose a\nnovel framework NoReal3D: which introduces the 3DStructureFormer, a learnable\n3D perception module capable of transforming monocular images into\ngeometrically meaningful pseudo-point cloud features, effectively fused with\nthe 2D encoder output features. Specially, the generated pseudo-point clouds\nretain geometric and topological structures so we design a pseudo-point cloud\nencoder to preserve these properties, making it well-suited for our framework.\nWe also investigate the effectiveness of different feature fusion\nstrategies.Our framework enhances the robot's understanding of 3D spatial\nstructures while completely eliminating the substantial costs associated with\n3D point cloud acquisition.Extensive experiments across various tasks validate\nthat our framework can achieve performance comparable to 3D point cloud-based\nmethods, without the actual point cloud data."}
{"id": "2509.16534", "pdf": "https://arxiv.org/pdf/2509.16534", "abs": "https://arxiv.org/abs/2509.16534", "authors": ["Cheng Jiayang", "Qianqian Zhuang", "Haoran Li", "Chunkit Chan", "Xin Liu", "Lin Qiu", "Yangqiu Song"], "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Grounding large language models (LLMs) in external knowledge sources is a\npromising method for faithful prediction. While existing grounding approaches\nwork well for simple queries, many real-world information needs require\nsynthesizing multiple pieces of evidence. We introduce \"integrative grounding\"\n-- the challenge of retrieving and verifying multiple inter-dependent pieces of\nevidence to support a hypothesis query. To systematically study this problem,\nwe repurpose data from four domains for evaluating integrative grounding\ncapabilities. Our investigation reveals two critical findings: First, in\ngroundedness verification, while LLMs are robust to redundant evidence, they\ntend to rationalize using internal knowledge when information is incomplete.\nSecond, in examining retrieval planning strategies, we find that undirected\nplanning can degrade performance through noise introduction, while premise\nabduction emerges as a promising approach due to its logical constraints.\nAdditionally, LLMs' zero-shot self-reflection capabilities consistently improve\ngrounding quality. These insights provide valuable direction for developing\nmore effective integrative grounding systems."}
{"id": "2509.16546", "pdf": "https://arxiv.org/pdf/2509.16546", "abs": "https://arxiv.org/abs/2509.16546", "authors": ["Ashley Kurian", "Aydin Aysu"], "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks", "categories": ["cs.CR", "cs.AI", "F.2.2, I.2.7"], "comment": "18 pages, 3 Figures", "summary": "Neural networks are valuable intellectual property due to the significant\ncomputational cost, expert labor, and proprietary data involved in their\ndevelopment. Consequently, protecting their parameters is critical not only for\nmaintaining a competitive advantage but also for enhancing the model's security\nand privacy. Prior works have demonstrated the growing capability of\ncryptanalytic attacks to scale to deeper models. In this paper, we present the\nfirst defense mechanism against cryptanalytic parameter extraction attacks. Our\nkey insight is to eliminate the neuron uniqueness necessary for these attacks\nto succeed. We achieve this by a novel, extraction-aware training method.\nSpecifically, we augment the standard loss function with an additional\nregularization term that minimizes the distance between neuron weights within a\nlayer. Therefore, the proposed defense has zero area-delay overhead during\ninference. We evaluate the effectiveness of our approach in mitigating\nextraction attacks while analyzing the model accuracy across different\narchitectures and datasets. When re-trained with the same model architecture,\nthe results show that our defense incurs a marginal accuracy change of less\nthan 1% with the modified loss function. Moreover, we present a theoretical\nframework to quantify the success probability of the attack. When tested\ncomprehensively with prior attack settings, our defense demonstrated empirical\nsuccess for sustained periods of extraction, whereas unprotected networks are\nextracted between 14 minutes to 4 hours."}
{"id": "2509.16550", "pdf": "https://arxiv.org/pdf/2509.16550", "abs": "https://arxiv.org/abs/2509.16550", "authors": ["Yinghao Wu", "Shuhong Hou", "Haowen Zheng", "Yichen Li", "Weiyi Lu", "Xun Zhou", "Yitian Shao"], "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 7 figures", "summary": "Robotic manipulation tasks such as inserting a key into a lock or plugging a\nUSB device into a port can fail when visual perception is insufficient to\ndetect misalignment. In these situations, touch sensing is crucial for the\nrobot to monitor the task's states and make precise, timely adjustments.\nCurrent touch sensing solutions are either insensitive to detect subtle changes\nor demand excessive sensor data. Here, we introduce TranTac, a data-efficient\nand low-cost tactile sensing and control framework that integrates a single\ncontact-sensitive 6-axis inertial measurement unit within the elastomeric tips\nof a robotic gripper for completing fine insertion tasks. Our customized\nsensing system can detect dynamic translational and torsional deformations at\nthe micrometer scale, enabling the tracking of visually imperceptible pose\nchanges of the grasped object. By leveraging transformer-based encoders and\ndiffusion policy, TranTac can imitate human insertion behaviors using transient\ntactile cues detected at the gripper's tip during insertion processes. These\ncues enable the robot to dynamically control and correct the 6-DoF pose of the\ngrasped object. When combined with vision, TranTac achieves an average success\nrate of 79% on object grasping and insertion tasks, outperforming both\nvision-only policy and the one augmented with end-effector 6D force/torque\nsensing. Contact localization performance is also validated through\ntactile-only misaligned insertion tasks, achieving an average success rate of\n88%. We assess the generalizability by training TranTac on a single prism-slot\npair and testing it on unseen data, including a USB plug and a metal key, and\nfind that the insertion tasks can still be completed with an average success\nrate of nearly 70%. The proposed framework may inspire new robotic tactile\nsensing systems for delicate manipulation tasks."}
{"id": "2509.16551", "pdf": "https://arxiv.org/pdf/2509.16551", "abs": "https://arxiv.org/abs/2509.16551", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Rethinking the Role of Text Complexity in Language Model Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in BabyLM Workshop at EMNLP 2025", "summary": "Improving pretraining data quality and size is known to boost downstream\nperformance, but the role of text complexity is less explored. Text complexity\nrefers to how hard a text is to read, and is typically estimated from surface\ncues such as sentence length, word choice, and sentence structure. We reduce\nsurface-level complexity--shorter sentences, simpler words, simpler\nstructure--while keeping core text content close to constant, and ask: (1) How\ndoes complexity affect language modeling across model sizes? (2) Can useful\nrepresentations be learned from simpler text alone? (3) How does pretraining\ntext complexity influence downstream language understanding? To answer these\nquestions, we simplify human-written texts using a large language model, then\npretrain causal models (28M-500M) from scratch on both original and simplified\ndata, and evaluate them in finetuning and zero-shot setups. We find that\nperplexity is sensitive to the interaction between model capacity and text\ncomplexity--smaller models degrade far less on simpler texts--while text\ncomplexity has little impact on finetuning evaluations, with zero-shot\nevaluations indicating that simpler texts benefit performance on linguistic\nknowledge tasks, whereas more complex texts favor tasks requiring world\nknowledge and entity tracking."}
{"id": "2509.16567", "pdf": "https://arxiv.org/pdf/2509.16567", "abs": "https://arxiv.org/abs/2509.16567", "authors": ["Nikolaos Spanos", "Maria Lymperaiou", "Giorgos Filandrianos", "Konstantinos Thomas", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in NeurIPS 2025", "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation."}
{"id": "2509.16584", "pdf": "https://arxiv.org/pdf/2509.16584", "abs": "https://arxiv.org/abs/2509.16584", "authors": ["Benlu Wang", "Iris Xia", "Yifan Zhang", "Junda Wang", "Feiyun Ouyang", "Shuo Han", "Arman Cohan", "Hong Yu", "Zonghai Yao"], "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear as an Oral\n  presentation in the proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2025", "summary": "Large language models (LLMs) have demonstrated promising performance on\nmedical benchmarks; however, their ability to perform medical calculations, a\ncrucial aspect of clinical decision-making, remains underexplored and poorly\nevaluated. Existing benchmarks often assess only the final answer with a wide\nnumerical tolerance, overlooking systematic reasoning failures and potentially\ncausing serious clinical misjudgments. In this work, we revisit medical\ncalculation evaluation with a stronger focus on clinical trustworthiness.\nFirst, we clean and restructure the MedCalc-Bench dataset and propose a new\nstep-by-step evaluation pipeline that independently assesses formula selection,\nentity extraction, and arithmetic computation. Under this granular framework,\nthe accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by\nprior evaluations. Second, we introduce an automatic error analysis framework\nthat generates structured attribution for each failure mode. Human evaluation\nconfirms its alignment with expert judgment, enabling scalable and explainable\ndiagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that\ncombines retrieval-augmented generation and Python-based code execution.\nWithout any fine-tuning, MedRaC improves the accuracy of different LLMs from\n16.35% up to 53.19%. Our work highlights the limitations of current benchmark\npractices and proposes a more clinically faithful methodology. By enabling\ntransparent and transferable reasoning evaluation, we move closer to making\nLLM-based systems trustworthy for real-world medical applications."}
{"id": "2509.16588", "pdf": "https://arxiv.org/pdf/2509.16588", "abs": "https://arxiv.org/abs/2509.16588", "authors": ["Haiming Zhang", "Yiyao Zhu", "Wending Zhou", "Xu Yan", "Yingjie Cai", "Bingbing Liu", "Shuguang Cui", "Zhen Li"], "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025 (Spotlight)", "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection)."}
{"id": "2509.16589", "pdf": "https://arxiv.org/pdf/2509.16589", "abs": "https://arxiv.org/abs/2509.16589", "authors": ["Qiongqiong Wang", "Hardik Bhupendra Sailor", "Tianchi Liu", "Wenyu Zhang", "Muhammad Huzaifah", "Nattadaporn Lertcheva", "Shuo Sun", "Nancy F. Chen", "Jinyang Wu", "AiTi Aw"], "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP Findings 2025", "summary": "Recent speech-LLMs have shown impressive performance in tasks like\ntranscription and translation, yet they remain limited in understanding the\nparalinguistic aspects of speech crucial for social and emotional intelligence.\nWe propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual\nparalinguistic reasoning the integration of verbal content with non-verbal cues\nlike emotion and prosody. The benchmark includes two curated question answering\n(QA) datasets requiring both linguistic and empathetic understanding. We\nevaluate state-of-the-art speech-LLMs from both open and closed-source models\nand perform a comprehensive analysis across different question types. The top\ntwo models were further analyzed under temperature tuning to understand its\neffect on this task. Our benchmark reveals a key gap in existing evaluations\nand offers insights into building more context-aware and emotionally\nintelligent speech-capable LLMs."}
{"id": "2509.16596", "pdf": "https://arxiv.org/pdf/2509.16596", "abs": "https://arxiv.org/abs/2509.16596", "authors": ["Junjie Ye", "Yuming Yang", "Yang Nan", "Shuo Li", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan"], "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Main Conference. arXiv admin note: text\n  overlap with arXiv:2409.15825", "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge."}
{"id": "2509.16598", "pdf": "https://arxiv.org/pdf/2509.16598", "abs": "https://arxiv.org/abs/2509.16598", "authors": ["Byeongho Yu", "Changhun Lee", "Jungyu Jin", "Eunhyeok Park"], "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To mitigate the hallucination problem in large language models, DoLa exploits\nearly exit logits from the same model as a contrastive prior. However, we found\nthat these early exit logits tend to be flat, low in magnitude, and fail to\nreflect meaningful contrasts. To address this, we propose PruneCD, a novel\ncontrastive decoding method that constructs the amateur model via layer pruning\nrather than early exit. This design leads to more informative and well-aligned\nlogits, enabling more effective contrastive decoding. Through qualitative and\nquantitative analyses, we demonstrate that PruneCD consistently improves\nfactuality with minimal inference overhead, offering a robust and practical\napproach to mitigating hallucinations in LLMs."}
{"id": "2509.16602", "pdf": "https://arxiv.org/pdf/2509.16602", "abs": "https://arxiv.org/abs/2509.16602", "authors": ["Minji Heo", "Simon S. Woo"], "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-step or hybrid deepfakes, created by sequentially applying different\ndeepfake creation methods such as Face-Swapping, GAN-based generation, and\nDiffusion methods, can pose an emerging and unforseen technical challenge for\ndetection models trained on single-step forgeries. While prior studies have\nmainly focused on detecting isolated single manipulation, little is known about\nthe detection model behavior under such compositional, hybrid, and complex\nmanipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a\nlarge-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using\nfive state-of-the-art representative generators. Using this approach, we\nanalyze detection performance and spectral properties across hybrid\nmanipulation at different step, along with varying generator combinations and\nquality settings. Surprisingly, our findings reveal that detection performance\nhighly depends on the final manipulation type, with F1-score dropping by up to\n\\textbf{58.83\\%} when it differs from training distribution. This clearly\ndemonstrates that detectors rely on last-stage artifacts rather than cumulative\nmanipulation traces, limiting generalization. Such findings highlight the need\nfor detection models to explicitly consider manipulation history and sequences.\nOur results highlight the importance of benchmarks such as FakeChain,\nreflecting growing synthesis complexity and diversity in real-world scenarios.\nOur sample code is available\nhere\\footnote{https://github.com/minjihh/FakeChain}."}
{"id": "2509.16617", "pdf": "https://arxiv.org/pdf/2509.16617", "abs": "https://arxiv.org/abs/2509.16617", "authors": ["David Kreismann"], "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model", "categories": ["cs.CV", "cs.AI", "I.2.6; I.5.4; I.6.8"], "comment": "12 pages, 4 figures, to appear in GI LNI (SKILL 2025)", "summary": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C."}
{"id": "2509.16618", "pdf": "https://arxiv.org/pdf/2509.16618", "abs": "https://arxiv.org/abs/2509.16618", "authors": ["Pengfei Hao", "Hongqiu Wang", "Shuaibo Li", "Zhaohu Xing", "Guang Yang", "Kaishun Wu", "Lei Zhu"], "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Early accepted by MICCAI2025", "summary": "In recent years, Visual Question Localized-Answering in robotic surgery\n(Surgical-VQLA) has gained significant attention for its potential to assist\nmedical students and junior doctors in understanding surgical scenes. Recently,\nthe rapid development of Large Language Models (LLMs) has provided more\npromising solutions for this task. However, current methods struggle to\nestablish complex dependencies between text and visual details, and have\ndifficulty perceiving the spatial information of surgical scenes. To address\nthese challenges, we propose a novel method, Surgical-MambaLLM, which is the\nfirst to combine Mamba2 with LLM in the surgical domain, that leverages\nMamba2's ability to effectively capture cross-modal dependencies and perceive\nspatial information in surgical scenes, thereby enhancing the LLMs'\nunderstanding of surgical images. Specifically, we propose the Cross-modal\nBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective\nmultimodal fusion, with its cross-modal integration capabilities. Additionally,\ntailored to the geometric characteristics of surgical scenes, we design the\nSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the\nsurgical images, enhancing the model's spatial understanding of the surgical\nscene. Extensive experiments demonstrate that our Surgical-MambaLLM model\noutperforms the state-of-the-art methods on the EndoVis17-VQLA and\nEndoVis18-VQLA datasets, significantly improving the performance of the\nSurgical-VQLA task."}
{"id": "2509.16622", "pdf": "https://arxiv.org/pdf/2509.16622", "abs": "https://arxiv.org/abs/2509.16622", "authors": ["Mengqi Wang", "Zhan Liu", "Zengrui Jin", "Guangzhi Sun", "Chao Zhang", "Philip C. Woodland"], "title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Diffusion-based large language models (DLLMs) have recently attracted growing\ninterest as an alternative to autoregressive decoders. In this work, we present\nan empirical study on using the diffusion-based large language model LLaDA for\nautomatic speech recognition (ASR). We first investigate its use as an external\ndeliberation-based processing module for Whisper-LLaMA transcripts. By\nleveraging the bidirectional attention and denoising capabilities of LLaDA, we\nexplore random masking, low-confidence masking, and semi-autoregressive\nstrategies, showing that Whisper-LLaDA substantially reduces WER compared with\nthe baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER\non test-clean/test-other, representing a 12.3% relative improvement over the\nWhisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA\nwithout acoustic features fails to improve accuracy, highlighting the\nimportance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA\nas a standalone decoder for ASR with diffusion-based and semi-autoregressive\ndecoding. Most experimental configurations achieve faster inference than the\nWhisper-LLaMA baseline, although recognition accuracy is slightly lower. These\nfindings offer an empirical view of diffusion-based LLMs for ASR and point to\npromising directions for improvements."}
{"id": "2509.16633", "pdf": "https://arxiv.org/pdf/2509.16633", "abs": "https://arxiv.org/abs/2509.16633", "authors": ["Abhirama Subramanyam Penamakuri", "Navlika Singh", "Piyush Arora", "Anand Mishra"], "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to EMNLP (Main) 2025", "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available."}
{"id": "2509.16638", "pdf": "https://arxiv.org/pdf/2509.16638", "abs": "https://arxiv.org/abs/2509.16638", "authors": ["Jinrui Han", "Weiji Xie", "Jiakun Zheng", "Jiyuan Shi", "Weinan Zhang", "Ting Xiao", "Chenjia Bai"], "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning versatile whole-body skills by tracking various human motions is a\nfundamental step toward general-purpose humanoid robots. This task is\nparticularly challenging because a single policy must master a broad repertoire\nof motion skills while ensuring stability over long-horizon sequences. To this\nend, we present VMS, a unified whole-body controller that enables humanoid\nrobots to learn diverse and dynamic behaviors within a single policy. Our\nframework integrates a hybrid tracking objective that balances local motion\nfidelity with global trajectory consistency, and an Orthogonal\nMixture-of-Experts (OMoE) architecture that encourages skill specialization\nwhile enhancing generalization across motions. A segment-level tracking reward\nis further introduced to relax rigid step-wise matching, enhancing robustness\nwhen handling global displacements and transient inaccuracies. We validate VMS\nextensively in both simulation and real-world experiments, demonstrating\naccurate imitation of dynamic skills, stable performance over minute-long\nsequences, and strong generalization to unseen motions. These results highlight\nthe potential of VMS as a scalable foundation for versatile humanoid whole-body\ncontrol. The project page is available at\nhttps://kungfubot2-humanoid.github.io."}
{"id": "2509.16649", "pdf": "https://arxiv.org/pdf/2509.16649", "abs": "https://arxiv.org/abs/2509.16649", "authors": ["Hyun Jun Kim", "Hyeong Yong Choi", "Changwon Lim"], "title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 1 figure, DCASE2025 Task2 technical report", "summary": "This report presents the AISTAT team's submission to the language-based audio\nretrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder\narchitecture, where audio and text modalities are encoded separately, and their\nrepresentations are aligned using contrastive learning. Drawing inspiration\nfrom methodologies of the previous year's challenge, we implemented a\ndistillation approach and leveraged large language models (LLMs) for effective\ndata augmentation techniques, including back-translation and LLM mix.\nAdditionally, we incorporated clustering to introduce an auxiliary\nclassification task for further finetuning. Our best single system achieved a\nmAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on\nthe Clotho development test split."}
{"id": "2509.16662", "pdf": "https://arxiv.org/pdf/2509.16662", "abs": "https://arxiv.org/abs/2509.16662", "authors": ["Eunjin Choi", "Hyerin Kim", "Jiwoo Ryu", "Juhan Nam", "Dasaem Jeong"], "title": "On the de-duplication of the Lakh MIDI dataset", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "comment": "The paper has been accepted for publication at ISMIR 2025", "summary": "A large-scale dataset is essential for training a well-generalized\ndeep-learning model. Most such datasets are collected via scraping from various\ninternet sources, inevitably introducing duplicated data. In the symbolic music\ndomain, these duplicates often come from multiple user arrangements and\nmetadata changes after simple editing. However, despite critical issues such as\nunreliable training evaluation from data leakage during random splitting,\ndataset duplication has not been extensively addressed in the MIR community.\nThis study investigates the dataset duplication issues regarding Lakh MIDI\nDataset (LMD), one of the largest publicly available sources in the symbolic\nmusic domain. To find and evaluate the best retrieval method for duplicated\ndata, we employed the Clean MIDI subset of the LMD as a benchmark test set, in\nwhich different versions of the same songs are grouped together. We first\nevaluated rule-based approaches and previous symbolic music retrieval models\nfor de-duplication and also investigated with a contrastive learning-based BERT\nmodel with various augmentations to find duplicate files. As a result, we\npropose three different versions of the filtered list of LMD, which filters out\nat least 38,134 samples in the most conservative settings among 178,561 files."}
{"id": "2509.16676", "pdf": "https://arxiv.org/pdf/2509.16676", "abs": "https://arxiv.org/abs/2509.16676", "authors": ["Nauman Ali Murad", "Safia Baloch"], "title": "Governed By Agents: A Survey On The Role Of Agentic AI In Future Computing Environments", "categories": ["cs.ET", "cs.AI"], "comment": null, "summary": "The emergence of agentic Artificial Intelligence (AI), which can operate\nautonomously, demonstrate goal-directed behavior, and adaptively learn,\nindicates the onset of a massive change in today's computing infrastructure.\nThis study investigates how agentic AI models' multiple characteristics may\nimpact the architecture, governance, and operation under which computing\nenvironments function. Agentic AI has the potential to reduce reliance on\nextremely large (public) cloud environments due to resource efficiency,\nespecially with processing and/or storage. The aforementioned characteristics\nprovide us with an opportunity to canvas the likelihood of strategic migration\nin computing infrastructures away from massive public cloud services, towards\nmore locally distributed architectures: edge computing and on-premises\ncomputing infrastructures. Many of these likely migrations will be spurred by\nfactors like on-premises processing needs, diminished data consumption\nfootprints, and cost savings. This study examines how a solution for\nimplementing AI's autonomy could result in a re-architecture of the systems and\nmodel a departure from today's governance models to help us manage these\nincreasingly autonomous agents, and an operational overhaul of processes over a\nvery diverse computing systems landscape that bring together computing via\ncloud, edge, and on-premises computing solutions. To enable us to explore these\nintertwined decisions, it will be fundamentally important to understand how to\nbest position agentic AI, and to navigate the future state of computing\ninfrastructures."}
{"id": "2509.16680", "pdf": "https://arxiv.org/pdf/2509.16680", "abs": "https://arxiv.org/abs/2509.16680", "authors": ["Xingjian Diao", "Weiyi Wu", "Keyi Kong", "Peijun Qing", "Xinwen Xu", "Ming Cheng", "Soroush Vosoughi", "Jiang Gui"], "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Visual Question Answering (VQA) is increasingly used in diverse applications\nranging from general visual reasoning to safety-critical domains such as\nmedical imaging and autonomous systems, where models must provide not only\naccurate answers but also explanations that humans can easily understand and\nverify. Prototype-based modeling has shown promise for interpretability by\ngrounding predictions in semantically meaningful regions for purely visual\nreasoning tasks, yet remains underexplored in the context of VQA. We present\nProtoVQA, a unified prototypical framework that (i) learns question-aware\nprototypes that serve as reasoning anchors, connecting answers to\ndiscriminative image regions, (ii) applies spatially constrained matching to\nensure that the selected evidence is coherent and semantically relevant, and\n(iii) supports both answering and grounding tasks through a shared prototype\nbackbone. To assess explanation quality, we propose the Visual-Linguistic\nAlignment Score (VLAS), which measures how well the model's attended regions\nalign with ground-truth evidence. Experiments on Visual7W show that ProtoVQA\nyields faithful, fine-grained explanations while maintaining competitive\naccuracy, advancing the development of transparent and trustworthy VQA systems."}
{"id": "2509.16682", "pdf": "https://arxiv.org/pdf/2509.16682", "abs": "https://arxiv.org/abs/2509.16682", "authors": ["Javier Jiménez-Román", "Florina Almenares-Mendoza", "Alfonso Sánchez-Macián"], "title": "Design and Development of an Intelligent LLM-based LDAP Honeypot", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Cybersecurity threats continue to increase, with a growing number of\npreviously unknown attacks each year targeting both large corporations and\nsmaller entities. This scenario demands the implementation of advanced security\nmeasures, not only to mitigate damage but also to anticipate emerging attack\ntrends. In this context, deception tools have become a key strategy, enabling\nthe detection, deterrence, and deception of potential attackers while\nfacilitating the collection of information about their tactics and methods.\nAmong these tools, honeypots have proven their value, although they have\ntraditionally been limited by rigidity and configuration complexity, hindering\ntheir adaptability to dynamic scenarios. The rise of artificial intelligence,\nand particularly general-purpose Large Language Models (LLMs), is driving the\ndevelopment of new deception solutions capable of offering greater adaptability\nand ease of use. This work proposes the design and implementation of an\nLLM-based honeypot to simulate an LDAP server, a critical protocol present in\nmost organizations due to its central role in identity and access management.\nThe proposed solution aims to provide a flexible and realistic tool capable of\nconvincingly interacting with attackers, thereby contributing to early\ndetection and threat analysis while enhancing the defensive capabilities of\ninfrastructures against intrusions targeting this service."}
{"id": "2509.16721", "pdf": "https://arxiv.org/pdf/2509.16721", "abs": "https://arxiv.org/abs/2509.16721", "authors": ["Haoyuan Li", "Rui Liu", "Hehe Fan", "Yi Yang"], "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "19 pages, 12 figures, 6 tables", "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released."}
{"id": "2509.16724", "pdf": "https://arxiv.org/pdf/2509.16724", "abs": "https://arxiv.org/abs/2509.16724", "authors": ["Italo Alberto Sousa", "Mariana Carvalho da Silva", "Jorge Machado", "José Carlos Vaz"], "title": "Exploring AI Capabilities in Participatory Budgeting within Smart Cities: The Case of Sao Paulo", "categories": ["cs.CY", "cs.AI"], "comment": "22 pages, Presented at 28th IPSA World Congress of Political Science,\n  Seoul 2025", "summary": "This research examines how Artificial Intelligence (AI) can improve\nparticipatory budgeting processes within smart cities. In response to\nchallenges like declining civic participation and resource allocation\nconflicts, the study explores how online political participation can be\nimproved by AI. It investigates the state capacity governments need to\nimplement AI-enhanced participatory tools, considering technological\ndependencies and vulnerabilities. It analyzes technological and administrative\nstructures, actors, interests, and strategies to understand the dynamics of\nonline political participation technologies in the case of Sao Paulo, Brazil.\nThe study contributes to understanding how technological advancements can\nreshape participatory budgeting processes. In a broader sense, the research\nhighlights how AI can transform participatory institutions by offering new\ntools for citizens and also for government officials in charge of participatory\nprocesses within smart cities."}
{"id": "2509.16743", "pdf": "https://arxiv.org/pdf/2509.16743", "abs": "https://arxiv.org/abs/2509.16743", "authors": ["Subhabrata Das", "Bodruzzaman Khan", "Xiao-Yang Liu"], "title": "A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurately forecasting power outages is a complex task influenced by diverse\nfactors such as weather conditions [1], vegetation, wildlife, and load\nfluctuations. These factors introduce substantial variability and noise into\noutage data, making reliable prediction challenging. Long Short-Term Memory\n(LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly\neffective for modeling nonlinear and dynamic time-series data, with proven\napplications in stock price forecasting [2], energy demand prediction, demand\nresponse [3], and traffic flow management [4]. This paper introduces a hybrid\ndeep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates\nPrincipal Component Analysis (PCA), Poisson Regression (PR), a\nSequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is\nemployed to reduce dimensionality and stabilize data variance, while Poisson\nRegression effectively models discrete outage events. The Seq2Seq-Adam-LSTM\ncomponent enhances temporal feature learning through efficient gradient\noptimization and long-term dependency capture. The framework is evaluated using\nreal-world outage records from Michigan, and results indicate that the proposed\napproach significantly improves forecasting accuracy and robustness compared to\nexisting methods."}
{"id": "2509.16745", "pdf": "https://arxiv.org/pdf/2509.16745", "abs": "https://arxiv.org/abs/2509.16745", "authors": ["Ritabrata Chakraborty", "Avijit Dasgupta", "Sandeep Chaurasia"], "title": "CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures, 6 tables", "summary": "Visual explanations are often plausible but not structurally faithful. We\nintroduce CAMBench-QR, a structure-aware benchmark that leverages the canonical\ngeometry of QR codes (finder patterns, timing lines, module grid) to test\nwhether CAM methods place saliency on requisite substructures while avoiding\nbackground. CAMBench-QR synthesizes QR/non-QR data with exact masks and\ncontrolled distortions, and reports structure-aware metrics (Finder/Timing Mass\nRatios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside\ncausal occlusion, insertion/deletion faithfulness, robustness, and latency. We\nbenchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)\nunder two practical regimes of zero-shot and last-block fine-tuning. The\nbenchmark, metrics, and training recipes provide a simple, reproducible\nyardstick for structure-aware evaluation of visual explanations. Hence we\npropose that CAMBENCH-QR can be used as a litmus test of whether visual\nexplanations are truly structure-aware."}
{"id": "2509.16765", "pdf": "https://arxiv.org/pdf/2509.16765", "abs": "https://arxiv.org/abs/2509.16765", "authors": ["Fagun Patel", "Duc Q. Nguyen", "Sang T. Truong", "Jody Vaynshtok", "Sanmi Koyejo", "Nick Haber"], "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "EMNLP 2025 Oral Presentation", "summary": "According to the U.S. National Institutes of Health, more than 3.4 million\nchildren experience speech disorders that require clinical intervention. The\nnumber of speech-language pathologists (SLPs) is roughly 20 times fewer than\nthe number of affected children, highlighting a significant gap in children's\ncare and a pressing need for technological support that improves the\nproductivity of SLPs. State-of-the-art multimodal language models (MLMs) show\npromise for supporting SLPs, but their use remains underexplored largely due to\na limited understanding of their performance in high-stakes clinical settings.\nTo address this gap, we collaborate with domain experts to develop a taxonomy\nof real-world use cases of MLMs in speech-language pathologies. Building on\nthis taxonomy, we introduce the first comprehensive benchmark for evaluating\nMLM across five core use cases, each containing 1,000 manually annotated data\npoints. This benchmark includes robustness and sensitivity tests under various\nsettings, including background noise, speaker gender, and accent. Our\nevaluation of 15 state-of-the-art MLMs reveals that no single model\nconsistently outperforms others across all tasks. Notably, we find systematic\ndisparities, with models performing better on male speakers, and observe that\nchain-of-thought prompting can degrade performance on classification tasks with\nlarge label spaces and narrow decision boundaries. Furthermore, we study\nfine-tuning MLMs on domain-specific data, achieving improvements of over 30%\ncompared to base models. These findings highlight both the potential and\nlimitations of current MLMs for speech-language pathology applications,\nunderscoring the need for further research and targeted development."}
{"id": "2509.16769", "pdf": "https://arxiv.org/pdf/2509.16769", "abs": "https://arxiv.org/abs/2509.16769", "authors": ["Prasanth K K", "Shubham Sharma"], "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 62H30, 62M45", "I.2.6; I.5.1; I.5.2; G.3"], "comment": "21 pages, 6 figures, 14 tables", "summary": "Many real world categories are multimodal, with single classes occupying\ndisjoint regions in feature space. Classical linear models (logistic\nregression, linear SVM) use a single global hyperplane and perform poorly on\nsuch data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal\nstructure but at the expense of interpretability, heavier tuning, and higher\ncomputational cost. We propose the Geometric Mixture Classifier (GMC), a\ndiscriminative model that represents each class as a mixture of hyperplanes.\nWithin each class, GMC combines plane scores via a temperature-controlled\nsoft-OR (log-sum-exp), smoothly approximating the max; across classes, standard\nsoftmax yields probabilistic posteriors. GMC optionally uses Random Fourier\nFeatures (RFF) for nonlinear mappings while keeping inference linear in the\nnumber of planes and features. Our practical training recipe: geometry-aware\nk-means initialization, silhouette-based plane budgeting, alpha annealing,\nusage-aware L2 regularization, label smoothing, and early stopping, makes GMC\nplug-and-play. Across synthetic multimodal datasets (moons, circles, blobs,\nspirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC\nconsistently outperforms linear baselines and k-NN, is competitive with\nRBF-SVM, Random Forests, and small MLPs, and provides geometric introspection\nvia per-plane and class responsibility visualizations. Inference scales\nlinearly in planes and features, making GMC CPU-friendly, with single-digit\nmicrosecond latency per example, often faster than RBF-SVM and compact MLPs.\nPost-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus\nstrikes a favorable balance of accuracy, interpretability, and efficiency: it\nis more expressive than linear models and lighter, more transparent, and faster\nthan kernel or deep models."}
{"id": "2509.16780", "pdf": "https://arxiv.org/pdf/2509.16780", "abs": "https://arxiv.org/abs/2509.16780", "authors": ["Eason Chen", "Chuangji Li", "Shizhuo Li", "Conrad Borchers", "Zimo Xiao", "Chloe Qianhui Zhao", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook", "categories": ["cs.IR", "cs.AI", "cs.HC"], "comment": null, "summary": "Technology-enhanced learning environments often help students retrieve\nrelevant learning content for questions arising during self-paced study. Large\nlanguage models (LLMs) have emerged as novel aids for information retrieval\nduring learning. While LLMs are effective for general-purpose\nquestion-answering, they typically lack alignment with the domain knowledge of\nspecific course materials such as textbooks and slides. We investigate\nRetrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced\nRAG approach, for page-level question answering in an undergraduate mathematics\ntextbook. While RAG has been effective for retrieving discrete, contextually\nrelevant passages, GraphRAG may excel in modeling interconnected concepts and\nhierarchical knowledge structures. We curate a dataset of 477 question-answer\npairs, each tied to a distinct textbook page. We then compare the standard\nembedding-based RAG methods to GraphRAG for evaluating both retrieval\naccuracy-whether the correct page is retrieved-and generated answer quality via\nF1 scores. Our findings show that embedding-based RAG achieves higher retrieval\naccuracy and better F1 scores compared to GraphRAG, which tends to retrieve\nexcessive and sometimes irrelevant content due to its entity-based structure.\nWe also explored re-ranking the retrieved pages with LLM and observed mixed\nresults, including performance drop and hallucinations when dealing with larger\ncontext windows. Overall, this study highlights both the promises and\nchallenges of page-level retrieval systems in educational contexts, emphasizing\nthe need for more refined retrieval methods to build reliable AI tutoring\nsolutions in providing reference page numbers."}
{"id": "2509.16788", "pdf": "https://arxiv.org/pdf/2509.16788", "abs": "https://arxiv.org/abs/2509.16788", "authors": ["Salha Alyami", "Amani Jamal", "Areej Alhothali"], "title": "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 excluding bibliography , journal article", "summary": "Aspect-based sentiment analysis (ABSA) in natural language processing enables\norganizations to understand customer opinions on specific product aspects.\nWhile deep learning models are widely used for English ABSA, their application\nin Arabic is limited due to the scarcity of labeled data. Researchers have\nattempted to tackle this issue by using pre-trained contextualized language\nmodels such as BERT. However, these models are often based on fact-based data,\nwhich can introduce bias in domain-specific tasks like ABSA. To our knowledge,\nno studies have applied adaptive pre-training with Arabic contextualized models\nfor ABSA. This research proposes a novel approach using domain-adaptive\npre-training for aspect-sentiment classification (ASC) and opinion target\nexpression (OTE) extraction. We examine fine-tuning strategies - feature\nextraction, full fine-tuning, and adapter-based methods - to enhance\nperformance and efficiency, utilizing multiple adaptation corpora and\ncontextualized models. Our results show that in-domain adaptive pre-training\nyields modest improvements. Adapter-based fine-tuning is a computationally\nefficient method that achieves competitive results. However, error analyses\nreveal issues with model predictions and dataset labeling. In ASC, common\nproblems include incorrect sentiment labeling, misinterpretation of contrastive\nmarkers, positivity bias for early terms, and challenges with conflicting\nopinions and subword tokenization. For OTE, issues involve mislabeling targets,\nconfusion over syntactic roles, difficulty with multi-word expressions, and\nreliance on shallow heuristics. These findings underscore the need for syntax-\nand semantics-aware models, such as graph convolutional networks, to more\neffectively capture long-distance relations and complex aspect-based opinion\nalignments."}
{"id": "2509.16804", "pdf": "https://arxiv.org/pdf/2509.16804", "abs": "https://arxiv.org/abs/2509.16804", "authors": ["Kozhin muhealddin Awlla", "Hadi Veisi", "Abdulhady Abas Abdullah"], "title": "KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper enhances the study of sentiment analysis for the Central Kurdish\nlanguage by integrating the Bidirectional Encoder Representations from\nTransformers (BERT) into Natural Language Processing techniques. Kurdish is a\nlow-resourced language, having a high level of linguistic diversity with\nminimal computational resources, making sentiment analysis somewhat\nchallenging. Earlier, this was done using a traditional word embedding model,\nsuch as Word2Vec, but with the emergence of new language models, specifically\nBERT, there is hope for improvements. The better word embedding capabilities of\nBERT lend to this study, aiding in the capturing of the nuanced semantic pool\nand the contextual intricacies of the language under study, the Kurdish\nlanguage, thus setting a new benchmark for sentiment analysis in low-resource\nlanguages."}
{"id": "2509.16812", "pdf": "https://arxiv.org/pdf/2509.16812", "abs": "https://arxiv.org/abs/2509.16812", "authors": ["Priyanshu Agrawal", "Shalabh Gupta", "Zongyuan Shen"], "title": "SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents SMART-3D, an extension of the SMART algorithm to 3D\nenvironments. SMART-3D is a tree-based adaptive replanning algorithm for\ndynamic environments with fast moving obstacles. SMART-3D morphs the underlying\ntree to find a new path in real-time whenever the current path is blocked by\nobstacles. SMART-3D removed the grid decomposition requirement of the SMART\nalgorithm by replacing the concept of hot-spots with that of hot-nodes, thus\nmaking it computationally efficient and scalable to 3D environments. The\nhot-nodes are nodes which allow for efficient reconnections to morph the\nexisting tree to find a new safe and reliable path. The performance of SMART-3D\nis evaluated by extensive simulations in 2D and 3D environments populated with\nrandomly moving dynamic obstacles. The results show that SMART-3D achieves high\nsuccess rates and low replanning times, thus highlighting its suitability for\nreal-time onboard applications."}
{"id": "2509.16825", "pdf": "https://arxiv.org/pdf/2509.16825", "abs": "https://arxiv.org/abs/2509.16825", "authors": ["Jin Lee", "Ziming Liu", "Xinling Yu", "Yixuan Wang", "Haewon Jeong", "Murphy Yuezhen Niu", "Zheng Zhang"], "title": "KANO: Kolmogorov-Arnold Neural Operator", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural\noperator jointly parameterized by both spectral and spatial bases with\nintrinsic symbolic interpretability. We theoretically demonstrate that KANO\novercomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO\nremains expressive over generic position-dependent dynamics for any physical\ninput, whereas FNO stays practical only for spectrally sparse operators and\nstrictly imposes a fast-decaying input Fourier tail. We verify our claims\nempirically on position-dependent differential operators, for which KANO\nrobustly generalizes but FNO fails to. In the quantum Hamiltonian learning\nbenchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic\nrepresentations accurate to the fourth decimal place in coefficients and\nattains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement\ndata, substantially outperforming that of the FNO trained with ideal full wave\nfunction data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude."}
{"id": "2509.16834", "pdf": "https://arxiv.org/pdf/2509.16834", "abs": "https://arxiv.org/abs/2509.16834", "authors": ["Jingxi Xu"], "title": "Robot Learning with Sparsity and Scarcity", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Unlike in language or vision, one of the fundamental challenges in robot\nlearning is the lack of access to vast data resources. We can further break\ndown the problem into (1) data sparsity from the angle of data representation\nand (2) data scarcity from the angle of data quantity. In this thesis, I will\ndiscuss selected works on two domains: (1) tactile sensing and (2)\nrehabilitation robots, which are exemplars of data sparsity and scarcity,\nrespectively. Tactile sensing is an essential modality for robotics, but\ntactile data are often sparse, and for each interaction with the physical\nworld, tactile sensors can only obtain information about the local area of\ncontact. I will discuss my work on learning vision-free tactile-only\nexploration and manipulation policies through model-free reinforcement learning\nto make efficient use of sparse tactile information. On the other hand,\nrehabilitation robots are an example of data scarcity to the extreme due to the\nsignificant challenge of collecting biosignals from disabled-bodied subjects at\nscale for training. I will discuss my work in collaboration with the medical\nschool and clinicians on intent inferral for stroke survivors, where a hand\northosis developed in our lab collects a set of biosignals from the patient and\nuses them to infer the activity that the patient intends to perform, so the\northosis can provide the right type of physical assistance at the right moment.\nMy work develops machine learning algorithms that enable intent inferral with\nminimal data, including semi-supervised, meta-learning, and generative AI\nmethods."}
{"id": "2509.16835", "pdf": "https://arxiv.org/pdf/2509.16835", "abs": "https://arxiv.org/abs/2509.16835", "authors": ["Melkamu Abay Mersha", "Jugal Kalita"], "title": "Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Virtual brainstorming sessions have become a central component of\ncollaborative problem solving, yet the large volume and uneven distribution of\nideas often make it difficult to extract valuable insights efficiently. Manual\ncoding of ideas is time-consuming and subjective, underscoring the need for\nautomated approaches to support the evaluation of group creativity. In this\nstudy, we propose a semantic-driven topic modeling framework that integrates\nfour modular components: transformer-based embeddings (Sentence-BERT),\ndimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction\nwith refinement. The framework captures semantic similarity at the sentence\nlevel, enabling the discovery of coherent themes from brainstorming transcripts\nwhile filtering noise and identifying outliers. We evaluate our approach on\nstructured Zoom brainstorming sessions involving student groups tasked with\nimproving their university. Results demonstrate that our model achieves higher\ntopic coherence compared to established methods such as LDA, ETM, and BERTopic,\nwith an average coherence score of 0.687 (CV), outperforming baselines by a\nsignificant margin. Beyond improved performance, the model provides\ninterpretable insights into the depth and diversity of topics explored,\nsupporting both convergent and divergent dimensions of group creativity. This\nwork highlights the potential of embedding-based topic modeling for analyzing\ncollaborative ideation and contributes an efficient and scalable framework for\nstudying creativity in synchronous virtual meetings."}
{"id": "2509.16857", "pdf": "https://arxiv.org/pdf/2509.16857", "abs": "https://arxiv.org/abs/2509.16857", "authors": ["Xingyu Xiang", "Raj Joshi", "Yuhan Liu", "Jiayi Yao", "Chenxingyu Zhao", "Junchen Jiang", "Yang Zhou", "Eddie Kohler", "Minlan Yu"], "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."}
{"id": "2509.16861", "pdf": "https://arxiv.org/pdf/2509.16861", "abs": "https://arxiv.org/abs/2509.16861", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "Accepted to the ASE 2025 International Conference on Automated\n  Software Engineering, Industry Showcase Track", "summary": "Guardrails are critical for the safe deployment of Large Language Models\n(LLMs)-powered software. Unlike traditional rule-based systems with limited,\npredefined input-output spaces that inherently constrain unsafe behavior, LLMs\nenable open-ended, intelligent interactions--opening the door to jailbreak\nattacks through user inputs. Guardrails serve as a protective layer, filtering\nunsafe prompts before they reach the LLM. However, prior research shows that\njailbreak attacks can still succeed over 70% of the time, even against advanced\nmodels like GPT-4o. While guardrails such as LlamaGuard report up to 95%\naccuracy, our preliminary analysis shows their performance can drop sharply--to\nas low as 12%--when confronted with unseen attacks. This highlights a growing\nsoftware engineering challenge: how to build a post-deployment guardrail that\nadapts dynamically to emerging threats? To address this, we propose\nAdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as\nout-of-distribution (OOD) inputs and learns to defend against them through a\ncontinual learning framework. Through empirical evaluation, AdaptiveGuard\nachieves 96% OOD detection accuracy, adapts to new attacks in just two update\nsteps, and retains over 85% F1-score on in-distribution data post-adaptation,\noutperforming other baselines. These results demonstrate that AdaptiveGuard is\na guardrail capable of evolving in response to emerging jailbreak strategies\npost deployment. We release our AdaptiveGuard and studied datasets at\nhttps://github.com/awsm-research/AdaptiveGuard to support further research."}
{"id": "2509.16869", "pdf": "https://arxiv.org/pdf/2509.16869", "abs": "https://arxiv.org/abs/2509.16869", "authors": ["Hrishav Bakul Barua", "Kalin Stefanov", "Ganesh Krishnasamy", "KokSheik Wong", "Abhinav Dhall"], "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM", "eess.IV", "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning", "I.3.3; I.4.5"], "comment": "Submitted to IEEE", "summary": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a\nfundamental task in many computational vision problems. Numerous data-driven\nmethods have been proposed to address this problem; however, they lack explicit\nmodeling of illumination, lighting, and scene geometry in images. This limits\nthe quality of the reconstructed HDR images. Since lighting and shadows\ninteract differently with different materials, (e.g., specular surfaces such as\nglass and metal, and lambertian or diffuse surfaces such as wood and stone),\nmodeling material-specific properties (e.g., specular and diffuse reflectance)\nhas the potential to improve the quality of HDR image reconstruction. This\npaper presents PhysHDR, a simple yet powerful latent diffusion-based generative\nmodel for HDR image reconstruction. The denoising process is conditioned on\nlighting and depth information and guided by a novel loss to incorporate\nmaterial properties of surfaces in the scene. The experimental results\nestablish the efficacy of PhysHDR in comparison to a number of recent\nstate-of-the-art methods."}
{"id": "2509.16882", "pdf": "https://arxiv.org/pdf/2509.16882", "abs": "https://arxiv.org/abs/2509.16882", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Xuming Hu"], "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated\nexpert subnetworks, yet adapting them to multiple domains without catastrophic\nforgetting remains an open challenge. Existing approaches either incur\nprohibitive computation, suffer cross-domain interference, or require separate\nruns per domain. We propose DES-MoE, a dynamic expert specialization framework\nfor multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses\ncatastrophic forgetting through three innovations: (1) an adaptive router\nbalancing pre-trained knowledge retention and task-specific updates via\ndistillation, (2) real-time expert-domain correlation mapping to isolate\ndomain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule\nthat progressively freezes non-specialized parameters. Evaluated on six domains\n(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while\ntraining one unified model, reduces forgetting by 89% compared to full\nfine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence\nthan conventional methods. Our work establishes dynamic expert isolation as a\nscalable paradigm for multi-task MoE adaptation."}
{"id": "2509.16892", "pdf": "https://arxiv.org/pdf/2509.16892", "abs": "https://arxiv.org/abs/2509.16892", "authors": ["Jiahe Qian", "Yaoyu Fang", "Ziqiao Weng", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "Spatial transcriptomics aims to connect high-resolution histology images with\nspatially resolved gene expression. To achieve better performance on downstream\ntasks such as gene expression prediction, large-scale pre-training is required\nto obtain generalisable representations that can bridge histology and\ntranscriptomics across tissues, protocols, and laboratories. Existing\ncross-modal pre-training approaches for spatial transcriptomics rely on either\ngene names or expression values in isolation, which strips the gene branch of\nessential semantics and breaks the association between each gene and its\nquantitative magnitude. In addition, by restricting supervision to image-text\nalignment, these methods ignore intrinsic visual cues that are critical for\nlearning robust image features. We present CoMTIP, the first Contrastive Masked\nText-Image Pretraining framework that jointly learns from images, gene names,\nand expression values while capturing fine-grained visual context for spatial\ntranscriptomics. The vision branch uses Masked Feature Modeling to reconstruct\noccluded patches and learn context-aware image embeddings. The text branch\napplies a scalable Gene-Text Encoder that processes all gene sentences in\nparallel, enriches each gene and its numerical value with dedicated embeddings,\nand employs Pair-aware Adversarial Training (PAAT) to preserve correct\ngene-value associations. Image and text representations are aligned in a shared\nInfoNCE-optimised space. Experiments on public spatial transcriptomics datasets\nshow that CoMTIP not only surpasses previous methods on diverse downstream\ntasks but also achieves zero-shot gene expression prediction, a capability that\nexisting approaches do not provide."}
{"id": "2509.16900", "pdf": "https://arxiv.org/pdf/2509.16900", "abs": "https://arxiv.org/abs/2509.16900", "authors": ["Chengsheng Zhang", "Linhao Qu", "Xiaoyu Liu", "Zhijian Song"], "title": "ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Survival analysis using whole-slide images (WSIs) is crucial in cancer\nresearch. Despite significant successes, pathology images typically only\nprovide slide-level labels, which hinders the learning of discriminative\nrepresentations from gigapixel WSIs. With the rapid advancement of\nhigh-throughput sequencing technologies, multimodal survival analysis\nintegrating pathology images and genomics data has emerged as a promising\napproach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures\ndiscriminative pathological and genomic features while enabling efficient\nintegration of both modalities. This approach achieves complementary\ninformation fusion without losing critical information from individual\nmodalities, thereby facilitating accurate cancer survival analysis.\nSpecifically, we first introduce a Pathology Expert and a Genomics Expert to\nprocess unimodal data separately. Both experts are designed with Mamba\narchitectures that incorporate conventional scanning and attention-based\nscanning mechanisms, allowing them to extract discriminative features from long\ninstance sequences containing substantial redundant or irrelevant information.\nSecond, we design a Synergistic Expert responsible for modality fusion. It\nexplicitly learns token-level local correspondences between the two modalities\nvia Optimal Transport, and implicitly enhances distribution consistency through\na global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused\nfeature representations are then passed to a mamba backbone for further\nintegration. Through the collaboration of the Pathology Expert, Genomics\nExpert, and Synergistic Expert, our method achieves stable and accurate\nsurvival analysis with relatively low computational complexity. Extensive\nexperimental results on five datasets in The Cancer Genome Atlas (TCGA)\ndemonstrate our state-of-the-art performance."}
{"id": "2509.16902", "pdf": "https://arxiv.org/pdf/2509.16902", "abs": "https://arxiv.org/abs/2509.16902", "authors": ["Letian Zhang", "Bo Chen", "Jieming Bian", "Lei Wang", "Jie Xu"], "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) enables distributed devices to collaboratively train\nmachine learning models while maintaining data privacy. However, the\nheterogeneous hardware capabilities of devices often result in significant\ntraining delays, as straggler clients with limited resources prolong the\naggregation process. Existing solutions such as client selection, asynchronous\nFL, and partial training partially address these challenges but encounter\nissues such as reduced accuracy, stale updates, and compromised model\nperformance due to inconsistent training contributions. To overcome these\nlimitations, we propose FedEL, a federated elastic learning framework that\nenhances training efficiency while maintaining model accuracy. FedEL introduces\na novel window-based training process, sliding the window to locate the\ntraining part of the model and dynamically selecting important tensors for\ntraining within a coordinated runtime budget. This approach ensures progressive\nand balanced training across all clients, including stragglers. Additionally,\nFedEL employs a tensor importance adjustment module, harmonizing local and\nglobal tensor importance to mitigate biases caused by data heterogeneity. The\nexperiment results show that FedEL achieves up to 3.87x improvement in\ntime-to-accuracy compared to baselines while maintaining or exceeding final\ntest accuracy."}
{"id": "2509.16922", "pdf": "https://arxiv.org/pdf/2509.16922", "abs": "https://arxiv.org/abs/2509.16922", "authors": ["Tianheng Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control", "categories": ["cs.SD", "cs.AI", "eess.IV"], "comment": "Main paper (15 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "Audio-driven talking head generation is crucial for applications in virtual\nreality, digital avatars, and film production. While NeRF-based methods enable\nhigh-fidelity reconstruction, they suffer from low rendering efficiency and\nsuboptimal audio-visual synchronization. This work presents PGSTalker, a\nreal-time audio-driven talking head synthesis framework based on 3D Gaussian\nSplatting (3DGS). To improve rendering performance, we propose a pixel-aware\ndensity control strategy that adaptively allocates point density, enhancing\ndetail in dynamic facial regions while reducing redundancy elsewhere.\nAdditionally, we introduce a lightweight Multimodal Gated Fusion Module to\neffectively fuse audio and spatial features, thereby improving the accuracy of\nGaussian deformation prediction. Extensive experiments on public datasets\ndemonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches\nin rendering quality, lip-sync precision, and inference speed. Our method\nexhibits strong generalization capabilities and practical potential for\nreal-world deployment."}
{"id": "2509.16926", "pdf": "https://arxiv.org/pdf/2509.16926", "abs": "https://arxiv.org/abs/2509.16926", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Accepted on Workshop on Detection and Classification of Acoustic\n  Scenes and Events (DCASE 2025)", "summary": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring,\nspatial audio systems, and acoustic localization. However, existing methods\noften struggle to address nonlinear clock drift and lack mechanisms for\nquantifying uncertainty. Traditional methods like Cross-correlation and Dynamic\nTime Warping assume simple drift patterns and provide no reliability measures.\nMeanwhile, recent deep learning models typically treat alignment as a binary\nclassification task, overlooking inter-channel dependencies and uncertainty\nestimation. We introduce a method that combines cross-attention mechanisms with\nconfidence-weighted scoring to improve multi-channel audio synchronization. We\nextend BEATs encoders with cross-attention layers to model temporal\nrelationships between channels. We also develop a confidence-weighted scoring\nfunction that uses the full prediction distribution instead of binary\nthresholding. Our method achieved first place in the BioDCASE 2025 Task 1\nchallenge with 0.30 MSE average across test datasets, compared to 0.58 for the\ndeep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU\ndata (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The\nframework supports probabilistic temporal alignment, moving beyond point\nestimates. While validated in a bioacoustic context, the approach is applicable\nto a broader range of multi-channel audio tasks where alignment confidence is\ncritical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA"}
{"id": "2509.16931", "pdf": "https://arxiv.org/pdf/2509.16931", "abs": "https://arxiv.org/abs/2509.16931", "authors": ["Yutong Li", "Yu Zhu", "Yichen Qiao", "Ziyu Guan", "Lv Shao", "Tong Liu", "Bo Zheng"], "title": "Equip Pre-ranking with Target Attention by Residual Quantization", "categories": ["cs.IR", "cs.AI", "cs.LG", "I.2.0; I.5.0; I.7.0"], "comment": "5 pages, 2 figures, submitted to WSDM 2026 Short Paper Track", "summary": "The pre-ranking stage in industrial recommendation systems faces a\nfundamental conflict between efficiency and effectiveness. While powerful\nmodels like Target Attention (TA) excel at capturing complex feature\ninteractions in the ranking stage, their high computational cost makes them\ninfeasible for pre-ranking, which often relies on simplistic vector-product\nmodels. This disparity creates a significant performance bottleneck for the\nentire system. To bridge this gap, we propose TARQ, a novel pre-ranking\nframework. Inspired by generative models, TARQ's key innovation is to equip\npre-ranking with an architecture approximate to TA by Residual Quantization.\nThis allows us to bring the modeling power of TA into the latency-critical\npre-ranking stage for the first time, establishing a new state-of-the-art\ntrade-off between accuracy and efficiency. Extensive offline experiments and\nlarge-scale online A/B tests at Taobao demonstrate TARQ's significant\nimprovements in ranking performance. Consequently, our model has been fully\ndeployed in production, serving tens of millions of daily active users and\nyielding substantial business improvements."}
{"id": "2509.16952", "pdf": "https://arxiv.org/pdf/2509.16952", "abs": "https://arxiv.org/abs/2509.16952", "authors": ["Tiancheng Huang", "Ruisheng Cao", "Yuxin Zhang", "Zhangyi Kang", "Zijian Wang", "Chenrun Wang", "Yijie Luo", "Hang Zheng", "Lirong Qian", "Lu Chen", "Kai Yu"], "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing volume of academic papers has made it increasingly difficult for\nresearchers to efficiently extract key information. While large language models\n(LLMs) based agents are capable of automating question answering (QA) workflows\nfor scientific papers, there still lacks a comprehensive and realistic\nbenchmark to evaluate their capabilities. Moreover, training an interactive\nagent for this specific task is hindered by the shortage of high-quality\ninteraction trajectories. In this work, we propose AirQA, a human-annotated\ncomprehensive paper QA dataset in the field of artificial intelligence (AI),\nwith 13,948 papers and 1,246 questions, that encompasses multi-task,\nmulti-modal and instance-level evaluation. Furthermore, we propose ExTrActor,\nan automated framework for instruction data synthesis. With three LLM-based\nagents, ExTrActor can perform example generation and trajectory collection\nwithout human intervention. Evaluations of multiple open-source and proprietary\nmodels show that most models underperform on AirQA, demonstrating the quality\nof our dataset. Extensive experiments confirm that ExTrActor consistently\nimproves the multi-turn tool-use capability of small models, enabling them to\nachieve performance comparable to larger ones."}
{"id": "2509.16959", "pdf": "https://arxiv.org/pdf/2509.16959", "abs": "https://arxiv.org/abs/2509.16959", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Gradient Interference-Aware Graph Coloring for Multitask Learning", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "When different objectives conflict with each other in multi-task learning,\ngradients begin to interfere and slow convergence, thereby reducing the final\nmodel's performance. To address this, we introduce a scheduler that computes\ngradient interference, constructs an interference graph, and then applies\ngreedy graph-coloring to partition tasks into groups that align well with each\nother. At each training step, only one group (color class) of tasks are\nactivated. The grouping partition is constantly recomputed as task\nrelationships evolve throughout training. By ensuring that each mini-batch\ncontains only tasks that pull the model in the same direction, our method\nimproves the effectiveness of any underlying multi-task learning optimizer\nwithout additional tuning. Since tasks within these groups will update in\ncompatible directions, model performance will be improved rather than impeded.\nEmpirical results on six different datasets show that this interference-aware\ngraph-coloring approach consistently outperforms baselines and state-of-the-art\nmulti-task optimizers."}
{"id": "2509.16972", "pdf": "https://arxiv.org/pdf/2509.16972", "abs": "https://arxiv.org/abs/2509.16972", "authors": ["Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji"], "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA", "categories": ["cs.CV", "cs.AI"], "comment": "1st place report of 7th LSVOS RVOS track in ICCV 2025. The code is\n  released in Sa2VA repository: https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) requires segmenting and tracking\nobjects in videos conditioned on natural-language expressions, demanding\nfine-grained understanding of both appearance and motion. Building on Sa2VA,\nwhich couples a Multi-modal Large Language Model (MLLM) with the video\nsegmentation model SAM2, we identify two key bottlenecks that limit\nsegmentation performance: sparse frame sampling and reliance on a single [SEG]\ntoken for an entire video. We propose Segmentation Augmented and Selective\nAveraged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge\n(RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and\nsurpassing the runner-up by 2.80 points. This result and ablation studies\ndemonstrate that efficient segmentation augmentation and test-time ensembling\nsubstantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA\nrepository: https://github.com/magic-research/Sa2VA."}
{"id": "2509.16979", "pdf": "https://arxiv.org/pdf/2509.16979", "abs": "https://arxiv.org/abs/2509.16979", "authors": ["Boxuan Cao", "Linkai Li", "Hanlin Yu", "Changgeng Mo", "Haoshuai Zhou", "Shan Xiang Wang"], "title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is\nessential for assessing hearing aid performance, traditionally relying on\nlistening tests or intrusive methods like HASPI. However, these methods require\nclean reference signals, which are often unavailable in real-world conditions,\ncreating a gap between lab-based and real-world assessments. To address this,\nwe propose a non-intrusive intelligibility prediction framework that leverages\nspeech enhancers to provide a parallel enhanced-signal pathway, enabling robust\npredictions without reference signals. We evaluate three state-of-the-art\nenhancers and demonstrate that prediction performance depends on the choice of\nenhancer, with ensembles of strong enhancers yielding the best results. To\nimprove cross-dataset generalization, we introduce a 2-clips augmentation\nstrategy that enhances listener-specific variability, boosting robustness on\nunseen datasets. Our approach consistently outperforms the non-intrusive\nbaseline, CPC2 Champion across multiple datasets, highlighting the potential of\nenhancer-guided non-intrusive intelligibility prediction for real-world\napplications."}
{"id": "2509.16989", "pdf": "https://arxiv.org/pdf/2509.16989", "abs": "https://arxiv.org/abs/2509.16989", "authors": ["He Xiao", "Runming Yang", "Qingyao Yang", "Wendong Xu", "Zheng Li", "Yupeng Su", "Zhengwu Liu", "Hongxia Yang", "Ngai Wong"], "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "under review", "summary": "Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments."}
{"id": "2509.16990", "pdf": "https://arxiv.org/pdf/2509.16990", "abs": "https://arxiv.org/abs/2509.16990", "authors": ["Avishai Elmakies", "Hagai Aronowitz", "Nimrod Shabtay", "Eli Schwartz", "Ron Hoory", "Avihu Dekel"], "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based\nmethod for training Speech-Aware Large Language Models (SALLMs) on open-format\nspeech understanding tasks, such as Spoken Question Answering and Automatic\nSpeech Translation. SALLMs have proven highly effective for speech\nunderstanding tasks. GRPO has recently gained traction for its efficiency in\ntraining LLMs, and prior work has explored its application to SALLMs, primarily\nin multiple-choice tasks. Building on this, we focus on open-format tasks that\nbetter reflect the generative abilities of the models. Our approach leverages\nGRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate\nempirically that it surpasses standard SFT across several key metrics. Finally,\nwe explore the potential of incorporating off-policy samples within GRPO for\nthese tasks, highlighting avenues for further improvement and further research."}
{"id": "2509.17000", "pdf": "https://arxiv.org/pdf/2509.17000", "abs": "https://arxiv.org/abs/2509.17000", "authors": ["Shuhao Jiang", "Songbo Wang", "Yang Qiao", "Chun Xu", "Chaoyang Zheng", "Shengyi Zhou", "Huanjun Wang", "Fangming Li", "Cong Zhang", "Jiyu Wang"], "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) often suffer from computational inefficiency\ndue to overthinking, where a fixed reasoning budget fails to match the varying\ncomplexity of tasks. To address this issue, we propose Adaptive Overclocking, a\nmethod that makes the overclocking hyperparameter $\\alpha$ dynamic and\ncontext-aware. Our method adjusts reasoning speed in real time through two\ncomplementary signals: (1) token-level model uncertainty for fine-grained\nstep-wise control, and (2) input complexity estimation for informed\ninitialization. We implement this approach with three strategies:\nUncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha\nInitialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that\ncombines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves\nsuperior accuracy-latency trade-offs, reducing unnecessary computation on\nsimple problems while allocating more resources to challenging ones. By\nmitigating overthinking, Adaptive Overclocking enhances both efficiency and\noverall reasoning performance."}
{"id": "2509.17024", "pdf": "https://arxiv.org/pdf/2509.17024", "abs": "https://arxiv.org/abs/2509.17024", "authors": ["Wenxuan Fang", "Jili Fan", "Chao Wang", "Xiantao Hu", "Jiangwei Weng", "Ying Tai", "Jian Yang", "Jun Li"], "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to\nthe unpredictable and dynamic nature of weather-related degradations.\nTraditional task-specific methods often fail to generalize to unseen or complex\ndegradation types, while recent prompt-learning approaches depend heavily on\nthe degradation estimation capabilities of vision-language models, resulting in\ninconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel\nframework comprising two key components: \\textit{Lumina-Chroma Decomposition\nNetwork} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN\nprocesses degraded images in the YCbCr color space, separately handling\ndegradation-related luminance and degradation-invariant chrominance components.\nThis decomposition effectively mitigates weather-induced degradation while\npreserving color fidelity. To further enhance restoration quality, LGDM\nleverages degradation-related luminance information as a guiding condition,\neliminating the need for explicit degradation prompts. Additionally, LGDM\nincorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising\nnetwork, ensuring a balanced recovery of both low- and high-frequency features\nin the image. Finally, we present DriveWeather, a comprehensive all-weather\ndriving dataset designed to enable robust evaluation. Extensive experiments\ndemonstrate that our approach surpasses state-of-the-art methods, setting a new\nbenchmark in AWIR. The dataset and code are available at:\nhttps://github.com/fiwy0527/LCDiff."}
{"id": "2509.17030", "pdf": "https://arxiv.org/pdf/2509.17030", "abs": "https://arxiv.org/abs/2509.17030", "authors": ["Hinata Tezuka", "Naoya Inoue"], "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "57 pages, 47 figures and 41 tables; Accepted to EMNLP 2025 Main", "summary": "Recent studies have suggested a processing framework for multilingual inputs\nin decoder-based LLMs: early layers convert inputs into English-centric and\nlanguage-agnostic representations; middle layers perform reasoning within an\nEnglish-centric latent space; and final layers generate outputs by transforming\nthese representations back into language-specific latent spaces. However, the\ninternal dynamics of such transformation and the underlying mechanism remain\nunderexplored. Towards a deeper understanding of this framework, we propose and\nempirically validate The Transfer Neurons Hypothesis: certain neurons in the\nMLP module are responsible for transferring representations between\nlanguage-specific latent spaces and a shared semantic latent space.\nFurthermore, we show that one function of language-specific neurons, as\nidentified in recent studies, is to facilitate movement between latent spaces.\nFinally, we show that transfer neurons are critical for reasoning in\nmultilingual LLMs."}
{"id": "2509.17040", "pdf": "https://arxiv.org/pdf/2509.17040", "abs": "https://arxiv.org/abs/2509.17040", "authors": ["Hang Du", "Jiayang Zhang", "Guoshun Nan", "Wendi Deng", "Zhenyan Chen", "Chenyang Zhang", "Wang Xiao", "Shan Huang", "Yuqi Pan", "Tao Qi", "Sicong Leng"], "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language\nModels (MLLMs) ability to jointly comprehend and reason across multiple images\nand their associated textual contexts, introducing unique challenges beyond\nsingle-image or non-interleaved multi-image tasks. While current multi-image\nbenchmarks overlook interleaved textual contexts and neglect distinct\nrelationships between individual images and their associated texts, enabling\nmodels to reason over multi-image interleaved data may significantly enhance\ntheir comprehension of complex scenes and better capture cross-modal\ncorrelations. To bridge this gap, we introduce a novel benchmark MIR, requiring\njoint reasoning over multiple images accompanied by interleaved textual\ncontexts to accurately associate image regions with corresponding texts and\nlogically connect information across images. To enhance MLLMs ability to\ncomprehend multi-image interleaved data, we introduce reasoning steps for each\ninstance within the benchmark and propose a stage-wise curriculum learning\nstrategy. This strategy follows an \"easy to hard\" approach, progressively\nguiding models from simple to complex scenarios, thereby enhancing their\nability to handle challenging tasks. Extensive experiments benchmarking\nmultiple MLLMs demonstrate that our method significantly enhances models\nreasoning performance on MIR and other established benchmarks. We believe that\nMIR will encourage further research into multi-image interleaved reasoning,\nfacilitating advancements in MLLMs capability to handle complex inter-modal\ntasks.Our code and dataset are available at\nhttps://github.com/Shelly-coder239/MIRBench."}
{"id": "2509.17046", "pdf": "https://arxiv.org/pdf/2509.17046", "abs": "https://arxiv.org/abs/2509.17046", "authors": ["Haojun Yu", "Youcheng Li", "Zihan Niu", "Nan Zhang", "Xuantong Gong", "Huan Li", "Zhiying Zou", "Haifeng Qi", "Zhenxiao Cao", "Zijie Lan", "Xingjian Yuan", "Jiating He", "Haokai Zhang", "Shengtao Zhang", "Zicheng Wang", "Dong Wang", "Ziwei Zhao", "Congying Chen", "Yong Wang", "Wangyan Qin", "Qingli Zhu"], "title": "A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All Histopathology Categories", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Breast ultrasound (BUS) is an essential tool for diagnosing breast lesions,\nwith millions of examinations per year. However, publicly available\nhigh-quality BUS benchmarks for AI development are limited in data scale and\nannotation richness. In this work, we present BUS-CoT, a BUS dataset for\nchain-of-thought (CoT) reasoning analysis, which contains 11,439 images of\n10,019 lesions from 4,838 patients and covers all 99 histopathology types. To\nfacilitate research on incentivizing CoT reasoning, we construct the reasoning\nprocesses based on observation, feature, diagnosis and pathology labels,\nannotated and verified by experienced experts. Moreover, by covering lesions of\nall histopathology types, we aim to facilitate robust AI systems in rare cases,\nwhich can be error-prone in clinical practice."}
{"id": "2509.17054", "pdf": "https://arxiv.org/pdf/2509.17054", "abs": "https://arxiv.org/abs/2509.17054", "authors": ["Yiwei Liu", "Emma Jane Pretty", "Jiahao Huang", "Saku Sugawara"], "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While recent studies explore Large Language Models' (LLMs) performance on\nTheory of Mind (ToM) reasoning tasks, research on ToM abilities that require\nmore nuanced social context is limited, such as white lies. We introduce\nTactfulToM, a novel English benchmark designed to evaluate LLMs' ability to\nunderstand white lies within real-life conversations and reason about prosocial\nmotivations behind them, particularly when they are used to spare others'\nfeelings and maintain social harmony. Our benchmark is generated through a\nmulti-stage human-in-the-loop pipeline where LLMs expand manually designed seed\nstories into conversations to maintain the information asymmetry between\nparticipants necessary for authentic white lies. We show that TactfulToM is\nchallenging for state-of-the-art models, which perform substantially below\nhumans, revealing shortcomings in their ability to fully comprehend the ToM\nreasoning that enables true understanding of white lies."}
{"id": "2509.17074", "pdf": "https://arxiv.org/pdf/2509.17074", "abs": "https://arxiv.org/abs/2509.17074", "authors": ["Qian Zhang", "Lin Zhang", "Xing Fang", "Mingxin Zhang", "Zhiyuan Wei", "Ran Song", "Wei Zhang"], "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2026", "summary": "Visual affordance learning is crucial for robots to understand and interact\neffectively with the physical world. Recent advances in this field attempt to\nleverage pre-trained knowledge of vision-language foundation models to learn\naffordance properties with limited training data, providing a novel paradigm\nfor visual affordance learning. However, these methods overlook the\nsignificance of maintaining feature alignment between visual images and\nlanguage descriptions for identifying affordance areas with textual guidance,\nand thus may lead to suboptimal results. In this paper, we present an\ninformative framework for text-guided affordance learning, which involves\ninformation-based constraints to achieve text-image alignment at feature level.\nSpecifically, we design an affordance mutual information constraint that helps\nlearn appropriate textual prompts and task-oriented visual features\nsimultaneously by maximizing the mutual information between the features of the\naffordance areas in the input images and the corresponding textual prompts. In\naddition, we propose an object-level information constraint that maximizes the\nmutual information between the visual features of a given object and the text\nfeatures of the category it belongs to. This enables the model to capture\nhigh-quality representations for the object, providing more reliable semantic\npriors for identifying affordance regions. Experimental results on the AGD20K\ndataset show that the proposed method outperforms existing approaches and\nachieves the new state-of-the-art in one-shot affordance learning."}
{"id": "2509.17094", "pdf": "https://arxiv.org/pdf/2509.17094", "abs": "https://arxiv.org/abs/2509.17094", "authors": ["Elton Pan", "Soonhyoung Kwon", "Sulin Liu", "Mingrou Xie", "Alexander J. Hoffman", "Yifei Duan", "Thorben Prein", "Killian Sheriff", "Yuriy Roman-Leshkov", "Manuel Moliner", "Rafael Gomez-Bombarelli", "Elsa Olivetti"], "title": "$\\texttt{DiffSyn}$: A Generative Diffusion Approach to Materials Synthesis Planning", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": null, "summary": "The synthesis of crystalline materials, such as zeolites, remains a\nsignificant challenge due to a high-dimensional synthesis space, intricate\nstructure-synthesis relationships and time-consuming experiments. Considering\nthe one-to-many relationship between structure and synthesis, we propose\n$\\texttt{DiffSyn}$, a generative diffusion model trained on over 23,000\nsynthesis recipes spanning 50 years of literature. $\\texttt{DiffSyn}$ generates\nprobable synthesis routes conditioned on a desired zeolite structure and an\norganic template. $\\texttt{DiffSyn}$ achieves state-of-the-art performance by\ncapturing the multi-modal nature of structure-synthesis relationships. We apply\n$\\texttt{DiffSyn}$ to differentiate among competing phases and generate optimal\nsynthesis routes. As a proof of concept, we synthesize a UFI material using\n$\\texttt{DiffSyn}$-generated synthesis routes. These routes, rationalized by\ndensity functional theory binding energies, resulted in the successful\nsynthesis of a UFI material with a high Si/Al$_{\\text{ICP}}$ of 19.0, which is\nexpected to improve thermal stability and is higher than that of any previously\nrecorded."}
{"id": "2509.17095", "pdf": "https://arxiv.org/pdf/2509.17095", "abs": "https://arxiv.org/abs/2509.17095", "authors": ["Jinbao Wang", "Jun Liu", "Shiliang Zhang", "Xuehui Ma"], "title": "Ultra-short-term solar power forecasting by deep learning and data reconstruction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The integration of solar power has been increasing as the green energy\ntransition rolls out. The penetration of solar power challenges the grid\nstability and energy scheduling, due to its intermittent energy generation.\nAccurate and near real-time solar power prediction is of critical importance to\ntolerant and support the permeation of distributed and volatile solar power\nproduction in the energy system. In this paper, we propose a deep-learning\nbased ultra-short-term solar power prediction with data reconstruction. We\ndecompose the data for the prediction to facilitate extensive exploration of\nthe spatial and temporal dependencies within the data. Particularly, we\nreconstruct the data into low- and high-frequency components, using ensemble\nempirical model decomposition with adaptive noise (CEEMDAN). We integrate\nmeteorological data with those two components, and employ deep-learning models\nto capture long- and short-term dependencies towards the target prediction\nperiod. In this way, we excessively exploit the features in historical data in\npredicting a ultra-short-term solar power production. Furthermore, as\nultra-short-term prediction is vulnerable to local optima, we modify the\noptimization in our deep-learning training by penalizing long prediction\nintervals. Numerical experiments with diverse settings demonstrate that,\ncompared to baseline models, the proposed method achieves improved\ngeneralization in data reconstruction and higher prediction accuracy for\nultra-short-term solar power production."}
{"id": "2509.17096", "pdf": "https://arxiv.org/pdf/2509.17096", "abs": "https://arxiv.org/abs/2509.17096", "authors": ["Ziyou Li", "Agnia Sergeyuk", "Maliheh Izadi"], "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large Language Models are transforming software engineering, yet prompt\nmanagement in practice remains ad hoc, hindering reliability, reuse, and\nintegration into industrial workflows. We present Prompt-with-Me, a practical\nsolution for structured prompt management embedded directly in the development\nenvironment. The system automatically classifies prompts using a\nfour-dimensional taxonomy encompassing intent, author role, software\ndevelopment lifecycle stage, and prompt type. To enhance prompt reuse and\nquality, Prompt-with-Me suggests language refinements, masks sensitive\ninformation, and extracts reusable templates from a developer's prompt library.\nOur taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can\naccurately classify software engineering prompts. Furthermore, our user study\nwith 11 participants shows strong developer acceptance, with high usability\n(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in\nprompt quality and efficiency through reduced repetitive effort. Lastly, we\noffer actionable insights for building the next generation of prompt management\nand maintenance tools for software engineering workflows."}
{"id": "2509.17119", "pdf": "https://arxiv.org/pdf/2509.17119", "abs": "https://arxiv.org/abs/2509.17119", "authors": ["Yifei Wu", "Bo Wang", "Jingshi Cui", "Pei-chun Lin", "Junzo Watada"], "title": "ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To address the intermittency of renewable energy source (RES) generation,\nscenario forecasting offers a series of stochastic realizations for predictive\nobjects with superior flexibility and direct views. Based on a long time-series\nperspective, this paper explores uncertainties in the realms of renewable power\nand deep learning. Then, an uncertainty-aware model is meticulously designed\nfor renewable scenario forecasting, which leverages an attention mechanism and\ngenerative adversarial networks (GANs) to precisely capture complex\nspatial-temporal dynamics. To improve the interpretability of uncertain\nbehavior in RES generation, Bayesian deep learning and adaptive instance\nnormalization (AdaIN) are incorporated to simulate typical patterns and\nvariations. Additionally, the integration of meteorological information,\nforecasts, and historical trajectories in the processing layer improves the\nsynergistic forecasting capability for multiscale periodic regularities.\nNumerical experiments and case analyses demonstrate that the proposed approach\nprovides an appropriate interpretation for renewable uncertainty\nrepresentation, including both aleatoric and epistemic uncertainties, and shows\nsuperior performance over state-of-the-art methods."}
{"id": "2509.17136", "pdf": "https://arxiv.org/pdf/2509.17136", "abs": "https://arxiv.org/abs/2509.17136", "authors": ["Yuhao Tian", "Zheming Yang"], "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 5 figures", "summary": "Industrial vision inspection requires high accuracy under stringent resource\nconstraints, yet existing approaches face a fundamental trade-off. Multimodal\nLLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive\ncomputational costs, while lightweight edge models often fail on complex cases.\nIn this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative\nindustrial vision inspection framework with MLLM. The framework is composed of\nthree synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect\nInspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)\nAdaptive Edge-Cloud Scheduler. Together, these modules enable robust defect\ndetection by tailoring multimodal reasoning to scene complexity and dynamically\nbalancing computation between edge and cloud resources. Experimental results on\nMVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%\naccuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It\nalso reduces runtime by up to 22.4% and cuts energy per correct decision by\n40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC."}
{"id": "2509.17143", "pdf": "https://arxiv.org/pdf/2509.17143", "abs": "https://arxiv.org/abs/2509.17143", "authors": ["Junhyeok Lee", "Helin Wang", "Yaohan Guan", "Thomas Thebaud", "Laureano Moro-Velazquez", "Jesús Villalba", "Najim Dehak"], "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances", "categories": ["eess.AS", "cs.AI"], "comment": null, "summary": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers\nmulti-factor controllability through multiple classifier-free guidances (CFGs).\nWhile previous VC models rely on a fixed conditioning scheme, MaskVCT\nintegrates diverse conditions in a single model. To further enhance robustness\nand control, the model can leverage continuous or quantized linguistic features\nto enhance intellgibility and speaker similarity, and can use or omit pitch\ncontour to control prosody. These choices allow users to seamlessly balance\nspeaker identity, linguistic content, and prosodic factors in a zero-shot VC\nsetting. Extensive experiments demonstrate that MaskVCT achieves the best\ntarget speaker and accent similarities while obtaining competitive word and\ncharacter error rates compared to existing baselines. Audio samples are\navailable at https://maskvct.github.io/."}
{"id": "2509.17153", "pdf": "https://arxiv.org/pdf/2509.17153", "abs": "https://arxiv.org/abs/2509.17153", "authors": ["Moule Lin", "Andrea Patane", "Weipeng Jing", "Shuhao Guan", "Goetz Botterweck"], "title": "Flow-Induced Diagonal Gaussian Processes", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages", "summary": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression\nframework that incorporates a compact inducing weight matrix to project a\nneural network's weight uncertainty into a lower-dimensional subspace.\nCritically, FiD-GP relies on normalising-flow priors and spectral\nregularisations to augment its expressiveness and align the inducing subspace\nwith feature-gradient geometry through a numerically stable projection\nmechanism objective. Furthermore, we demonstrate how the prediction framework\nin FiD-GP can help to design a single-pass projection for Out-of-Distribution\n(OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation\nability on various tasks compared with SVGP-based baselines, satisfies tight\nspectral residual bounds with theoretically guaranteed OoD detection, and\nsignificantly compresses the neural network's storage requirements at the cost\nof increased inference computation dependent on the number of inducing weights\nemployed. Specifically, in a comprehensive empirical study spanning regression,\nimage classification, semantic segmentation, and out-of-distribution detection\nbenchmarks, it cuts Bayesian training cost by several orders of magnitude,\ncompresses parameters by roughly 51%, reduces model size by about 75%, and\nmatches state-of-the-art accuracy and uncertainty estimation."}
{"id": "2509.17165", "pdf": "https://arxiv.org/pdf/2509.17165", "abs": "https://arxiv.org/abs/2509.17165", "authors": ["Sahar Koohfar", "Wubeshet Woldemariam"], "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series data is a prevalent form of data found in various fields. It\nconsists of a series of measurements taken over time. Forecasting is a crucial\napplication of time series models, where future values are predicted based on\nhistorical data. Accurate forecasting is essential for making well-informed\ndecisions across industries. When it comes to electric vehicles (EVs), precise\npredictions play a key role in planning infrastructure development, load\nbalancing, and energy management. This study introduces a BI-LSTM embedding\ndenoising autoencoder model (BDM) designed to address time series problems,\nfocusing on short-term EV charging load prediction. The performance of the\nproposed model is evaluated by comparing it with benchmark models like\nTransformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the\nproposed model outperforms the benchmark models in four of the five-time steps,\ndemonstrating its effectiveness for time series forecasting. This research\nmakes a significant contribution to enhancing time series forecasting, thereby\nimproving decision-making processes."}
{"id": "2509.17183", "pdf": "https://arxiv.org/pdf/2509.17183", "abs": "https://arxiv.org/abs/2509.17183", "authors": ["Junsong Li", "Jie Zhou", "Bihao Zhan", "Yutao Yang", "Qianjun Pan", "Shilian Chen", "Tianyu Huai", "Xin Li", "Qin Chen", "Liang He"], "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub."}
{"id": "2509.17186", "pdf": "https://arxiv.org/pdf/2509.17186", "abs": "https://arxiv.org/abs/2509.17186", "authors": ["Dehao Zhang", "Malu Zhang", "Shuai Wang", "Jingya Wang", "Wenjie Wei", "Zeyu Ma", "Guoqing Wang", "Yang Yang", "HaiZhou Li"], "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The explosive growth in sequence length has intensified the demand for\neffective and efficient long sequence modeling. Benefiting from intrinsic\noscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently\nextract frequency components from input signals and encode them into\nspatiotemporal spike trains, making them well-suited for long sequence\nmodeling. However, RF neurons exhibit limited effective memory capacity and a\ntrade-off between energy efficiency and training speed on complex temporal\ntasks. Inspired by the dendritic structure of biological neurons, we propose a\nDendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a\nmulti-dendritic and soma architecture. Each dendritic branch encodes specific\nfrequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons,\nthereby collectively achieving comprehensive frequency representation.\nFurthermore, we introduce an adaptive threshold mechanism into the soma\nstructure that adjusts the threshold based on historical spiking activity,\nreducing redundant spikes while maintaining training efficiency in long\nsequence tasks. Extensive experiments demonstrate that our method maintains\ncompetitive accuracy while substantially ensuring sparse spikes without\ncompromising computational efficiency during training. These results underscore\nits potential as an effective and efficient solution for long sequence modeling\non edge platforms."}
{"id": "2509.17187", "pdf": "https://arxiv.org/pdf/2509.17187", "abs": "https://arxiv.org/abs/2509.17187", "authors": ["Lalith Bharadwaj Baru", "Kamalaker Dadi", "Tapabrata Chakraborti", "Raju S. Bapi"], "title": "Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 (11 pages, 2 figures, 1 table, and 26 references)", "summary": "Accurate segmentation of medical images is challenging due to unclear lesion\nboundaries and mask variability. We introduce \\emph{Segmentation Sch\\\"{o}dinger\nBridge (SSB)}, the first application of Sch\\\"{o}dinger Bridge for ambiguous\nmedical image segmentation, modelling joint image-mask dynamics to enhance\nperformance. SSB preserves structural integrity, delineates unclear boundaries\nwithout additional guidance, and maintains diversity using a novel loss\nfunction. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$)\nto quantify inter-rater variability, capturing both diversity and consensus.\nSSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER\n(in-house) datasets."}
{"id": "2509.17190", "pdf": "https://arxiv.org/pdf/2509.17190", "abs": "https://arxiv.org/abs/2509.17190", "authors": ["Kabir Hamzah Muhammad", "Marawan Elbatel", "Yi Qin", "Xiaomeng Li"], "title": "Echo-Path: Pathology-Conditioned Echo Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures, MICCAI-AMAI2025 Workshop", "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nglobally, and echocardiography is critical for diagnosis of both common and\ncongenital cardiac conditions. However, echocardiographic data for certain\npathologies are scarce, hindering the development of robust automated diagnosis\nmodels. In this work, we propose Echo-Path, a novel generative framework to\nproduce echocardiogram videos conditioned on specific cardiac pathologies.\nEcho-Path can synthesize realistic ultrasound video sequences that exhibit\ntargeted abnormalities, focusing here on atrial septal defect (ASD) and\npulmonary arterial hypertension (PAH). Our approach introduces a\npathology-conditioning mechanism into a state-of-the-art echo video generator,\nallowing the model to learn and control disease-specific structural and motion\npatterns in the heart. Quantitative evaluation demonstrates that the synthetic\nvideos achieve low distribution distances, indicating high visual fidelity.\nClinically, the generated echoes exhibit plausible pathology markers.\nFurthermore, classifiers trained on our synthetic data generalize well to real\ndata and, when used to augment real training sets, it improves downstream\ndiagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset\nare available here https://github.com/Marshall-mk/EchoPathv1"}
{"id": "2509.17196", "pdf": "https://arxiv.org/pdf/2509.17196", "abs": "https://arxiv.org/abs/2509.17196", "authors": ["Xuyang Ge", "Wentao Shu", "Jiaxing Wu", "Yunhua Zhou", "Zhengfu He", "Xipeng Qiu"], "title": "Evolution of Concepts in Language Model Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 25 figures", "summary": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics."}
{"id": "2509.17197", "pdf": "https://arxiv.org/pdf/2509.17197", "abs": "https://arxiv.org/abs/2509.17197", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "11 pages", "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings."}
{"id": "2509.17206", "pdf": "https://arxiv.org/pdf/2509.17206", "abs": "https://arxiv.org/abs/2509.17206", "authors": ["Gunner Stone", "Sushmita Sarker", "Alireza Tavakkoli"], "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features."}
{"id": "2509.17207", "pdf": "https://arxiv.org/pdf/2509.17207", "abs": "https://arxiv.org/abs/2509.17207", "authors": ["Gunner Stone", "Youngsook Choi", "Alireza Tavakkoli", "Ankita Shukla"], "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Pre-training strategies play a critical role in advancing the performance of\ntransformer-based models for 3D point cloud tasks. In this paper, we introduce\nPoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed to\nimprove token robustness through a corruption-reconstruction framework. Unlike\ntraditional mask-based reconstruction tasks that hide data segments for later\nprediction, Point-RTD corrupts point cloud tokens and leverages a\ndiscriminator-generator architecture for denoising. This shift enables more\neffective learning of structural priors and significantly enhances model\nperformance and efficiency. On the ShapeNet dataset, Point-RTD reduces\nreconstruction error by over 93% compared to PointMAE, and achieves more than\n14x lower Chamfer Distance on the test set. Our method also converges faster\nand yields higher classification accuracy on ShapeNet, ModelNet10, and\nModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework\nin every case."}
{"id": "2509.17255", "pdf": "https://arxiv.org/pdf/2509.17255", "abs": "https://arxiv.org/abs/2509.17255", "authors": ["Thorsten Hellert", "Drew Bertwistle", "Simon C. Leemann", "Antonin Sulc", "Marco Venturini"], "title": "Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User Facility Particle Accelerator", "categories": ["physics.acc-ph", "cs.AI"], "comment": null, "summary": "We present the first language-model-driven agentic artificial intelligence\n(AI) system to autonomously execute multi-stage physics experiments on a\nproduction synchrotron light source. Implemented at the Advanced Light Source\nparticle accelerator, the system translates natural language user prompts into\nstructured execution plans that combine archive data retrieval, control-system\nchannel resolution, automated script generation, controlled machine\ninteraction, and analysis. In a representative machine physics task, we show\nthat preparation time was reduced by two orders of magnitude relative to manual\nscripting even for a system expert, while operator-standard safety constraints\nwere strictly upheld. Core architectural features, plan-first orchestration,\nbounded tool access, and dynamic capability selection, enable transparent,\nauditable execution with fully reproducible artifacts. These results establish\na blueprint for the safe integration of agentic AI into accelerator experiments\nand demanding machine physics studies, as well as routine operations, with\ndirect portability across accelerators worldwide and, more broadly, to other\nlarge-scale scientific infrastructures."}
{"id": "2509.17276", "pdf": "https://arxiv.org/pdf/2509.17276", "abs": "https://arxiv.org/abs/2509.17276", "authors": ["Runjia Zeng", "James Chenhao Liang", "Cheng Han", "Zhiwen Cao", "Jiahao Liu", "Xiaojun Quan", "Yingjie Victor Chen", "Lifu Huang", "Tong Geng", "Qifan Wang", "Dongfang Liu"], "title": "Probabilistic Token Alignment for Large Language Model Fusion", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "NeurIPS 2025", "summary": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/."}
{"id": "2509.17280", "pdf": "https://arxiv.org/pdf/2509.17280", "abs": "https://arxiv.org/abs/2509.17280", "authors": ["Thomas Serre", "Ellie Pavlick"], "title": "From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Generative pretraining (the \"GPT\" in ChatGPT) enables language models to\nlearn from vast amounts of internet text without human supervision. This\napproach has driven breakthroughs across AI by allowing deep neural networks to\nlearn from massive, unstructured datasets. We use the term foundation models to\nrefer to large pretrained systems that can be adapted to a wide range of tasks\nwithin and across domains, and these models are increasingly applied beyond\nlanguage to the brain sciences. These models achieve strong predictive\naccuracy, raising hopes that they might illuminate computational principles.\nBut predictive success alone does not guarantee scientific understanding. Here,\nwe outline how foundation models can be productively integrated into the brain\nsciences, highlighting both their promise and their limitations. The central\nchallenge is to move from prediction to explanation: linking model computations\nto mechanisms underlying neural activity and cognition."}
{"id": "2509.17281", "pdf": "https://arxiv.org/pdf/2509.17281", "abs": "https://arxiv.org/abs/2509.17281", "authors": ["Raisa Amiruddin", "Nikolay Y. Yordanov", "Nazanin Maleki", "Pascal Fehringer", "Athanasios Gkampenis", "Anastasia Janas", "Kiril Krantchev", "Ahmed Moawad", "Fabian Umeh", "Salma Abosabie", "Sara Abosabie", "Albara Alotaibi", "Mohamed Ghonim", "Mohanad Ghonim", "Sedra Abou Ali Mhana", "Nathan Page", "Marko Jakovljevic", "Yasaman Sharifi", "Prisha Bhatia", "Amirreza Manteghinejad", "Melisa Guelen", "Michael Veronesi", "Virginia Hill", "Tiffany So", "Mark Krycia", "Bojan Petrovic", "Fatima Memon", "Justin Cramer", "Elizabeth Schrickel", "Vilma Kosovic", "Lorenna Vidal", "Gerard Thompson", "Ichiro Ikuta", "Basimah Albalooshy", "Ali Nabavizadeh", "Nourel Hoda Tahon", "Karuna Shekdar", "Aashim Bhatia", "Claudia Kirsch", "Gennaro D'Anna", "Philipp Lohmann", "Amal Saleh Nour", "Andriy Myronenko", "Adam Goldman-Yassen", "Janet R. Reid", "Sanjay Aneja", "Spyridon Bakas", "Mariam Aboian"], "title": "Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "23 pages, 9 figures, 1 table, 3 supplementary tables", "summary": "High-quality reference standard image data creation by neuroradiology experts\nfor automated clinical tools can be a powerful tool for neuroradiology &\nartificial intelligence education. We developed a multimodal educational\napproach for students and trainees during the MICCAI Brain Tumor Segmentation\nLighthouse Challenge 2025, a landmark initiative to develop accurate brain\ntumor segmentation algorithms. Fifty-six medical students & radiology trainees\nvolunteered to annotate brain tumor MR images for the BraTS challenges of 2023\n& 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56\nannotators, 14 select volunteers were then paired with neuroradiology faculty\nfor guided one-on-one annotation sessions for BraTS 2025. Lectures on\nneuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were\norganized online. Annotators & audience members completed surveys on their\nperceived knowledge before & after annotations & lectures respectively.\nFourteen coordinators, each paired with a neuroradiologist, completed the data\nannotation process, averaging 1322.9+/-760.7 hours per dataset per pair and\n1200 segmentations in total. On a scale of 1-10, annotation coordinators\nreported significant increase in familiarity with image segmentation software\npre- and post-annotation, moving from initial average of 6+/-2.9 to final\naverage of 8.9+/-1.1, and significant increase in familiarity with brain tumor\nfeatures pre- and post-annotation, moving from initial average of 6.2+/-2.4 to\nfinal average of 8.1+/-1.2. We demonstrate an innovative offering for providing\nneuroradiology & AI education through an image segmentation challenge to\nenhance understanding of algorithm development, reinforce the concept of data\nreference standard, and diversify opportunities for AI-driven image analysis\namong future physicians."}
{"id": "2509.17283", "pdf": "https://arxiv.org/pdf/2509.17283", "abs": "https://arxiv.org/abs/2509.17283", "authors": ["Licheng Zhan", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Building compliance checking (BCC) is a critical process for ensuring that\nconstructed facilities meet regulatory standards. A core component of BCC is\nthe accurate enumeration of facility types and their spatial distribution.\nDespite its importance, this problem has been largely overlooked in the\nliterature, posing a significant challenge for BCC and leaving a critical gap\nin existing workflows. Performing this task manually is time-consuming and\nlabor-intensive. Recent advances in large language models (LLMs) offer new\nopportunities to enhance automation by combining visual recognition with\nreasoning capabilities. In this paper, we introduce a new task for BCC:\nautomated facility enumeration, which involves validating the quantity of each\nfacility type against statutory requirements. To address it, we propose a novel\nmethod that integrates door detection with LLM-based reasoning. We are the\nfirst to apply LLMs to this task and further enhance their performance through\na Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse\ndatasets and facility types. Experiments on both real-world and synthetic floor\nplan data demonstrate the effectiveness and robustness of our method."}
{"id": "2509.17292", "pdf": "https://arxiv.org/pdf/2509.17292", "abs": "https://arxiv.org/abs/2509.17292", "authors": ["Jun Seo Kim", "Hyemi Kim", "Woo Joo Oh", "Hongjin Cho", "Hochul Lee", "Hye Hyeon Kim"], "title": "Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cognitive distortions have been closely linked to mental health disorders,\nyet their automatic detection remained challenging due to contextual ambiguity,\nco-occurrence, and semantic overlap. We proposed a novel framework that\ncombines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)\narchitecture to enhance interpretability and expression-level reasoning. Each\nutterance was decomposed into Emotion, Logic, and Behavior (ELB) components,\nwhich were processed by LLMs to infer multiple distortion instances, each with\na predicted type, expression, and model-assigned salience score. These\ninstances were integrated via a Multi-View Gated Attention mechanism for final\nclassification. Experiments on Korean (KoACD) and English (Therapist QA)\ndatasets demonstrate that incorporating ELB and LLM-inferred salience scores\nimproves classification performance, especially for distortions with high\ninterpretive ambiguity. Our results suggested a psychologically grounded and\ngeneralizable approach for fine-grained reasoning in mental health NLP."}
{"id": "2509.17317", "pdf": "https://arxiv.org/pdf/2509.17317", "abs": "https://arxiv.org/abs/2509.17317", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Most languages lack sufficient data for large-scale monolingual pretraining,\ncreating a \"data wall.\" Multilingual pretraining helps but is limited by\nlanguage imbalance and the \"curse of multilinguality.\" An alternative is to\ntranslate high-resource text with machine translation (MT), which raises three\nquestions: (1) How does MT-derived data scale with model capacity? (2) Can\nsource-side transformations (e.g., simplifying English with an LLM) improve\ngeneralization to native text? (3) How well do models pretrained on MT-derived\ndata adapt when continually trained on limited native text? We investigate\nthese questions by translating English into Indonesian and Tamil--two\ntypologically distant, lower-resource languages--and pretraining GPT-2 models\n(124M-774M) on native or MT-derived corpora from raw and LLM-simplified\nEnglish. We evaluate cross-entropy loss on native text, along with accuracy on\nsyntactic probes and downstream tasks. Our results show that (1) MT-pretrained\nmodels benefit from scaling; (2) source-side simplification harms\ngeneralization to native text; and (3) adapting MT-pretrained models on native\ntext often yields better performance than native-only models, even with less\nnative data. However, tasks requiring cultural nuance (e.g., toxicity\ndetection) demand more exposure to native data."}
{"id": "2509.17325", "pdf": "https://arxiv.org/pdf/2509.17325", "abs": "https://arxiv.org/abs/2509.17325", "authors": ["Weihua Du", "Hailei Gong", "Zhan Ling", "Kang Liu", "Lingfeng Shen", "Xuesong Yao", "Yufei Xu", "Dingyuan Shi", "Yiming Yang", "Jiecao Chen"], "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "22 pages. Project available at https://github.com/StigLidu/CodeGym", "summary": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage\nexternal tools to solve diverse tasks and interface with the real world.\nHowever, current training practices largely rely on supervised fine-tuning\n(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,\nand generalize poorly beyond development settings, leading to brittleness with\nnew tools and unseen workflows. Because code execution reflects many structures\nof real-world workflows, coding problems provide a natural basis for building\nagent training environments. Motivated by this, we introduce CodeGym, a\nscalable framework that synthesizes diverse, verifiable, and controllable\nmulti-turn tool-use environments for agent RL, enabling LLM agents to explore\nand master various workflows actively. CodeGym rewrites static coding problems\ninto interactive environments by extracting atomic functions or logic into\ncallable tools, yielding verifiable tasks that span various tool-execution\nworkflows. Models of varying sizes and chain-of-thought configurations, trained\nin CodeGym, exhibit consistent out-of-distribution generalizability; for\nexample, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points\non the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step\ntoward scalable general-purpose RL environments that align with real-world\nagent workflows."}
{"id": "2509.17334", "pdf": "https://arxiv.org/pdf/2509.17334", "abs": "https://arxiv.org/abs/2509.17334", "authors": ["Jiawen Wei", "Elena Verona", "Andrea Bertolini", "Gianmarco Mengaldo"], "title": "Explainability matters: The effect of liability rules on the healthcare sector", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Explainability, the capability of an artificial intelligence system (AIS) to\nexplain its outcomes in a manner that is comprehensible to human beings at an\nacceptable level, has been deemed essential for critical sectors, such as\nhealthcare. Is it really the case? In this perspective, we consider two extreme\ncases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with\nexplainability) for a thorough analysis. We discuss how the level of automation\nand explainability of AIS can affect the determination of liability among the\nmedical practitioner/facility and manufacturer of AIS. We argue that\nexplainability plays a crucial role in setting a responsibility framework in\nhealthcare, from a legal standpoint, to shape the behavior of all involved\nparties and mitigate the risk of potential defensive medicine practices."}
{"id": "2509.17348", "pdf": "https://arxiv.org/pdf/2509.17348", "abs": "https://arxiv.org/abs/2509.17348", "authors": ["Yujie Feng", "Jian Li", "Xiaoyu Dong", "Pengfei Xu", "Xiaohui Zhou", "Yujia Zhang", "Zexin LU", "Yasha Wang", "Alan Zhao", "Xu Chu", "Xiao-Ming Wu"], "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Continual learning (CL) is essential for deploying large language models\n(LLMs) in dynamic real-world environments without the need for costly\nretraining. Recent model merging-based methods have attracted significant\nattention, but they still struggle to effectively manage the trade-off between\nlearning new knowledge and preventing forgetting, a challenge largely stemming\nfrom suboptimal number of merges and merging frequency. In this paper, we\nintroduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework\nthat utilizes learning and forgetting signals from the training trajectory to\ndynamically monitor the model's training status. Guided by dynamic monitoring,\nthe training trajectory-guided merge controller adaptively determines the\ntiming and frequency of iterative fusion, while the rehearsal-based knowledge\nfusion module computes the merging weights and executes the fusion.\nComprehensive experiments on three CL benchmarks with various model sizes (from\n770M to 13B) demonstrate that AimMerging achieves significant performance\nimprovements over existing state-of-the-art methods, with an average relative\nimprovement of 80% and 59% on FWT and BWT, respectively. The source code is\nprovided for reproducibility."}
{"id": "2509.17349", "pdf": "https://arxiv.org/pdf/2509.17349", "abs": "https://arxiv.org/abs/2509.17349", "authors": ["Peter Polák", "Sara Papi", "Luisa Bentivogli", "Ondřej Bojar"], "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems."}
{"id": "2509.17361", "pdf": "https://arxiv.org/pdf/2509.17361", "abs": "https://arxiv.org/abs/2509.17361", "authors": ["Ruihan Luo", "Xuanjing Chen", "Ziyang Ding"], "title": "SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized content marketing has become a crucial strategy for digital\nplatforms, aiming to deliver tailored advertisements and recommendations that\nmatch user preferences. Traditional recommendation systems often suffer from\ntwo limitations: (1) reliance on limited supervised signals derived from\nexplicit user feedback, and (2) vulnerability to noisy or unintentional\ninteractions. To address these challenges, we propose SeqUDA-Rec, a novel deep\nlearning framework that integrates user behavior sequences with global\nunsupervised data augmentation to enhance recommendation accuracy and\nrobustness. Our approach first constructs a Global User-Item Interaction Graph\n(GUIG) from all user behavior sequences, capturing both local and global item\nassociations. Then, a graph contrastive learning module is applied to generate\nrobust embeddings, while a sequential Transformer-based encoder models users'\nevolving preferences. To further enhance diversity and counteract sparse\nsupervised labels, we employ a GAN-based augmentation strategy, generating\nplausible interaction patterns and supplementing training data. Extensive\nexperiments on two real-world marketing datasets (Amazon Ads and TikTok Ad\nClicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art\nbaselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%\nimprovement in NDCG@10 and 11.3% improvement in HR@10, proving its\neffectiveness in personalized advertising and intelligent content\nrecommendation."}
{"id": "2509.17365", "pdf": "https://arxiv.org/pdf/2509.17365", "abs": "https://arxiv.org/abs/2509.17365", "authors": ["Amanuel Tafese Dufera"], "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic image captioning, a multifaceted task bridging computer vision and\nnatural lan- guage processing, aims to generate descriptive textual content\nfrom visual input. While Convolutional Neural Networks (CNNs) and Long\nShort-Term Memory (LSTM) networks have achieved significant advancements, they\npresent limitations. The inherent sequential nature of RNNs leads to sluggish\ntraining and inference times. LSTMs further struggle with retaining information\nfrom earlier sequence elements when dealing with very long se- quences. This\nproject presents a comprehensive guide to constructing and comprehending\ntransformer models for image captioning. Transformers employ self-attention\nmechanisms, capturing both short- and long-range dependencies within the data.\nThis facilitates efficient parallelization during both training and inference\nphases. We leverage the well-established Transformer architecture, recognized\nfor its effectiveness in managing sequential data, and present a meticulous\nmethodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,\nconstruct a model architecture that integrates an EfficientNetB0 CNN for fea-\nture extraction, and train the model with attention mechanisms incorporated.\nOur approach exemplifies the utilization of parallelization for efficient\ntraining and inference. You can find the project on GitHub."}
{"id": "2509.17401", "pdf": "https://arxiv.org/pdf/2509.17401", "abs": "https://arxiv.org/abs/2509.17401", "authors": ["Jinyeong Kim", "Junhyeok Kim", "Yumin Shim", "Joohyeok Kim", "Sunyoung Jung", "Seong Jae Hwang"], "title": "Interpreting vision transformers via residual replacement model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "How do vision transformers (ViTs) represent and process the world? This paper\naddresses this long-standing question through the first systematic analysis of\n6.6K features across all layers, extracted via sparse autoencoders, and by\nintroducing the residual replacement model, which replaces ViT computations\nwith interpretable features in the residual stream. Our analysis reveals not\nonly a feature evolution from low-level patterns to high-level semantics, but\nalso how ViTs encode curves and spatial positions through specialized feature\ntypes. The residual replacement model scalably produces a faithful yet\nparsimonious circuit for human-scale interpretability by significantly\nsimplifying the original computations. As a result, this framework enables\nintuitive understanding of ViT mechanisms. Finally, we demonstrate the utility\nof our framework in debiasing spurious correlations."}
{"id": "2509.17404", "pdf": "https://arxiv.org/pdf/2509.17404", "abs": "https://arxiv.org/abs/2509.17404", "authors": ["Wei Tan", "Shun Lei", "Huaicheng Zhang", "Guangzheng Li", "Yixuan Zhang", "Hangting Chen", "Jianwei Yu", "Rongzhi Gu", "Dong Yu"], "title": "SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Artificial Intelligence Generated Content (AIGC) is currently a popular\nresearch area. Among its various branches, song generation has attracted\ngrowing interest. Despite the abundance of available songs, effective data\npreparation remains a significant challenge. Converting these songs into\ntraining-ready datasets typically requires extensive manual labeling, which is\nboth time consuming and costly. To address this issue, we propose SongPrep, an\nautomated preprocessing pipeline designed specifically for song data. This\nframework streamlines key processes such as source separation, structure\nanalysis, and lyric recognition, producing structured data that can be directly\nused to train song generation models. Furthermore, we introduce SongPrepE2E, an\nend-to-end structured lyrics recognition model based on pretrained language\nmodels. Without the need for additional source separation, SongPrepE2E is able\nto analyze the structure and lyrics of entire songs and provide precise\ntimestamps. By leveraging context from the whole song alongside pretrained\nsemantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and\nWord Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks\ndemonstrate that training song generation models with the data output by\nSongPrepE2E enables the generated songs to closely resemble those produced by\nhumans."}
{"id": "2509.17406", "pdf": "https://arxiv.org/pdf/2509.17406", "abs": "https://arxiv.org/abs/2509.17406", "authors": ["Jonathan Wuntu", "Muhamad Dwisnanto Putro", "Rendy Syahputra"], "title": "Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Indonesia's marine ecosystems, part of the globally recognized Coral\nTriangle, are among the richest in biodiversity, requiring efficient monitoring\ntools to support conservation. Traditional fish detection methods are\ntime-consuming and demand expert knowledge, prompting the need for automated\nsolutions. This study explores the implementation of YOLOv10-nano, a\nstate-of-the-art deep learning model, for real-time marine fish detection in\nIndonesian waters, using test data from Bunaken National Marine Park. YOLOv10's\narchitecture, featuring improvements like the CSPNet backbone, PAN for feature\nfusion, and Pyramid Spatial Attention Block, enables efficient and accurate\nobject detection even in complex environments. The model was evaluated on the\nDeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano\nachieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606\nwhile maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It\nalso delivered an average inference speed of 29.29 FPS on the CPU, making it\nsuitable for real-time deployment. Although OpenImages V7-Fish alone provided\nlower accuracy, it complemented DeepFish in enhancing model robustness.\nOverall, this study demonstrates YOLOv10-nano's potential for efficient,\nscalable marine fish monitoring and conservation applications in data-limited\nenvironments."}
{"id": "2509.17413", "pdf": "https://arxiv.org/pdf/2509.17413", "abs": "https://arxiv.org/abs/2509.17413", "authors": ["Masako Kishida"], "title": "Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Ensuring the safety of neural networks under input uncertainty is a\nfundamental challenge in safety-critical applications. This paper builds on and\nexpands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP)\nframework for neural network verification to a distributionally robust and\ntail-risk-aware setting by integrating worst-case Conditional Value-at-Risk\n(WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The\nresulting conditions remain SDP-checkable and explicitly account for tail risk.\nThis integration broadens input-uncertainty geometry-covering ellipsoids,\npolytopes, and hyperplanes-and extends applicability to safety-critical domains\nwhere tail-event severity matters. Applications to closed-loop reachability of\ncontrol systems and classification are demonstrated through numerical\nexperiments, illustrating how the risk level $\\varepsilon$ trades conservatism\nfor tolerance to tail events-while preserving the computational structure of\nprior QC/SDP methods for neural network verification and robustness analysis."}
{"id": "2509.17446", "pdf": "https://arxiv.org/pdf/2509.17446", "abs": "https://arxiv.org/abs/2509.17446", "authors": ["Haofeng Huang", "Yifei Han", "Long Zhang", "Bin Li", "Yangfan He"], "title": "MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to ICASSP 2026", "summary": "Multimodal intent recognition (MMIR) suffers from weak semantic grounding and\npoor robustness under noisy or rare-class conditions. We propose MVCL-DAF++,\nwhich extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive\nalignment, aligning instances to class-level prototypes to enhance semantic\nconsistency; and (2) Coarse-to-fine attention fusion, integrating global\nmodality summaries with token-level features for hierarchical cross-modal\ninteraction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new\nstate-of-the-art results, improving rare-class recognition by +1.05\\% and\n+4.18\\% WF1, respectively. These results demonstrate the effectiveness of\nprototype-guided learning and coarse-to-fine fusion for robust multimodal\nunderstanding. The source code is available at\nhttps://github.com/chr1s623/MVCL-DAF-PlusPlus."}
{"id": "2509.17452", "pdf": "https://arxiv.org/pdf/2509.17452", "abs": "https://arxiv.org/abs/2509.17452", "authors": ["Dujin Lee", "Sojung An", "Jungmyung Wi", "Kuniaki Saito", "Donghyun Kim"], "title": "Training-Free Label Space Alignment for Universal Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Universal domain adaptation (UniDA) transfers knowledge from a labeled source\ndomain to an unlabeled target domain, where label spaces may differ and the\ntarget domain may contain private classes. Previous UniDA methods primarily\nfocused on visual space alignment but often struggled with visual ambiguities\ndue to content differences, which limited their robustness and\ngeneralizability. To overcome this, we introduce a novel approach that\nleverages the strong \\textit{zero-shot capabilities} of recent vision-language\nfoundation models (VLMs) like CLIP, concentrating solely on label space\nalignment to enhance adaptation stability. CLIP can generate task-specific\nclassifiers based only on label names. However, adapting CLIP to UniDA is\nchallenging because the label space is not fully known in advance. In this\nstudy, we first utilize generative vision-language models to identify unknown\ncategories in the target domain. Noise and semantic ambiguities in the\ndiscovered labels -- such as those similar to source labels (e.g., synonyms,\nhypernyms, hyponyms) -- complicate label alignment. To address this, we propose\na training-free label-space alignment method for UniDA (\\ours). Our method\naligns label spaces instead of visual spaces by filtering and refining noisy\nlabels between the domains. We then construct a \\textit{universal classifier}\nthat integrates both shared knowledge and target-private class information,\nthereby improving generalizability under domain shifts. The results reveal that\nthe proposed method considerably outperforms existing UniDA techniques across\nkey DomainBed benchmarks, delivering an average improvement of\n\\textcolor{blue}{+7.9\\%}in H-score and \\textcolor{blue}{+6.1\\%} in H$^3$-score.\nFurthermore, incorporating self-training further enhances performance and\nachieves an additional (\\textcolor{blue}{+1.6\\%}) increment in both H- and\nH$^3$-scores."}
{"id": "2509.17455", "pdf": "https://arxiv.org/pdf/2509.17455", "abs": "https://arxiv.org/abs/2509.17455", "authors": ["Haoyang Chen", "Kumiko Tanaka-Ishii"], "title": "Codifying Natural Langauge Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to Journal of Automated Software Engineering", "summary": "We explore the applicability of text-to-code to solve real-world problems\nthat are typically solved in natural language, such as legal judgment and\nmedical QA. Unlike previous works, our approach leverages the explicit\nreasoning provided by program generation. We present ICRAG, a framework that\ntransforms natural language into executable programs through iterative\nrefinement using external knowledge from domain resources and GitHub. Across 13\nbenchmarks, ICRAG achieves up to 161.1\\% relative improvement. We provide a\ndetailed analysis of the generated code and the impact of external knowledge,\nand we discuss the limitations of applying text-to-code approaches to\nreal-world natural language tasks."}
{"id": "2509.17457", "pdf": "https://arxiv.org/pdf/2509.17457", "abs": "https://arxiv.org/abs/2509.17457", "authors": ["Paweł Jakub Borsukiewicz", "Jordan Samhi", "Jacques Klein", "Tegawendé F. Bissyandé"], "title": "Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.10; I.4.m"], "comment": "22 pages; 24 tables; 11 figures", "summary": "The proliferation of facial recognition systems presents major privacy risks,\ndriving the need for effective countermeasures. Current adversarial techniques\napply generalized methods rather than adapting to individual facial\ncharacteristics, limiting their effectiveness and inconspicuousness. In this\nwork, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique\nthat identifies which facial areas contribute most to recognition at an\nindividual level. Unlike adversarial attack methods that aim to fool\nrecognition systems, LEAM is an explainability technique designed to understand\nhow these systems work, providing insights that could inform future privacy\nprotection research. We integrate LEAM with a face parser to analyze data from\n1000 individuals across 9 pre-trained facial recognition models.\n  Our analysis reveals that while different layers within facial recognition\nmodels vary significantly in their focus areas, these models generally\nprioritize similar facial regions across architectures when considering their\noverall activation patterns, which show significantly higher similarity between\nimages of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.\ndifferent individuals (0.04-0.13), validating the existence of person-specific\nrecognition patterns. Our results show that facial recognition models\nprioritize the central region of face images (with nose areas accounting for\n18.9-29.7% of critical recognition regions), while still distributing attention\nacross multiple facial fragments. Proper selection of relevant facial areas was\nconfirmed using validation occlusions, based on just 1% of the most relevant,\nLEAM-identified, image pixels, which proved to be transferable across different\nmodels. Our findings establish the foundation for future individually tailored\nprivacy protection systems centered around LEAM's choice of areas to be\nperturbed."}
{"id": "2509.17466", "pdf": "https://arxiv.org/pdf/2509.17466", "abs": "https://arxiv.org/abs/2509.17466", "authors": ["Migyeong Yang", "Kyungah Lee", "Jinyoung Han", "SoHyun Park", "Young-Ho Kim"], "title": "Autiverse: Eliciting Autistic Adolescents' Daily Narratives through AI-guided Multimodal Journaling", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "19 pages excluding reference", "summary": "Journaling can potentially serve as an effective method for autistic\nadolescents to improve narrative skills. However, its text-centric nature and\nhigh executive functioning demands present barriers to practice. We present\nAutiverse, an AI-guided multimodal journaling app for tablets that scaffolds\nstorytelling through conversational prompts and visual supports. Autiverse\nelicits key details through a stepwise dialogue with peer-like, customizable AI\nand composes them into an editable four-panel comic strip. Through a two-week\ndeployment study with 10 autistic adolescent-parent dyads, we examine how\nAutiverse supports autistic adolescents to organize their daily experience and\nemotion. Autiverse helped them construct coherent narratives, while enabling\nparents to learn additional details of their child's events and emotions. The\ncustomized AI peer created a comfortable space for sharing, fostering enjoyment\nand a strong sense of agency. We discuss the implications of designing\ntechnologies that complement autistic adolescents' strengths while ensuring\ntheir autonomy and safety in sharing experiences."}
{"id": "2509.17470", "pdf": "https://arxiv.org/pdf/2509.17470", "abs": "https://arxiv.org/abs/2509.17470", "authors": ["Mohammadreza Sharifi", "Danial Ahmadzadeh"], "title": "Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for Entity Resolution", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": "Accepted at ICCKE 2025 Conference. 6 tables, 7 figures", "summary": "Entity resolution plays a significant role in enterprise systems where data\nintegrity must be rigorously maintained. Traditional methods often struggle\nwith handling noisy data or semantic understanding, while modern methods suffer\nfrom computational costs or the excessive need for parallel computation. In\nthis study, we introduce a scalable hybrid framework, which is designed to\naddress several important problems, including scalability, noise robustness,\nand reliable results. We utilized a pre-trained language model to encode each\nstructured data into corresponding semantic embedding vectors. Subsequently,\nafter retrieving a semantically relevant subset of candidates, we apply a\nsyntactic verification stage using fuzzy string matching techniques to refine\nclassification on the unlabeled data. This approach was applied to a real-world\nentity resolution task, which exposed a linkage between a central user\nmanagement database and numerous shared hosting server records. Compared to\nother methods, this approach exhibits an outstanding performance in terms of\nboth processing time and robustness, making it a reliable solution for a\nserver-side product. Crucially, this efficiency does not compromise results, as\nthe system maintains a high retrieval recall of approximately 0.97. The\nscalability of the framework makes it deployable on standard CPU-based\ninfrastructure, offering a practical and effective solution for\nenterprise-level data integrity auditing."}
{"id": "2509.17477", "pdf": "https://arxiv.org/pdf/2509.17477", "abs": "https://arxiv.org/abs/2509.17477", "authors": ["Yeonsun Yang", "Sang Won Lee", "Jean Y. Song", "Sangdoo Yun", "Young-Ho Kim"], "title": "LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages except reference", "summary": "Non-native English speakers performing English-related tasks at work struggle\nto sustain ESL learning, despite their motivation. Often, study materials are\ndisconnected from their work context. Although workers rely on LLM assistants\nto address their immediate needs, these interactions may not directly\ncontribute to their English skills. We present LingoQ, an AI-mediated system\nthat allows workers to practice English using quizzes generated from their LLM\nqueries during work. LingoQ leverages these queries using AI to generate\npersonalized quizzes that workers can review and practice on their smartphones.\nWe conducted a three-week deployment study with 28 ESL workers to evaluate\nLingoQ. Participants valued the relevance of quizzes that reflect their own\ncontext, constantly engaging with the app during the study. This active\nengagement improved self-efficacy and led to learning gains for beginners and,\npotentially, for intermediate learners. We discuss opportunities of leveraging\nusers' reliance on LLMs to situate their learning in the user context for\nimproved learning."}
{"id": "2509.17481", "pdf": "https://arxiv.org/pdf/2509.17481", "abs": "https://arxiv.org/abs/2509.17481", "authors": ["Xingqi Wang", "Yiming Cui", "Xin Yao", "Shijin Wang", "Guoping Hu", "Xiaoyu Qin"], "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal ."}
{"id": "2509.17488", "pdf": "https://arxiv.org/pdf/2509.17488", "abs": "https://arxiv.org/abs/2509.17488", "authors": ["Shouju Wang", "Fenglin Yu", "Xirui Liu", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan"], "title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents", "categories": ["cs.CR", "cs.AI"], "comment": "To appear at EMNLP 2025 (Findings)", "summary": "The increasing autonomy of LLM agents in handling sensitive communications,\naccelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)\nframeworks, creates urgent privacy challenges. While recent work reveals\nsignificant gaps between LLMs' privacy Q&A performance and their agent\nbehavior, existing benchmarks remain limited to static, simplified scenarios.\nWe present PrivacyChecker, a model-agnostic, contextual integrity based\nmitigation approach that effectively reduces privacy leakage from 36.08% to\n7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving\ntask helpfulness. We also introduce PrivacyLens-Live, transforming static\nbenchmarks into dynamic MCP and A2A environments that reveal substantially\nhigher privacy risks in practical. Our modular mitigation approach integrates\nseamlessly into agent protocols through three deployment strategies, providing\npractical privacy protection for the emerging agentic ecosystem. Our data and\ncode will be made available at https://aka.ms/privacy_in_action."}
{"id": "2509.17489", "pdf": "https://arxiv.org/pdf/2509.17489", "abs": "https://arxiv.org/abs/2509.17489", "authors": ["Woongkyu Lee", "Junhee Cho", "Jungwook Choi"], "title": "MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have advanced code generation from\nsingle-function tasks to competitive-programming problems, but existing\nmulti-agent solutions either rely on costly large-scale ($>$ 30B) models or\ncollapse when downsized to small open-source models. We present MapCoder-Lite,\nwhich upgrades a single 7B model into four role-specialised agents-retriever,\nplanner, coder, and debugger-using only rank-32, role-specific LoRA adapters\n($<3\\%$ extra parameters). Three lightweight techniques make this possible: (i)\ntrajectory distillation from strong LLMs fixes format fragility in retrieval\nand debugging, (ii) supervisor-guided correction strengthens planning and\ncoding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient\nspecialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests\nshows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\\%$ to\n$28.3\\%$), eliminates all format failures, and closes to within six points of a\n32B baseline while cutting GPU memory and token-generation time by $4\\times$.\nThese results demonstrate that careful agent-wise fine-tuning unleashes\nhigh-quality multi-agent coding on a small language model."}
{"id": "2509.17492", "pdf": "https://arxiv.org/pdf/2509.17492", "abs": "https://arxiv.org/abs/2509.17492", "authors": ["Qinghua Lin", "Guang-Hai Liu", "Zuoyong Li", "Yang Li", "Yuting Jiang", "Xiang Wu"], "title": "Multimodal Medical Image Classification via Synergistic Learning Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal pathological images are usually in clinical diagnosis, but\ncomputer vision-based multimodal image-assisted diagnosis faces challenges with\nmodality fusion, especially in the absence of expert-annotated data. To achieve\nthe modality fusion in multimodal images with label scarcity, we propose a\nnovel ``pretraining + fine-tuning\" framework for multimodal semi-supervised\nmedical image classification. Specifically, we propose a synergistic learning\npretraining framework of consistency, reconstructive, and aligned learning. By\ntreating one modality as an augmented sample of another modality, we implement\na self-supervised learning pre-train, enhancing the baseline model's feature\nrepresentation capability. Then, we design a fine-tuning method for multimodal\nfusion. During the fine-tuning stage, we set different encoders to extract\nfeatures from the original modalities and provide a multimodal fusion encoder\nfor fusion modality. In addition, we propose a distribution shift method for\nmultimodal fusion features, which alleviates the prediction uncertainty and\noverfitting risks caused by the lack of labeled samples. We conduct extensive\nexperiments on the publicly available gastroscopy image datasets Kvasir and\nKvasirv2. Quantitative and qualitative results demonstrate that the proposed\nmethod outperforms the current state-of-the-art classification methods. The\ncode will be released at: https://github.com/LQH89757/MICS."}
{"id": "2509.17505", "pdf": "https://arxiv.org/pdf/2509.17505", "abs": "https://arxiv.org/abs/2509.17505", "authors": ["Tuğba Pamay Arslan", "Emircan Erol", "Gülşen Eryiğit"], "title": "CorefInst: Leveraging LLMs for Multilingual Coreference Resolution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.\n  Revision: July, 2025. Acceptance: August, 2025", "summary": "Coreference Resolution (CR) is a crucial yet challenging task in natural\nlanguage understanding, often constrained by task-specific architectures and\nencoder-based language models that demand extensive training and lack\nadaptability. This study introduces the first multilingual CR methodology which\nleverages decoder-only LLMs to handle both overt and zero mentions. The article\nexplores how to model the CR task for LLMs via five different instruction sets\nusing a controlled inference method. The approach is evaluated across three\nLLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when\ninstruction-tuned with a suitable instruction set, can surpass state-of-the-art\ntask-specific architectures. Specifically, our best model, a fully fine-tuned\nLlama 3.1 for multilingual CR, outperforms the leading multilingual CR model\n(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages\nin the CorefUD v1.2 dataset collection."}
{"id": "2509.17533", "pdf": "https://arxiv.org/pdf/2509.17533", "abs": "https://arxiv.org/abs/2509.17533", "authors": ["Anastasios Fanariotis", "Theofanis Orphanoudakis", "Vasilis Fotopoulos"], "title": "Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers", "categories": ["cs.ET", "cs.AI", "cs.LG"], "comment": null, "summary": "The deployment of machine learning (ML) models on microcontrollers (MCUs) is\nconstrained by strict energy, latency, and memory requirements, particularly in\nbattery-operated and real-time edge devices. While software-level optimizations\nsuch as quantization and pruning reduce model size and computation, hardware\nacceleration has emerged as a decisive enabler for efficient embedded\ninference. This paper evaluates the impact of Neural Processing Units (NPUs) on\nMCU-based ML execution, using the ARM Cortex-M55 core combined with the\nEthos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a\nrepresentative platform. A rigorous measurement methodology was employed,\nincorporating per-inference net energy accounting via GPIO-triggered\nhigh-resolution digital multimeter synchronization and idle-state subtraction,\nensuring accurate attribution of energy costs. Experimental results across six\nrepresentative ML models -including MiniResNet, MobileNetV2, FD-MobileNet,\nMNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains\nwhen inference is offloaded to the NPU. For moderate to large networks, latency\nimprovements ranged from 7x to over 125x, with per-inference net energy\nreductions up to 143x. Notably, the NPU enabled execution of models unsupported\non CPU-only paths, such as SSD-MobileNet, highlighting its functional as well\nas efficiency advantages. These findings establish NPUs as a cornerstone of\nenergy-aware embedded AI, enabling real-time, power-constrained ML inference at\nthe MCU level."}
{"id": "2509.17552", "pdf": "https://arxiv.org/pdf/2509.17552", "abs": "https://arxiv.org/abs/2509.17552", "authors": ["Tianle Zhang", "Wanlong Fang", "Jonathan Woo", "Paridhi Latawa", "Deepak A. Subramanian", "Alvin Chan"], "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "categories": ["cs.CL", "cs.AI"], "comment": "NIPS 2025", "summary": "The remarkable performance of Large Language Models (LLMs) can be enhanced\nwith test-time computation, which relies on external tools and even other deep\nlearning models. However, existing approaches for integrating non-text modality\nrepresentations into LLMs typically require additional costly supervised\ntraining, restricting on-the-fly adaptation to new domains and modalities. In\nthis work, we explore the feasibility of integrating representations from\nnon-text foundational models (FMs) into text-based LLMs in a training-free\nmanner. We propose In-Context Representation Learning (ICRL) as a\nproof-of-concept to allow LLMs to adaptively utilize non-text modality\nrepresentations with few-shot learning. Unlike traditional in-context learning,\nwhich incorporates text-label pairs, ICRL replaces text inputs with FM\nrepresentations, enabling the LLM to perform multi-modal inference without\nfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,\ninvestigating three core research questions: (i) how to map FM representations\ninto LLMs in a training-free manner, (ii) what factors influence ICRL\nperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. To\nthe best of our knowledge, ICRL is the first training-free framework for\nintegrating non-text modality representations into text-based LLMs, presenting\na promising direction for adaptable, multi-modal generalization."}
{"id": "2509.17561", "pdf": "https://arxiv.org/pdf/2509.17561", "abs": "https://arxiv.org/abs/2509.17561", "authors": ["Edwine Nabahirwa", "Wei Song", "Minghua Zhang", "Shufan Chen"], "title": "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "28 Pages, 12 Figures", "summary": "Underwater object detection (UOD) remains a critical challenge in computer\nvision due to underwater distortions which degrade low-level features and\ncompromise the reliability of even state-of-the-art detectors. While YOLO\nmodels have become the backbone of real-time object detection, little work has\nsystematically examined their robustness under these uniquely challenging\nconditions. This raises a critical question: Are YOLO models genuinely robust\nwhen operating under the chaotic and unpredictable conditions of underwater\nenvironments? In this study, we present one of the first comprehensive\nevaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated\nunderwater environments. Using a unified dataset of 10,000 annotated images\nfrom DUO and Roboflow100, we not only benchmark model robustness but also\nanalyze how distortions affect key low-level features such as texture, edges,\nand color. Our findings show that (1) YOLOv12 delivers the strongest overall\nperformance but is highly vulnerable to noise, and (2) noise disrupts edge and\ntexture features, explaining the poor detection performance in noisy images.\nClass imbalance is a persistent challenge in UOD. Experiments revealed that (3)\nimage counts and instance frequency primarily drive detection performance,\nwhile object appearance exerts only a secondary influence. Finally, we\nevaluated lightweight training-aware strategies: noise-aware sample injection,\nwhich improves robustness in both noisy and real-world conditions, and\nfine-tuning with advanced enhancement, which boosts accuracy in enhanced\ndomains but slightly lowers performance in original data, demonstrating strong\npotential for domain adaptation, respectively. Together, these insights provide\npractical guidance for building resilient and cost-efficient UOD systems."}
{"id": "2509.17566", "pdf": "https://arxiv.org/pdf/2509.17566", "abs": "https://arxiv.org/abs/2509.17566", "authors": ["Ding Shaodong", "Liu Ziyang", "Zhou Yijun", "Liu Tao"], "title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data", "categories": ["cs.CV", "cs.AI"], "comment": "First-place solution of the classification track for MICCAI'2025\n  PDCADxFoundation Challenge", "summary": "The automatic diagnosis of Parkinson's disease is in high clinical demand due\nto its prevalence and the importance of targeted treatment. Current clinical\npractice often relies on diagnostic biomarkers in QSM and NM-MRI images.\nHowever, the lack of large, high-quality datasets makes training diagnostic\nmodels from scratch prone to overfitting. Adapting pre-trained 3D medical\nmodels is also challenging, as the diversity of medical imaging leads to\nmismatches in voxel spacing and modality between pre-training and fine-tuning\ndata. In this paper, we address these challenges by leveraging 2D vision\nfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM and\nQSM images, process each ROI through separate branches to compress the ROI into\na token, and then combine these tokens into a unified patient representation\nfor classification. Within each branch, we use 2D VFMs to encode axial slices\nof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary\nsegmentation head that steers the feature extraction toward specific brain\nnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,\nwhich improves diagnostic performance by pulling together representations of\npatients from the same class while pushing away those from different classes.\nOur approach achieved first place in the MICCAI 2025 PDCADxFoundation\nchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled\nQSM and NM-MRI scans, outperforming the second-place method by 5.5%.These\nresults highlight the potential of 2D VFMs for clinical analysis of 3D MR\nimages."}
{"id": "2509.17588", "pdf": "https://arxiv.org/pdf/2509.17588", "abs": "https://arxiv.org/abs/2509.17588", "authors": ["Jinyeong Kim", "Seil Kang", "Jiwoo Park", "Junhyeok Kim", "Seong Jae Hwang"], "title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) answer visual questions by transferring\ninformation from images to text through a series of attention heads. While this\nimage-to-text information flow is central to visual question answering, its\nunderlying mechanism remains difficult to interpret due to the simultaneous\noperation of numerous attention heads. To address this challenge, we propose\nhead attribution, a technique inspired by component attribution methods, to\nidentify consistent patterns among attention heads that play a key role in\ninformation transfer. Using head attribution, we investigate how LVLMs rely on\nspecific attention heads to identify and answer questions about the main object\nin an image. Our analysis reveals that a distinct subset of attention heads\nfacilitates the image-to-text information flow. Remarkably, we find that the\nselection of these heads is governed by the semantic content of the input image\nrather than its visual appearance. We further examine the flow of information\nat the token level and discover that (1) text information first propagates to\nrole-related tokens and the final token before receiving image information, and\n(2) image information is embedded in both object-related and background tokens.\nOur work provides evidence that image-to-text information flow follows a\nstructured process, and that analysis at the attention-head level offers a\npromising direction toward understanding the mechanisms of LVLMs."}
{"id": "2509.17608", "pdf": "https://arxiv.org/pdf/2509.17608", "abs": "https://arxiv.org/abs/2509.17608", "authors": ["Jungeun Lee", "Kyungah Lee", "Inseok Hwang", "SoHyun Park", "Young-Ho Kim"], "title": "AutiHero: Leveraging Generative AI in Social Narratives to Engage Parents in Story-Driven Behavioral Guidance for Autistic Children", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "22 pages except reference", "summary": "Social narratives are known to help autistic children understand and navigate\nsocial situations through stories. To ensure effectiveness, however, the\nmaterials need to be customized to reflect each child's unique behavioral\ncontext, requiring considerable time and effort for parents to practice at\nhome. We present AutiHero, a generative AI-based social narrative system for\nbehavioral guidance, which supports parents to create personalized stories for\ntheir autistic children and read them together. AutiHero generates text and\nvisual illustrations that reflect their children's interests, target behaviors,\nand everyday contexts. In a two-week deployment study with 16 autistic\nchild-parent dyads, parents created 218 stories and read an average of 4.25\nstories per day, demonstrating a high level of engagement. AutiHero also\nprovided an effective, low-demanding means to guide children's social\nbehaviors, encouraging positive change. We discuss the implications of\ngenerative AI-infused tools to empower parents in guiding their children's\nbehaviors, fostering their social learning."}
{"id": "2509.17621", "pdf": "https://arxiv.org/pdf/2509.17621", "abs": "https://arxiv.org/abs/2509.17621", "authors": ["Khoa Tran", "Hung-Cuong Trinh", "Vy-Rin Nguyen", "T. Nguyen-Thoi", "Vin Nguyen-Thai"], "title": "SeqBattNet: A Discrete-State Physics-Informed Neural Network with Aging Adaptation for Battery Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate battery modeling is essential for reliable state estimation in\nmodern applications, such as predicting the remaining discharge time and\nremaining discharge energy in battery management systems. Existing approaches\nface several limitations: model-based methods require a large number of\nparameters; data-driven methods rely heavily on labeled datasets; and current\nphysics-informed neural networks (PINNs) often lack aging adaptation, or still\ndepend on many parameters, or continuously regenerate states. In this work, we\npropose SeqBattNet, a discrete-state PINN with built-in aging adaptation for\nbattery modeling, to predict terminal voltage during the discharge process.\nSeqBattNet consists of two components: (i) an encoder, implemented as the\nproposed HRM-GRU deep learning module, which generates cycle-specific aging\nadaptation parameters; and (ii) a decoder, based on the equivalent circuit\nmodel (ECM) combined with deep learning, which uses these parameters together\nwith the input current to predict voltage. The model requires only three basic\nbattery parameters and, when trained on data from a single cell, still achieves\nrobust performance. Extensive evaluations across three benchmark datasets (TRI,\nRT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms\nclassical sequence models and PINN baselines, achieving consistently lower RMSE\nwhile maintaining computational efficiency."}
{"id": "2509.17628", "pdf": "https://arxiv.org/pdf/2509.17628", "abs": "https://arxiv.org/abs/2509.17628", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE."}
{"id": "2509.17638", "pdf": "https://arxiv.org/pdf/2509.17638", "abs": "https://arxiv.org/abs/2509.17638", "authors": ["Zilin Gao", "Qilong Wang", "Bingbing Zhang", "Qinghua Hu", "Peihua Li"], "title": "A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 13 figures, 7 tables", "summary": "Thanks to capability to alleviate the cost of large-scale annotation,\nfew-shot action recognition (FSAR) has attracted increased attention of\nresearchers in recent years. Existing FSAR approaches typically neglect the\nrole of individual motion pattern in comparison, and under-explore the feature\nstatistics for video dynamics. Thereby, they struggle to handle the challenging\ntemporal misalignment in video dynamics, particularly by using 2D backbones. To\novercome these limitations, this work proposes an adaptively aligned\nmulti-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the\nlatent video dynamics with a collection of powerful representation candidates\nand adaptively align them in an instance-guided manner. To this end, our\nA$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$\nmodule) for matching, and multi-scale second-order moment (M$^2$ block) for\nstrong representation. Specifically, M$^2$ block develops a collection of\nsemantic second-order descriptors at multiple spatio-temporal scales.\nFurthermore, A$^2$ module aims to adaptively select informative candidate\ndescriptors while considering the individual motion pattern. By such means, our\nA$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem\nby establishing an adaptive alignment protocol for strong representation.\nNotably, our proposed method generalizes well to various few-shot settings and\ndiverse metrics. The experiments are conducted on five widely used FSAR\nbenchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive\nperformance compared to state-of-the-arts, demonstrating its effectiveness and\ngeneralization."}
{"id": "2509.17641", "pdf": "https://arxiv.org/pdf/2509.17641", "abs": "https://arxiv.org/abs/2509.17641", "authors": ["Hyunjong Ok", "Suho Yoo", "Hyeonjun Kim", "Jaeho Lee"], "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "comment": "Preprint", "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io."}
{"id": "2509.17647", "pdf": "https://arxiv.org/pdf/2509.17647", "abs": "https://arxiv.org/abs/2509.17647", "authors": ["Yu Liu", "Baoxiong Jia", "Ruijie Lu", "Chuyue Gan", "Huayu Chen", "Junfeng Ni", "Song-Chun Zhu", "Siyuan Huang"], "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io."}
{"id": "2509.17664", "pdf": "https://arxiv.org/pdf/2509.17664", "abs": "https://arxiv.org/abs/2509.17664", "authors": ["Pingyi Chen", "Yujing Lou", "Shen Cao", "Jinhui Guo", "Lubin Fan", "Yue Wu", "Lin Yang", "Lizhuang Ma", "Jieping Ye"], "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NeurIPS 2025", "summary": "While vision language models (VLMs) excel in 2D semantic visual\nunderstanding, their ability to quantitatively reason about 3D spatial\nrelationships remains under-explored, due to the deficiency of 2D images'\nspatial representation ability. In this paper, we analyze the problem hindering\nVLMs' spatial understanding abilities and propose SD-VLM, a novel framework\nthat significantly enhances fundamental spatial perception abilities of VLMs\nthrough two key contributions: (1) propose Massive Spatial Measuring and\nUnderstanding (MSMU) dataset with precise spatial annotations, and (2)\nintroduce a simple depth positional encoding method strengthening VLMs' spatial\nawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA\npairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented\nsamples. We have trained SD-VLM, a strong generalist VLM which shows superior\nquantitative spatial measuring and understanding capability. SD-VLM not only\nachieves state-of-the-art performance on our proposed MSMU-Bench, but also\nshows spatial generalization abilities on other spatial understanding\nbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments\ndemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and\n25.56% respectively on MSMU-Bench. Code and models are released at\nhttps://github.com/cpystan/SD-VLM."}
{"id": "2509.17665", "pdf": "https://arxiv.org/pdf/2509.17665", "abs": "https://arxiv.org/abs/2509.17665", "authors": ["Katharina Simbeck", "Mariam Mahran"], "title": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "Accepted at AEQUITAS 2025: Workshop on Fairness and Bias in AI |\n  co-located with ECAI, October 26th, 2025, Bologna, Italy. 12 pages, 1 figure", "summary": "Despite growing research on bias in large language models (LLMs), most work\nhas focused on gender and race, with little attention to religious identity.\nThis paper explores how religion is internally represented in LLMs and how it\nintersects with concepts of violence and geography. Using mechanistic\ninterpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we\nanalyze latent feature activations across five models. We measure overlap\nbetween religion- and violence-related prompts and probe semantic patterns in\nactivation contexts. While all five religions show comparable internal\ncohesion, Islam is more frequently linked to features associated with violent\nlanguage. In contrast, geographic associations largely reflect real-world\nreligious demographics, revealing how models embed both factual distributions\nand cultural stereotypes. These findings highlight the value of structural\nanalysis in auditing not just outputs but also internal representations that\nshape model behavior."}
{"id": "2509.17671", "pdf": "https://arxiv.org/pdf/2509.17671", "abs": "https://arxiv.org/abs/2509.17671", "authors": ["Selva Taş", "Mahmut El Huseyni", "Özay Ezerceli", "Reyhan Bayraktar", "Fatma Betül Terzioğlu"], "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages."}
{"id": "2509.17686", "pdf": "https://arxiv.org/pdf/2509.17686", "abs": "https://arxiv.org/abs/2509.17686", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 10 figures, VEHITS conference 2025", "summary": "Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it\nplays a key role in detecting and measuring objects in the vehicle's\nsurroundings. However, a significant challenge in this domain arises from\nmissing information in Depth images, where certain points are not measurable\ndue to gaps or inconsistencies in pixel data. Our research addresses two key\ntasks to overcome this challenge. First, we developed an algorithm using a\nmulti-layered training approach to generate Depth images from a single RGB\nimage. Second, we addressed the issue of missing information in Depth images by\napplying our algorithm to rectify these gaps, resulting in Depth images with\ncomplete and accurate data. We further tested our algorithm on the Cityscapes\ndataset and successfully resolved the missing information in its Depth images,\ndemonstrating the effectiveness of our approach in real-world urban\nenvironments."}
{"id": "2509.17694", "pdf": "https://arxiv.org/pdf/2509.17694", "abs": "https://arxiv.org/abs/2509.17694", "authors": ["Dongxu Lu", "Johan Jeuring", "Albert Gatt"], "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at the 18th International Natural Language\n  Generation Conference (INLG 2025)", "summary": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations."}
{"id": "2509.17695", "pdf": "https://arxiv.org/pdf/2509.17695", "abs": "https://arxiv.org/abs/2509.17695", "authors": ["Leszek Sliwko"], "title": "Cluster Workload Allocation: A Predictive Approach Leveraging Machine Learning Efficiency", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "comment": "This is the accepted version of the paper published in IEEE Access.\n  The final version is available at:\n  https://doi.org/10.1109/ACCESS.2024.3520422", "summary": "This research investigates how Machine Learning (ML) algorithms can assist in\nworkload allocation strategies by detecting tasks with node affinity operators\n(referred to as constraint operators), which constrain their execution to a\nlimited number of nodes. Using real-world Google Cluster Data (GCD) workload\ntraces and the AGOCS framework, the study extracts node attributes and task\nconstraints, then analyses them to identify suitable node-task pairings. It\nfocuses on tasks that can be executed on either a single node or fewer than a\nthousand out of 12.5k nodes in the analysed GCD cluster. Task constraint\noperators are compacted, pre-processed with one-hot encoding, and used as\nfeatures in a training dataset. Various ML classifiers, including Artificial\nNeural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge\nRegression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for\naccuracy and F1-scores. The final ensemble voting classifier model achieved 98%\naccuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable\nnode."}
{"id": "2509.17701", "pdf": "https://arxiv.org/pdf/2509.17701", "abs": "https://arxiv.org/abs/2509.17701", "authors": ["Mariam Mahran", "Katharina Simbeck"], "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at edu4AI'25: 2nd Workshop on Education for Artificial\n  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7\n  pages, 0 figures", "summary": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education."}
{"id": "2509.17747", "pdf": "https://arxiv.org/pdf/2509.17747", "abs": "https://arxiv.org/abs/2509.17747", "authors": ["Sheng Huang", "Jiexuan Yan", "Beiyan Liu", "Bo Liu", "Richang Hong"], "title": "Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by IEEE Transactions on Image Processing", "summary": "Real-world datasets often exhibit class imbalance across multiple categories,\nmanifesting as long-tailed distributions and few-shot scenarios. This is\nespecially challenging in Class-Imbalanced Multi-Label Image Classification\n(CI-MLIC) tasks, where data imbalance and multi-object recognition present\nsignificant obstacles. To address these challenges, we propose a novel method\ntermed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which\nleverages multi-modal knowledge from vision-language pretrained (VLP) models to\nmitigate the class-imbalance problem in multi-label settings. Specifically,\nHP-DVAL employs dual-view alignment learning to transfer the powerful feature\nrepresentation capabilities from VLP models by extracting complementary\nfeatures for accurate image-text alignment. To better adapt VLP models for\nCI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes\nglobal and local prompts to learn task-specific and context-related prior\nknowledge. Additionally, we design a semantic consistency loss during prompt\ntuning to prevent learned prompts from deviating from general knowledge\nembedded in VLP models. The effectiveness of our approach is validated on two\nCI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results\ndemonstrate the superiority of our method over SOTA approaches, achieving mAP\nimprovements of 10.0\\% and 5.2\\% on the long-tailed multi-label image\nclassification task, and 6.8\\% and 2.9\\% on the multi-label few-shot image\nclassification task."}
{"id": "2509.17752", "pdf": "https://arxiv.org/pdf/2509.17752", "abs": "https://arxiv.org/abs/2509.17752", "authors": ["Miao Li", "Phuc Nguyen", "Christopher Tam", "Alexandra Morgan", "Kenneth Ge", "Rahul Bansal", "Linzi Yu", "Rima Arnaout", "Ramy Arnaout"], "title": "GEM-T: Generative Tabular Data via Fitting Moments", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "18 pages, 4 figures", "summary": "Tabular data dominates data science but poses challenges for generative\nmodels, especially when the data is limited or sensitive. We present a novel\napproach to generating synthetic tabular data based on the principle of maximum\nentropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for\ntables.'' GEM-T directly captures nth-order interactions -- pairwise,\nthird-order, etc. -- among columns of training data. In extensive testing,\nGEM-T matches or exceeds deep neural network approaches previously regarded as\nstate-of-the-art in 23 of 34 publicly available datasets representing diverse\nsubject domains (68\\%). Notably, GEM-T involves orders-of-magnitude fewer\ntrainable parameters, demonstrating that much of the information in real-world\ndata resides in low-dimensional, potentially human-interpretable correlations,\nprovided that the input data is appropriately transformed first. Furthermore,\nMaxEnt better handles heterogeneous data types (continuous vs. discrete vs.\ncategorical), lack of local structure, and other features of tabular data.\nGEM-T represents a promising direction for light-weight high-performance\ngenerative models for structured data."}
{"id": "2509.17765", "pdf": "https://arxiv.org/pdf/2509.17765", "abs": "https://arxiv.org/abs/2509.17765", "authors": ["Jin Xu", "Zhifang Guo", "Hangrui Hu", "Yunfei Chu", "Xiong Wang", "Jinzheng He", "Yuxuan Wang", "Xian Shi", "Ting He", "Xinfa Zhu", "Yuanjun Lv", "Yongqi Wang", "Dake Guo", "He Wang", "Linhan Ma", "Pei Zhang", "Xinyu Zhang", "Hongkun Hao", "Zishan Guo", "Baosong Yang", "Bin Zhang", "Ziyang Ma", "Xipin Wei", "Shuai Bai", "Keqin Chen", "Xuejing Liu", "Peng Wang", "Mingkun Yang", "Dayiheng Liu", "Xingzhang Ren", "Bo Zheng", "Rui Men", "Fan Zhou", "Bowen Yu", "Jianxin Yang", "Le Yu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-Omni Technical Report", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.AS"], "comment": "https://github.com/QwenLM/Qwen3-Omni", "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense."}
{"id": "2509.17766", "pdf": "https://arxiv.org/pdf/2509.17766", "abs": "https://arxiv.org/abs/2509.17766", "authors": ["Ziyi Liu"], "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents."}
{"id": "2509.17768", "pdf": "https://arxiv.org/pdf/2509.17768", "abs": "https://arxiv.org/abs/2509.17768", "authors": ["Jessica Ojo", "Zina Kamel", "David Ifeoluwa Adelani"], "title": "DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language Identification (LID) is a core task in multilingual NLP, yet current\nsystems often overfit to clean, monolingual data. This work introduces\nDIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across\ndiverse domains, including speech transcripts, web text, social media texts,\nchildren's stories, and code-switched text. Our findings reveal that while\nmodels achieve high accuracy on curated datasets, performance degrades sharply\non noisy and informal inputs. We also introduce DIVERS-CS, a diverse\ncode-switching benchmark dataset spanning 10 language pairs, and show that\nexisting models struggle to detect multiple languages within the same sentence.\nThese results highlight the need for more robust and inclusive LID systems in\nreal-world settings."}
{"id": "2509.17784", "pdf": "https://arxiv.org/pdf/2509.17784", "abs": "https://arxiv.org/abs/2509.17784", "authors": ["Jin Li", "Shoujin Wang", "Qi Zhang", "Feng Liu", "Tongliang Liu", "Longbing Cao", "Shui Yu", "Fang Chen"], "title": "Revealing Multimodal Causality with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at NeurIPS 2025", "summary": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data."}
{"id": "2509.17786", "pdf": "https://arxiv.org/pdf/2509.17786", "abs": "https://arxiv.org/abs/2509.17786", "authors": ["Aniello Panariello", "Daniel Marczak", "Simone Magistri", "Angelo Porrello", "Bartłomiej Twardowski", "Andrew D. Bagdanov", "Simone Calderara", "Joost van de Weijer"], "title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025), San Diego, USA", "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging."}
{"id": "2509.17788", "pdf": "https://arxiv.org/pdf/2509.17788", "abs": "https://arxiv.org/abs/2509.17788", "authors": ["Xingyu Fan", "Feifei Li", "Wenhui Que", "Hailong Li"], "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "7 pages", "summary": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment."}
{"id": "2509.17802", "pdf": "https://arxiv.org/pdf/2509.17802", "abs": "https://arxiv.org/abs/2509.17802", "authors": ["Qi'ao Xu", "Pengfei Wang", "Bo Zhong", "Tianwen Qian", "Xiaoling Wang", "Ye Wang", "Hong Yu"], "title": "TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 4 figures", "summary": "Medical time series (MedTS) classification is pivotal for intelligent\nhealthcare, yet its efficacy is severely limited by poor cross-subject\ngeneration due to the profound cross-individual heterogeneity. Despite advances\nin architectural innovations and transfer learning techniques, current methods\nremain constrained by modality-specific inductive biases that limit their\nability to learn universally invariant representations. To overcome this, we\npropose TS-P$^2$CL, a novel plug-and-play framework that leverages the\nuniversal pattern recognition capabilities of pre-trained vision models. We\nintroduce a vision-guided paradigm that transforms 1D physiological signals\ninto 2D pseudo-images, establishing a bridge to the visual domain. This\ntransformation enables implicit access to rich semantic priors learned from\nnatural images. Within this unified space, we employ a dual-contrastive\nlearning strategy: intra-modal consistency enforces temporal coherence, while\ncross-modal alignment aligns time-series dynamics with visual semantics,\nthereby mitigating individual-specific biases and learning robust,\ndomain-invariant features. Extensive experiments on six MedTS datasets\ndemonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both\nsubject-dependent and subject-independent settings."}
{"id": "2509.17830", "pdf": "https://arxiv.org/pdf/2509.17830", "abs": "https://arxiv.org/abs/2509.17830", "authors": ["Lekkala Sai Teja", "Annepaka Yadagiri", "and Partha Pakray", "Chukhu Chunka", "Mangadoddi Srikar Vardhan"], "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 14 figures", "summary": "Generation of Artificial Intelligence (AI) texts in important works has\nbecome a common practice that can be used to misuse and abuse AI at various\nlevels. Traditional AI detectors often rely on document-level classification,\nwhich struggles to identify AI content in hybrid or slightly edited texts\ndesigned to avoid detection, leading to concerns about the model's efficiency,\nwhich makes it hard to distinguish between human-written and AI-generated\ntexts. A sentence-level sequence labeling model proposed to detect transitions\nbetween human- and AI-generated text, leveraging nuanced linguistic signals\noverlooked by document-level classifiers. By this method, detecting and\nsegmenting AI and human-written text within a single document at the\ntoken-level granularity is achieved. Our model combines the state-of-the-art\npre-trained Transformer models, incorporating Neural Networks (NN) and\nConditional Random Fields (CRFs). This approach extends the power of\ntransformers to extract semantic and syntactic patterns, and the neural network\ncomponent to capture enhanced sequence-level representations, thereby improving\nthe boundary predictions by the CRF layer, which enhances sequence recognition\nand further identification of the partition between Human- and AI-generated\ntexts. The evaluation is performed on two publicly available benchmark datasets\ncontaining collaborative human and AI-generated texts. Our experimental\ncomparisons are with zero-shot detectors and the existing state-of-the-art\nmodels, along with rigorous ablation studies to justify that this approach, in\nparticular, can accurately detect the spans of AI texts in a completely\ncollaborative text. All our source code and the processed datasets are\navailable in our GitHub repository."}
{"id": "2509.17834", "pdf": "https://arxiv.org/pdf/2509.17834", "abs": "https://arxiv.org/abs/2509.17834", "authors": ["Duygu Kabakci-Zorlu", "Fabio Lorenzi", "John Sheehan", "Karol Lynch", "Bradley Eck"], "title": "From Documents to Database: Failure Modes for Industrial Assets", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition\n  & Management (AI4KAM) Workshop @ IJCAI 2025", "summary": "We propose an interactive system using foundation models and user-provided\ntechnical documents to generate Failure Mode and Effects Analyses (FMEA) for\nindustrial equipment. Our system aggregates unstructured content across\ndocuments to generate an FMEA and stores it in a relational database.\nLeveraging this tool, the time required for creation of this\nknowledge-intensive content is reduced, outperforming traditional manual\napproaches. This demonstration showcases the potential of foundation models to\nfacilitate the creation of specialized structured content for enterprise asset\nmanagement systems."}
{"id": "2509.17866", "pdf": "https://arxiv.org/pdf/2509.17866", "abs": "https://arxiv.org/abs/2509.17866", "authors": ["Xinyu He", "Xianghui Cao"], "title": "Understanding Post-Training Structural Changes in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "38 pages, 26 figures", "summary": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes."}
{"id": "2509.17879", "pdf": "https://arxiv.org/pdf/2509.17879", "abs": "https://arxiv.org/abs/2509.17879", "authors": ["Tu Nguyen", "Kevin Du", "Alexander Miserlis Hoyle", "Ryan Cotterell"], "title": "How Persuasive is Your Context?", "categories": ["cs.CL", "cs.AI"], "comment": "Long paper accepted at EMNLP 2025", "summary": "Two central capabilities of language models (LMs) are: (i) drawing on prior\nknowledge about entities, which allows them to answer queries such as \"What's\nthe official language of Austria?\", and (ii) adapting to new information\nprovided in context, e.g., \"Pretend the official language of Austria is\nTagalog.\", that is pre-pended to the question. In this article, we introduce\ntargeted persuasion score (TPS), designed to quantify how persuasive a given\ncontext is to an LM where persuasion is operationalized as the ability of the\ncontext to alter the LM's answer to the question. In contrast to evaluating\npersuasiveness only by inspecting the greedily decoded answer under the model,\nTPS provides a more fine-grained view of model behavior. Based on the\nWasserstein distance, TPS measures how much a context shifts a model's original\nanswer distribution toward a target distribution. Empirically, through a series\nof experiments, we show that TPS captures a more nuanced notion of\npersuasiveness than previously proposed metrics."}
{"id": "2509.17885", "pdf": "https://arxiv.org/pdf/2509.17885", "abs": "https://arxiv.org/abs/2509.17885", "authors": ["Saad Mokssit", "Ouassim Karrakchou", "Alejandro Mousist", "Mounir Ghogho"], "title": "Confidence-gated training for efficient early-exit neural networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments."}
{"id": "2509.17888", "pdf": "https://arxiv.org/pdf/2509.17888", "abs": "https://arxiv.org/abs/2509.17888", "authors": ["Divya Mereddy", "Marcos Quinones-Grueiro", "Ashwin T S", "Eduardo Davalos", "Gautam Biswas", "Kent Etherton", "Tyler Davis", "Katelyn Kay", "Jill Lear", "Benjamin Goldberg"], "title": "Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study examines how Critical Care Air Transport Team (CCATT) members are\ntrained using mixed-reality simulations that replicate the high-pressure\nconditions of aeromedical evacuation. Each team - a physician, nurse, and\nrespiratory therapist - must stabilize severely injured soldiers by managing\nventilators, IV pumps, and suction devices during flight. Proficient\nperformance requires clinical expertise and cognitive skills, such as\nsituational awareness, rapid decision-making, effective communication, and\ncoordinated task management, all of which must be maintained under stress.\nRecent advances in simulation and multimodal data analytics enable more\nobjective and comprehensive performance evaluation. In contrast, traditional\ninstructor-led assessments are subjective and may overlook critical events,\nthereby limiting generalizability and consistency. However, AI-based automated\nand more objective evaluation metrics still demand human input to train the AI\nalgorithms to assess complex team dynamics in the presence of environmental\nnoise and the need for accurate re-identification in multi-person tracking. To\naddress these challenges, we introduce a systematic, data-driven assessment\nframework that combines Cognitive Task Analysis (CTA) with Multimodal Learning\nAnalytics (MMLA). We have developed a domain-specific CTA model for CCATT\ntraining and a vision-based action recognition pipeline using a fine-tuned\nHuman-Object Interaction model, the Cascade Disentangling Network (CDN), to\ndetect and track trainee-equipment interactions over time. These interactions\nautomatically yield performance indicators (e.g., reaction time, task\nduration), which are mapped onto a hierarchical CTA model tailored to CCATT\noperations, enabling interpretable, domain-relevant performance evaluations."}
{"id": "2509.17930", "pdf": "https://arxiv.org/pdf/2509.17930", "abs": "https://arxiv.org/abs/2509.17930", "authors": ["Yiwen Guan", "Jacob Whitehill"], "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation faces challenges of computational redundancy and\nlimited accuracy for low-resource languages, especially in speech translation.\nTo address this, we propose a novel hierarchical Transformer Encoder Tree (TET)\ncombined with non-autoregressive encoder-only models trained with Connectionist\nTemporal Classification for multilingual translation. By sharing intermediate\nrepresentations among linguistically similar target languages, TET can improve\naccuracy on low-resource languages, reduce computational redundancy, and allow\ngenerating all target languages in a single forward pass, thus eliminating\nsequential bottlenecks and improving parallelism. For speech translation,\ncombining TET with a non-autoregressive speech recognition backbone (wav2vec2)\nshows promising results in terms of translation quality compared to\nautoregressive systems while being 7-14 times faster."}
{"id": "2509.17941", "pdf": "https://arxiv.org/pdf/2509.17941", "abs": "https://arxiv.org/abs/2509.17941", "authors": ["Zichao Hu", "Chen Tang", "Michael J. Munje", "Yifeng Zhu", "Alex Liu", "Shuijing Liu", "Garrett Warnell", "Peter Stone", "Joydeep Biswas"], "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://amrl.cs.utexas.edu/ComposableNav/", "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/"}
{"id": "2509.17942", "pdf": "https://arxiv.org/pdf/2509.17942", "abs": "https://arxiv.org/abs/2509.17942", "authors": ["Nicholas Kraabel", "Jiangtao Liu", "Yuchen Bian", "Daniel Kifer", "Chaopeng Shen"], "title": "StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stewarding natural resources, mitigating floods, droughts, wildfires, and\nlandslides, and meeting growing demands require models that can predict\nclimate-driven land-surface responses and human feedback with high accuracy.\nTraditional impact models, whether process-based, statistical, or machine\nlearning, struggle with spatial generalization due to limited observations and\nconcept drift. Recently proposed vision foundation models trained on satellite\nimagery demand massive compute and are ill-suited for dynamic land-surface\nprediction. We introduce StefaLand, a generative spatiotemporal earth\nfoundation model centered on landscape interactions. StefaLand improves\npredictions on three tasks and four datasets: streamflow, soil moisture, and\nsoil composition, compared to prior state-of-the-art. Results highlight its\nability to generalize across diverse, data-scarce regions and support broad\nland-surface applications. The model builds on a masked autoencoder backbone\nthat learns deep joint representations of landscape attributes, with a\nlocation-aware architecture fusing static and time-series inputs,\nattribute-based representations that drastically reduce compute, and residual\nfine-tuning adapters that enhance transfer. While inspired by prior methods,\ntheir alignment with geoscience and integration in one model enables robust\nperformance on dynamic land-surface tasks. StefaLand can be pretrained and\nfinetuned on academic compute yet outperforms state-of-the-art baselines and\neven fine-tuned vision foundation models. To our knowledge, this is the first\ngeoscience land-surface foundation model that demonstrably improves dynamic\nland-surface interaction predictions and supports diverse downstream\napplications."}
{"id": "2509.17946", "pdf": "https://arxiv.org/pdf/2509.17946", "abs": "https://arxiv.org/abs/2509.17946", "authors": ["Mian Zhong", "Pristina Wang", "Anjalie Field"], "title": "HICode: Hierarchical Inductive Coding with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures", "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data."}
{"id": "2509.17970", "pdf": "https://arxiv.org/pdf/2509.17970", "abs": "https://arxiv.org/abs/2509.17970", "authors": ["Yunchu Han", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices."}
{"id": "2509.17971", "pdf": "https://arxiv.org/pdf/2509.17971", "abs": "https://arxiv.org/abs/2509.17971", "authors": ["Tan-Ha Mai", "Hsuan-Tien Lin"], "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "22 pages, 10 figures", "summary": "In this paper, we investigate the challenges of complementary-label learning\n(CLL), a specialized form of weakly-supervised learning (WSL) where models are\ntrained with labels indicating classes to which instances do not belong, rather\nthan standard ordinary labels. This alternative supervision is appealing\nbecause collecting complementary labels is generally cheaper and less\nlabor-intensive. Although most existing research in CLL emphasizes the\ndevelopment of novel loss functions, the potential of data augmentation in this\ndomain remains largely underexplored. In this work, we uncover that the\nwidely-used Mixup data augmentation technique is ineffective when directly\napplied to CLL. Through in-depth analysis, we identify that the\ncomplementary-label noise generated by Mixup negatively impacts the performance\nof CLL models. We then propose an improved technique called Intra-Cluster Mixup\n(ICM), which only synthesizes augmented data from nearby examples, to mitigate\nthe noise effect. ICM carries the benefits of encouraging complementary label\nsharing of nearby examples, and leads to substantial performance improvements\nacross synthetic and real-world labeled datasets. In particular, our wide\nspectrum of experimental results on both balanced and imbalanced CLL settings\njustifies the potential of ICM in allying with state-of-the-art CLL algorithms,\nachieving significant accuracy increases of 30% and 10% on MNIST and CIFAR\ndatasets, respectively."}
{"id": "2509.17991", "pdf": "https://arxiv.org/pdf/2509.17991", "abs": "https://arxiv.org/abs/2509.17991", "authors": ["Aakash Kumar Agarwal", "Saprativa Bhattacharjee", "Mauli Rastogi", "Jemima S. Jacob", "Biplab Banerjee", "Rashmi Gupta", "Pushpak Bhattacharyya"], "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Almost 50% depression patients face the risk of going into relapse. The risk\nincreases to 80% after the second episode of depression. Although, depression\ndetection from social media has attained considerable attention, depression\nrelapse detection has remained largely unexplored due to the lack of curated\ndatasets and the difficulty of distinguishing relapse and non-relapse users. In\nthis work, we present ReDepress, the first clinically validated social media\ndataset focused on relapse, comprising 204 Reddit users annotated by mental\nhealth professionals. Unlike prior approaches, our framework draws on cognitive\ntheories of depression, incorporating constructs such as attention bias,\ninterpretation bias, memory bias and rumination into both annotation and\nmodeling. Through statistical analyses and machine learning experiments, we\ndemonstrate that cognitive markers significantly differentiate relapse and\nnon-relapse groups, and that models enriched with these features achieve\ncompetitive performance, with transformer-based temporal models attaining an F1\nof 0.86. Our findings validate psychological theories in real-world textual\ndata and underscore the potential of cognitive-informed computational methods\nfor early relapse detection, paving the way for scalable, low-cost\ninterventions in mental healthcare."}
{"id": "2509.17995", "pdf": "https://arxiv.org/pdf/2509.17995", "abs": "https://arxiv.org/abs/2509.17995", "authors": ["Yefan Zhou", "Austin Xu", "Yilun Zhou", "Janvijay Singh", "Jiang Gui", "Shafiq Joty"], "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges."}
{"id": "2509.17998", "pdf": "https://arxiv.org/pdf/2509.17998", "abs": "https://arxiv.org/abs/2509.17998", "authors": ["Richard Cornelius Suwandi", "Feng Yin", "Juntao Wang", "Renjie Li", "Tsung-Hui Chang", "Sergios Theodoridis"], "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as Poster at NeurIPS 2025", "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake."}
{"id": "2509.17999", "pdf": "https://arxiv.org/pdf/2509.17999", "abs": "https://arxiv.org/abs/2509.17999", "authors": ["Riccardo Cadei", "Christian Internò"], "title": "The Narcissus Hypothesis:Descending to the Rung of Illusion", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion."}
{"id": "2509.18001", "pdf": "https://arxiv.org/pdf/2509.18001", "abs": "https://arxiv.org/abs/2509.18001", "authors": ["Haocheng Luo", "Mehrtash Harandi", "Dinh Phung", "Trung Le"], "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sharpness-aware minimization (SAM) has emerged as a highly effective\ntechnique for improving model generalization, but its underlying principles are\nnot fully understood. We investigated the phenomenon known as m-sharpness,\nwhere the performance of SAM improves monotonically as the micro-batch size for\ncomputing perturbations decreases. Leveraging an extended Stochastic\nDifferential Equation (SDE) framework, combined with an analysis of the\nstructure of stochastic gradient noise (SGN), we precisely characterize the\ndynamics of various SAM variants. Our findings reveal that the stochastic noise\nintroduced during SAM perturbations inherently induces a variance-based\nsharpness regularization effect. Motivated by our theoretical insights, we\nintroduce Reweighted SAM, which employs sharpness-weighted sampling to mimic\nthe generalization benefits of m-SAM while remaining parallelizable.\nComprehensive experiments validate the effectiveness of our theoretical\nanalysis and proposed method."}
{"id": "2509.18008", "pdf": "https://arxiv.org/pdf/2509.18008", "abs": "https://arxiv.org/abs/2509.18008", "authors": ["Bingsheng Yao", "Jiaju Chen", "Chaoran Chen", "April Wang", "Toby Jia-jun Li", "Dakuo Wang"], "title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis."}
{"id": "2509.18010", "pdf": "https://arxiv.org/pdf/2509.18010", "abs": "https://arxiv.org/abs/2509.18010", "authors": ["Sara Papi", "Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli"], "title": "Cross-Attention is Half Explanation in Speech-to-Text Models", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels."}
{"id": "2509.18015", "pdf": "https://arxiv.org/pdf/2509.18015", "abs": "https://arxiv.org/abs/2509.18015", "authors": ["Advait Gosai", "Arun Kavishwar", "Stephanie L. McNamara", "Soujanya Samineni", "Renato Umeton", "Alexander Chowdhury", "William Lotter"], "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use."}
{"id": "2509.18025", "pdf": "https://arxiv.org/pdf/2509.18025", "abs": "https://arxiv.org/abs/2509.18025", "authors": ["Gilles Bareilles", "Allen Gehret", "Johannes Aspman", "Jana Lepšová", "Jakub Mareček"], "title": "Deep Learning as the Disciplined Construction of Tame Objects", "categories": ["math.OC", "cs.AI", "cs.LG", "math.LO", "stat.ML"], "comment": "35 pages, 8 figures", "summary": "One can see deep-learning models as compositions of functions within the\nso-called tame geometry. In this expository note, we give an overview of some\ntopics at the interface of tame geometry (also known as o-minimality),\noptimization theory, and deep learning theory and practice. To do so, we\ngradually introduce the concepts and tools used to build convergence guarantees\nfor stochastic gradient descent in a general nonsmooth nonconvex, but tame,\nsetting. This illustrates some ways in which tame geometry is a natural\nmathematical framework for the study of AI systems, especially within Deep\nLearning."}
{"id": "2509.18044", "pdf": "https://arxiv.org/pdf/2509.18044", "abs": "https://arxiv.org/abs/2509.18044", "authors": ["Saeid Sheikhi", "Panos Kostakos", "Lauri Loven"], "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions."}
{"id": "2509.18046", "pdf": "https://arxiv.org/pdf/2509.18046", "abs": "https://arxiv.org/abs/2509.18046", "authors": ["Yinuo Wang", "Yuanyang Qi", "Jinzhao Zhou", "Gavin Tao"], "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.SY", "eess.SP", "eess.SY"], "comment": "10 pages", "summary": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing\nfor its compact perception-action mapping, yet practical policies often suffer\nfrom training instability, inefficient feature fusion, and high actuation cost.\nWe present HuMam, a state-centric end-to-end RL framework that employs a\nsingle-layer Mamba encoder to fuse robot-centric states with oriented footstep\ntargets and a continuous phase clock. The policy outputs joint position targets\ntracked by a low-level PD loop and is optimized with PPO. A concise six-term\nreward balances contact quality, swing smoothness, foot placement, posture, and\nbody stability while implicitly promoting energy saving. On the JVRC-1 humanoid\nin mc-mujoco, HuMam consistently improves learning efficiency, training\nstability, and overall task performance over a strong feedforward baseline,\nwhile reducing power consumption and torque peaks. To our knowledge, this is\nthe first end-to-end humanoid RL controller that adopts Mamba as the fusion\nbackbone, demonstrating tangible gains in efficiency, stability, and control\neconomy."}
{"id": "2509.18054", "pdf": "https://arxiv.org/pdf/2509.18054", "abs": "https://arxiv.org/abs/2509.18054", "authors": ["Nikhil N S", "Amol Dilip Joshi", "Bilal Muhammed", "Soban Babu"], "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "10 pages, 5 figures", "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot."}
{"id": "2509.18057", "pdf": "https://arxiv.org/pdf/2509.18057", "abs": "https://arxiv.org/abs/2509.18057", "authors": ["Ansh Nagda", "Prabhakar Raghavan", "Abhradeep Thakurta"], "title": "Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory", "categories": ["cs.LG", "cs.AI", "cs.CC", "math.CO"], "comment": null, "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."}
{"id": "2509.18058", "pdf": "https://arxiv.org/pdf/2509.18058", "abs": "https://arxiv.org/abs/2509.18058", "authors": ["Alexander Panfilov", "Evgenii Kortukov", "Kristina Nikolić", "Matthias Bethge", "Sebastian Lapuschkin", "Wojciech Samek", "Ameya Prabhu", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict."}
{"id": "2509.18060", "pdf": "https://arxiv.org/pdf/2509.18060", "abs": "https://arxiv.org/abs/2509.18060", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Renzeng Duojie", "Yuqing Cai", "Yongbin Yu", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task."}
{"id": "2509.18085", "pdf": "https://arxiv.org/pdf/2509.18085", "abs": "https://arxiv.org/abs/2509.18085", "authors": ["Sudhanshu Agrawal", "Risheek Garrepalli", "Raghavv Goel", "Mingu Lee", "Christopher Lott", "Fatih Porikli"], "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."}
{"id": "2509.18091", "pdf": "https://arxiv.org/pdf/2509.18091", "abs": "https://arxiv.org/abs/2509.18091", "authors": ["Sunhao Dai", "Jiakai Tang", "Jiahua Wu", "Kun Wang", "Yuxuan Zhu", "Bingjun Chen", "Bangyang Hong", "Yu Zhao", "Cong Fu", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Wenjie Wang", "Xu Chen", "Jun Xu", "See-Kiong Ng"], "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "OnePiece Technical Report; Applied in Shopee", "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue."}
{"id": "2509.18093", "pdf": "https://arxiv.org/pdf/2509.18093", "abs": "https://arxiv.org/abs/2509.18093", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "SEQR: Secure and Efficient QR-based LoRA Routing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency."}
{"id": "2509.18094", "pdf": "https://arxiv.org/pdf/2509.18094", "abs": "https://arxiv.org/abs/2509.18094", "authors": ["Ye Liu", "Zongyang Ma", "Junfu Pu", "Zhongang Qi", "Yang Wu", "Ying Shan", "Chang Wen Chen"], "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/", "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method."}
