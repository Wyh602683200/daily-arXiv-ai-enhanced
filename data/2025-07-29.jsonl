{"id": "2507.19489", "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital."}
{"id": "2507.19543", "pdf": "https://arxiv.org/pdf/2507.19543", "abs": "https://arxiv.org/abs/2507.19543", "authors": ["Maria Emilia Mazzolenis", "Ruirui Zhang"], "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.7"], "comment": "Accepted at the ICML 2025 Workshop on Multi-Agent Systems in the Era\n  of Foundation Models: Opportunities, Challenges, and Futures. Code repo:\n  https://github.com/emiliamazzo/WARPP/", "summary": "Large language models (LLMs) are increasingly applied in task-oriented\ndialogue (TOD) systems but often struggle with long, conditional workflows that\ninvolve external tool calls and depend on user-specific information. We present\nWorkflow Adherence via Runtime Parallel Personalization, or WARPP, a\ntraining-free, modular framework that combines multi-agent orchestration with\nruntime personalization to improve workflow adherence in LLM-based systems. By\ndynamically pruning conditional branches based on user attributes, the\nframework reduces reasoning overhead and narrows tool selection at runtime.\nWARPP deploys a parallelized architecture where a dedicated Personalizer agent\noperates alongside modular, domain-specific agents to dynamically tailor\nexecution paths in real time. The framework is evaluated across five\nrepresentative user intents of varying complexity within three domains:\nbanking, flights, and healthcare. Our evaluation leverages synthetic datasets\nand LLM-powered simulated users to test scenarios with conditional\ndependencies. Our results demonstrate that WARPP outperforms both the\nnon-personalized method and the ReAct baseline, achieving increasingly larger\ngains in parameter fidelity and tool accuracy as intent complexity grows, while\nalso reducing average token usage, without any additional training."}
{"id": "2507.19593", "pdf": "https://arxiv.org/pdf/2507.19593", "abs": "https://arxiv.org/abs/2507.19593", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Classical game-theoretic models typically assume rational agents, complete\ninformation, and common knowledge of payoffs - assumptions that are often\nviolated in real-world MAS characterized by uncertainty, misaligned\nperceptions, and nested beliefs. To overcome these limitations, researchers\nhave proposed extensions that incorporate models of cognitive constraints,\nsubjective beliefs, and heterogeneous reasoning. Among these, hypergame theory\nextends the classical paradigm by explicitly modeling agents' subjective\nperceptions of the strategic scenario, known as perceptual games, in which\nagents may hold divergent beliefs about the structure, payoffs, or available\nactions. We present a systematic review of agent-compatible applications of\nhypergame theory, examining how its descriptive capabilities have been adapted\nto dynamic and interactive MAS contexts. We analyze 44 selected studies from\ncybersecurity, robotics, social simulation, communications, and general\ngame-theoretic modeling. Building on a formal introduction to hypergame theory\nand its two major extensions - hierarchical hypergames and HNF - we develop\nagent-compatibility criteria and an agent-based classification framework to\nassess integration patterns and practical applicability. Our analysis reveals\nprevailing tendencies, including the prevalence of hierarchical and graph-based\nmodels in deceptive reasoning and the simplification of extensive theoretical\nframeworks in practical applications. We identify structural gaps, including\nthe limited adoption of HNF-based models, the lack of formal hypergame\nlanguages, and unexplored opportunities for modeling human-agent and\nagent-agent misalignment. By synthesizing trends, challenges, and open research\ndirections, this review provides a new roadmap for applying hypergame theory to\nenhance the realism and effectiveness of strategic modeling in dynamic\nmulti-agent environments."}
{"id": "2507.19608", "pdf": "https://arxiv.org/pdf/2507.19608", "abs": "https://arxiv.org/abs/2507.19608", "authors": ["Jiawen Qi", "Chang Gao", "Zhaochun Ren", "Qinyu Chen"], "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference", "categories": ["cs.AI", "eess.SP"], "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge devices remains challenging\ndue to their quadratically increasing computations with the sequence length.\nExisting studies for dynamic attention pruning are designed for hardware with\nmassively parallel computation capabilities, such as GPUs or TPUs, and aim at\nlong context lengths (e.g., 64K), making them unsuitable for edge scenarios. We\npresent DeltaLLM, a training-free framework that exploits temporal sparsity in\nattention patterns to enable efficient LLM inference across both the prefilling\nand decoding stages, on resource-constrained edge devices. DeltaLLM introduces\nan accuracy- and memory-aware delta matrix construction strategy that\nintroduces temporal sparsity, and a context-aware hybrid attention mechanism\nthat combines full attention in a local context window with delta approximation\noutside it to increase accuracy. We evaluate our framework on the\nedge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model\nacross diverse language tasks. The results show that on BitNet, our framework\nincreases the attention sparsity from 0% to 60% during the prefilling stage\nwith slight accuracy improvement on the WG task, and 0% to 57% across both the\nprefilling and decoding stages, with even higher F1 score from 29.63 to 30.97\non SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity\nduring the prefilling stage and around 57% across both stages with negligible\naccuracy drop. These results demonstrate that DeltaLLM offers a promising\nsolution for efficient edge deployment, requiring no fine-tuning and seamlessly\nintegrating with existing inference pipelines."}
{"id": "2507.19672", "pdf": "https://arxiv.org/pdf/2507.19672", "abs": "https://arxiv.org/abs/2507.19672", "authors": ["Haoran Lu", "Luyang Fang", "Ruidong Zhang", "Xinliang Li", "Jiazhang Cai", "Huimin Cheng", "Lin Tang", "Ziyu Liu", "Zeliang Sun", "Tao Wang", "Yingchuan Zhang", "Arif Hassan Zidan", "Jinwen Xu", "Jincheng Yu", "Meizhi Yu", "Hanqi Jiang", "Xilin Gong", "Weidi Luo", "Bolun Sun", "Yongkai Chen", "Terry Ma", "Shushan Wu", "Yifan Zhou", "Junhao Chen", "Haotian Xiang", "Jing Zhang", "Afrar Jahin", "Wei Ruan", "Ke Deng", "Yi Pan", "Peilong Wang", "Jiahui Li", "Zhengliang Liu", "Lu Zhang", "Lin Zhao", "Wei Liu", "Dajiang Zhu", "Xin Xing", "Fei Dou", "Wei Zhang", "Chao Huang", "Rongjie Liu", "Mengrui Zhang", "Yiwen Liu", "Xiaoxiao Sun", "Qin Lu", "Zhen Xiang", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "119 pages, 10 figures, 7 tables", "summary": "Due to the remarkable capabilities and growing impact of large language\nmodels (LLMs), they have been deeply integrated into many aspects of society.\nThus, ensuring their alignment with human values and intentions has emerged as\na critical challenge. This survey provides a comprehensive overview of\npractical alignment techniques, training protocols, and empirical findings in\nLLM alignment. We analyze the development of alignment methods across diverse\nparadigms, characterizing the fundamental trade-offs between core alignment\nobjectives. Our analysis shows that while supervised fine-tuning enables basic\ninstruction-following, preference-based methods offer more flexibility for\naligning with nuanced human intent. We discuss state-of-the-art techniques,\nincluding Direct Preference Optimization (DPO), Constitutional AI,\nbrain-inspired methods, and alignment uncertainty quantification (AUQ),\nhighlighting their approaches to balancing quality and efficiency. We review\nexisting evaluation frameworks and benchmarking datasets, emphasizing\nlimitations such as reward misspecification, distributional robustness, and\nscalable oversight. We summarize strategies adopted by leading AI labs to\nillustrate the current state of practice. We conclude by outlining open\nproblems in oversight, value pluralism, robustness, and continuous alignment.\nThis survey aims to inform both researchers and practitioners navigating the\nevolving landscape of LLM alignment."}
{"id": "2507.19703", "pdf": "https://arxiv.org/pdf/2507.19703", "abs": "https://arxiv.org/abs/2507.19703", "authors": ["Peter V. Coveney", "Sauro Succi"], "title": "The wall confronting large language models", "categories": ["cs.AI"], "comment": null, "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated."}
{"id": "2507.19725", "pdf": "https://arxiv.org/pdf/2507.19725", "abs": "https://arxiv.org/abs/2507.19725", "authors": ["Leonardo Villalobos-Arias", "Grant Forbes", "Jianxun Wang", "David L Roberts", "Arnav Jhala"], "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors", "categories": ["cs.AI"], "comment": "11 pages, 7 figures, 3 tables", "summary": "Games are challenging for Reinforcement Learning~(RL) agents due to their\nreward-sparsity, as rewards are only obtainable after long sequences of\ndeliberate actions. Intrinsic Motivation~(IM) methods -- which introduce\nexploration rewards -- are an effective solution to reward-sparsity. However,\nIM also causes an issue known as `reward hacking' where the agent optimizes for\nthe new reward at the expense of properly playing the game. The larger problem\nis that reward hacking itself is largely unknown; there is no answer to\nwhether, and to what extent, IM rewards change the behavior of RL agents. This\nstudy takes a first step by empirically evaluating the impact on behavior of\nthree IM techniques on the MiniGrid game-like environment. We compare these IM\nmodels with Generalized Reward Matching~(GRM), a method that can be used with\nany intrinsic reward function to guarantee optimality. Our results suggest that\nIM causes noticeable change by increasing the initial rewards, but also\naltering the way the agent plays; and that GRM mitigated reward hacking in some\nscenarios."}
{"id": "2507.19726", "pdf": "https://arxiv.org/pdf/2507.19726", "abs": "https://arxiv.org/abs/2507.19726", "authors": ["Yuzhang Xie", "Xu Han", "Ran Xu", "Xiao Hu", "Jiaying Lu", "Carl Yang"], "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare", "categories": ["cs.AI", "cs.LG"], "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025), Main Tracks, Research Track, Oral", "summary": "Knowledge graphs (KGs) are important products of the semantic web, which are\nwidely used in various application domains. Healthcare is one of such domains\nwhere KGs are intensively used, due to the high requirement for knowledge\naccuracy and interconnected nature of healthcare data. However, KGs storing\ngeneral factual information often lack the ability to account for important\ncontexts of the knowledge such as the status of specific patients, which are\ncrucial in precision healthcare. Meanwhile, electronic health records (EHRs)\nprovide rich personal data, including various diagnoses and medications, which\nprovide natural contexts for general KGs. In this paper, we propose HypKG, a\nframework that integrates patient information from EHRs into KGs to generate\ncontextualized knowledge representations for accurate healthcare predictions.\nUsing advanced entity-linking techniques, we connect relevant knowledge from\ngeneral KGs with patient information from EHRs, and then utilize a hypergraph\nmodel to \"contextualize\" the knowledge with the patient information. Finally,\nwe employ hypergraph transformers guided by downstream prediction tasks to\njointly learn proper contextualized representations for both KGs and patients,\nfully leveraging existing knowledge in KGs and patient contexts in EHRs. In\nexperiments using a large biomedical KG and two real-world EHR datasets, HypKG\ndemonstrates significant improvements in healthcare prediction tasks across\nmultiple evaluation metrics. Additionally, by integrating external contexts,\nHypKG can learn to adjust the representations of entities and relations in KG,\npotentially improving the quality and real-world utility of knowledge."}
{"id": "2507.19733", "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "categories": ["cs.AI", "cs.DB"], "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL."}
{"id": "2507.19749", "pdf": "https://arxiv.org/pdf/2507.19749", "abs": "https://arxiv.org/abs/2507.19749", "authors": ["Lin Ren", "Guohui Xiao", "Guilin Qi", "Yishuai Geng", "Haohan Xue"], "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)", "categories": ["cs.AI"], "comment": "Accepted for publication at the 22nd International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2025). The code is\n  available at https://github.com/HomuraT/ASPBench", "summary": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonic\nreasoning. Recently, large language models (LLMs) have demonstrated promising\ncapabilities in logical reasoning. Despite this potential, current evaluations\nof LLM capabilities in ASP are often limited. Existing works normally employ\noverly simplified ASP programs, do not support negation, disjunction, or\nmultiple answer sets. Furthermore, there is a lack of benchmarks that introduce\ntasks specifically designed for ASP solving. To bridge this gap, we introduce\nASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:\nASP entailment, answer set verification, and answer set computation. Our\nextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,\nincluding \\emph{deepseek-r1}, \\emph{o4-mini}, and\n\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two\nsimpler tasks, they struggle with answer set computation, which is the core of\nASP solving. These findings offer insights into the current limitations of LLMs\nin ASP solving. This highlights the need for new approaches that integrate\nsymbolic reasoning capabilities more effectively. The code and dataset are\navailable at https://github.com/HomuraT/ASPBench."}
{"id": "2507.19788", "pdf": "https://arxiv.org/pdf/2507.19788", "abs": "https://arxiv.org/abs/2507.19788", "authors": ["Rifny Rachman", "Josh Tingey", "Richard Allmendinger", "Pradyumn Shukla", "Wei Pan"], "title": "Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation", "categories": ["cs.AI"], "comment": null, "summary": "This study develops a generalised multi-objective, multi-echelon supply chain\noptimisation model with non-stationary markets based on a Markov decision\nprocess, incorporating economic, environmental, and social considerations. The\nmodel is evaluated using a multi-objective reinforcement learning (RL) method,\nbenchmarked against an originally single-objective RL algorithm modified with\nweighted sum using predefined weights, and a multi-objective evolutionary\nalgorithm (MOEA)-based approach. We conduct experiments on varying network\ncomplexities, mimicking typical real-world challenges using a customisable\nsimulator. The model determines production and delivery quantities across\nsupply chain routes to achieve near-optimal trade-offs between competing\nobjectives, approximating Pareto front sets. The results demonstrate that the\nprimary approach provides the most balanced trade-off between optimality,\ndiversity, and density, further enhanced with a shared experience buffer that\nallows knowledge transfer among policies. In complex settings, it achieves up\nto 75\\% higher hypervolume than the MOEA-based method and generates solutions\nthat are approximately eleven times denser, signifying better robustness, than\nthose produced by the modified single-objective RL method. Moreover, it ensures\nstable production and inventory levels while minimising demand loss."}
{"id": "2507.19882", "pdf": "https://arxiv.org/pdf/2507.19882", "abs": "https://arxiv.org/abs/2507.19882", "authors": ["Xinshu Li", "Ruoyu Wang", "Erdun Gao", "Mingming Gong", "Lina Yao"], "title": "Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation", "categories": ["cs.AI"], "comment": null, "summary": "Prompt learning has garnered attention for its efficiency over traditional\nmodel training and fine-tuning. However, existing methods, constrained by\ninadequate theoretical foundations, encounter difficulties in achieving\ncausally invariant prompts, ultimately falling short of capturing robust\nfeatures that generalize effectively across categories. To address these\nchallenges, we introduce the $\\textit{\\textbf{DiCap}}$ model, a theoretically\ngrounded $\\textbf{Di}$ffusion-based $\\textbf{C}$ounterf$\\textbf{a}$ctual\n$\\textbf{p}$rompt learning framework, which leverages a diffusion process to\niteratively sample gradients from the marginal and conditional distributions of\nthe causal model, guiding the generation of counterfactuals that satisfy the\nminimal sufficiency criterion. Grounded in rigorous theoretical derivations,\nthis approach guarantees the identifiability of counterfactual outcomes while\nimposing strict bounds on estimation errors. We further employ a contrastive\nlearning framework that leverages the generated counterfactuals, thereby\nenabling the refined extraction of prompts that are precisely aligned with the\ncausal features of the data. Extensive experimental results demonstrate that\nour method performs excellently across tasks such as image classification,\nimage-text retrieval, and visual question answering, with particularly strong\nadvantages in unseen categories."}
{"id": "2507.19960", "pdf": "https://arxiv.org/pdf/2507.19960", "abs": "https://arxiv.org/abs/2507.19960", "authors": ["Olivia Guest"], "title": "What Does 'Human-Centred AI' Mean?", "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "comment": null, "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes."}
{"id": "2507.19973", "pdf": "https://arxiv.org/pdf/2507.19973", "abs": "https://arxiv.org/abs/2507.19973", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o."}
{"id": "2507.19974", "pdf": "https://arxiv.org/pdf/2507.19974", "abs": "https://arxiv.org/abs/2507.19974", "authors": ["Tongjie Li", "Jianhua Zhang", "Li Yu", "Yuxiang Zhang", "Yunlong Cai", "Fan Xu", "Guangyi Liu"], "title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks."}
{"id": "2507.20000", "pdf": "https://arxiv.org/pdf/2507.20000", "abs": "https://arxiv.org/abs/2507.20000", "authors": ["Renaud Fabre", "Daniel Egret", "Patrice Bellot"], "title": "Matching Game Preferences Through Dialogical Large Language Models: A Perspective", "categories": ["cs.AI", "cs.DL"], "comment": "28 pages, 1 figure. Published in Applied Sciences", "summary": "This perspective paper explores the future potential of \"conversational\nintelligence\" by examining how Large Language Models (LLMs) could be combined\nwith GRAPHYP's network system to better understand human conversations and\npreferences. Using recent research and case studies, we propose a conceptual\nframework that could make AI rea-soning transparent and traceable, allowing\nhumans to see and understand how AI reaches its conclusions. We present the\nconceptual perspective of \"Matching Game Preferences through Dialogical Large\nLanguage Models (D-LLMs),\" a proposed system that would allow multiple users to\nshare their different preferences through structured conversations. This\napproach envisions personalizing LLMs by embedding individual user preferences\ndirectly into how the model makes decisions. The proposed D-LLM framework would\nrequire three main components: (1) reasoning processes that could analyze\ndifferent search experiences and guide performance, (2) classification systems\nthat would identify user preference patterns, and (3) dialogue approaches that\ncould help humans resolve conflicting information. This perspective framework\naims to create an interpretable AI system where users could examine,\nunderstand, and combine the different human preferences that influence AI\nresponses, detected through GRAPHYP's search experience networks. The goal of\nthis perspective is to envision AI systems that would not only provide answers\nbut also show users how those answers were reached, making artificial\nintelligence more transparent and trustworthy for human decision-making."}
{"id": "2507.20010", "pdf": "https://arxiv.org/pdf/2507.20010", "abs": "https://arxiv.org/abs/2507.20010", "authors": ["Müge Fidan", "Esra Erdem"], "title": "Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems", "categories": ["cs.AI", "cs.GT", "cs.LO"], "comment": null, "summary": "The Stable Roommates problems are characterized by the preferences of agents\nover other agents as roommates. A solution is a partition of the agents into\npairs that are acceptable to each other (i.e., they are in the preference lists\nof each other), and the matching is stable (i.e., there do not exist any two\nagents who prefer each other to their roommates, and thus block the matching).\nMotivated by real-world applications, and considering that stable roommates\nproblems do not always have solutions, we continue our studies to compute\n\"good-enough\" matchings. In addition to the agents' habits and habitual\npreferences, we consider their networks of preferred friends, and introduce a\nmethod to generate personalized solutions to stable roommates problems. We\nillustrate the usefulness of our method with examples and empirical\nevaluations."}
{"id": "2507.20067", "pdf": "https://arxiv.org/pdf/2507.20067", "abs": "https://arxiv.org/abs/2507.20067", "authors": ["Sarat Chandra Bobbili", "Ujwal Dinesha", "Dheeraj Narasimha", "Srinivas Shakkottai"], "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inference-time alignment enables large language models (LLMs) to generate\noutputs aligned with end-user preferences without further training. Recent\npost-training methods achieve this by using small guidance models to modify\ntoken generation during inference. These methods typically optimize a reward\nfunction KL-regularized by the original LLM taken as the reference policy. A\ncritical limitation, however, is their dependence on a pre-trained reward\nmodel, which requires fitting to human preference feedback--a potentially\nunstable process. In contrast, we introduce PITA, a novel framework that\nintegrates preference feedback directly into the LLM's token generation,\neliminating the need for a reward model. PITA learns a small preference-based\nguidance policy to modify token probabilities at inference time without LLM\nfine-tuning, reducing computational cost and bypassing the pre-trained reward\nmodel dependency. The problem is framed as identifying an underlying preference\ndistribution, solved through stochastic search and iterative refinement of the\npreference-based guidance model. We evaluate PITA across diverse tasks,\nincluding mathematical reasoning and sentiment classification, demonstrating\nits effectiveness in aligning LLM outputs with user preferences."}
{"id": "2507.20143", "pdf": "https://arxiv.org/pdf/2507.20143", "abs": "https://arxiv.org/abs/2507.20143", "authors": ["Zhonghan Ge", "Yuanyang Zhu", "Chunlin Chen"], "title": "Concept Learning for Cooperative Multi-Agent Reinforcement Learning", "categories": ["cs.AI"], "comment": "IEEE-China Conference on System Simulation Technology and its\n  Applications, 2025", "summary": "Despite substantial progress in applying neural networks (NN) to multi-agent\nreinforcement learning (MARL) areas, they still largely suffer from a lack of\ntransparency and interoperability. However, its implicit cooperative mechanism\nis not yet fully understood due to black-box networks. In this work, we study\nan interpretable value decomposition framework via concept bottleneck models,\nwhich promote trustworthiness by conditioning credit assignment on an\nintermediate level of human-like cooperation concepts. To address this problem,\nwe propose a novel value-based method, named Concepts learning for Multi-agent\nQ-learning (CMQ), that goes beyond the current performance-vs-interpretability\ntrade-off by learning interpretable cooperation concepts. CMQ represents each\ncooperation concept as a supervised vector, as opposed to existing models where\nthe information flowing through their end-to-end mechanism is concept-agnostic.\nIntuitively, using individual action value conditioning on global state\nembeddings to represent each concept allows for extra cooperation\nrepresentation capacity. Empirical evaluations on the StarCraft II\nmicromanagement challenge and level-based foraging (LBF) show that CMQ achieves\nsuperior performance compared with the state-of-the-art counterparts. The\nresults also demonstrate that CMQ provides more cooperation concept\nrepresentation capturing meaningful cooperation modes, and supports test-time\nconcept interventions for detecting potential biases of cooperation mode and\nidentifying spurious artifacts that impact cooperation."}
{"id": "2507.20150", "pdf": "https://arxiv.org/pdf/2507.20150", "abs": "https://arxiv.org/abs/2507.20150", "authors": ["Xingcheng Xu"], "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) plays a crucial role in shaping the behavior of\nlarge language and reasoning models (LLMs/LRMs). However, it often produces\nbrittle and unstable policies, leading to critical failures such as spurious\nreasoning, deceptive alignment, and instruction disobedience that undermine the\ntrustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified\ntheoretical explanation and are typically addressed using ad-hoc heuristics.\nThis paper presents a rigorous mathematical framework for analyzing the\nstability of the mapping from a reward function to the optimal policy. We show\nthat policy brittleness often stems from non-unique optimal actions, a common\noccurrence when multiple valid traces exist in a reasoning task. This\ntheoretical lens provides a unified explanation for a range of seemingly\ndisparate failures, reframing them as rational outcomes of optimizing rewards\nthat may be incomplete or noisy, especially in the presence of action\ndegeneracy. We extend this analysis from the fundamental single-reward setting\nto the more realistic multi-reward RL across diverse domains, showing how\nstability is governed by an \"effective reward\" aggregation mechanism. We also\nprove that entropy regularization restores policy stability at the cost of\nincreased stochasticity. Our framework provides a unified explanation for\nrecent empirical findings on deceptive reasoning, instruction-following\ntrade-offs, and RLHF-induced sophistry, and is further validated through\nperturbation experiments in multi-reward RL. This work advances\npolicy-stability analysis from empirical heuristics towards a principled\ntheory, offering essential insights for designing safer and more trustworthy AI\nsystems."}
{"id": "2507.20199", "pdf": "https://arxiv.org/pdf/2507.20199", "abs": "https://arxiv.org/abs/2507.20199", "authors": ["Shijie Shang", "Ruosi Wan", "Yue Peng", "Yutong Wu", "Xiong-hui Chen", "Jie Yan", "Xiangyu Zhang"], "title": "StepFun-Prover Preview: Let's Think and Verify Step by Step", "categories": ["cs.AI"], "comment": "25 pages, 4 figures", "summary": "We present StepFun-Prover Preview, a large language model designed for formal\ntheorem proving through tool-integrated reasoning. Using a reinforcement\nlearning pipeline that incorporates tool-based interactions, StepFun-Prover can\nachieve strong performance in generating Lean 4 proofs with minimal sampling.\nOur approach enables the model to emulate human-like problem-solving strategies\nby iteratively refining proofs based on real-time environment feedback. On the\nminiF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of\n$70.0\\%$. Beyond advancing benchmark performance, we introduce an end-to-end\ntraining framework for developing tool-integrated reasoning models, offering a\npromising direction for automated theorem proving and Math AI assistant."}
{"id": "2507.20226", "pdf": "https://arxiv.org/pdf/2507.20226", "abs": "https://arxiv.org/abs/2507.20226", "authors": ["Shuyang Guo", "Wenjin Xie", "Ping Lu", "Ting Deng", "Richong Zhang", "Jianxin Li", "Xiangping Huang", "Zhongyi Liu"], "title": "Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks", "categories": ["cs.AI"], "comment": null, "summary": "Homomorphism is a key mapping technique between graphs that preserves their\nstructure. Given a graph and a pattern, the subgraph homomorphism problem\ninvolves finding a mapping from the pattern to the graph, ensuring that\nadjacent vertices in the pattern are mapped to adjacent vertices in the graph.\nUnlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism\nallows multiple vertices in the pattern to map to the same vertex in the graph,\nmaking it more complex. We propose HFrame, the first graph neural network-based\nframework for subgraph homomorphism, which integrates traditional algorithms\nwith machine learning techniques. We demonstrate that HFrame outperforms\nstandard graph neural networks by being able to distinguish more graph pairs\nwhere the pattern is not homomorphic to the graph. Additionally, we provide a\ngeneralization error bound for HFrame. Through experiments on both real-world\nand synthetic graphs, we show that HFrame is up to 101.91 times faster than\nexact matching algorithms and achieves an average accuracy of 0.962."}
{"id": "2507.20230", "pdf": "https://arxiv.org/pdf/2507.20230", "abs": "https://arxiv.org/abs/2507.20230", "authors": ["Yufan Chen", "Ching Ting Leung", "Bowen Yu", "Jianwei Sun", "Yong Huang", "Linyan Li", "Hao Chen", "Hanyu Gao"], "title": "A Multi-Agent System for Information Extraction from the Chemical Literature", "categories": ["cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "To fully expedite AI-powered chemical research, high-quality chemical\ndatabases are the cornerstone. Automatic extraction of chemical information\nfrom the literature is essential for constructing reaction databases, but it is\ncurrently limited by the multimodality and style variability of chemical\ninformation. In this work, we developed a multimodal large language model\n(MLLM)-based multi-agent system for automatic chemical information extraction.\nWe used the MLLM's strong reasoning capability to understand the structure of\ncomplex chemical graphics, decompose the extraction task into sub-tasks and\ncoordinate a set of specialized agents to solve them. Our system achieved an F1\nscore of 80.8% on a benchmark dataset of complex chemical reaction graphics\nfrom the literature, surpassing the previous state-of-the-art model (F1 score:\n35.6%) by a significant margin. Additionally, it demonstrated consistent\nimprovements in key sub-tasks, including molecular image recognition, reaction\nimage parsing, named entity recognition and text-based reaction extraction.\nThis work is a critical step toward automated chemical information extraction\ninto structured datasets, which will be a strong promoter of AI-driven chemical\nresearch."}
{"id": "2507.20280", "pdf": "https://arxiv.org/pdf/2507.20280", "abs": "https://arxiv.org/abs/2507.20280", "authors": ["Keyan Ding", "Jing Yu", "Junjie Huang", "Yuchen Yang", "Qiang Zhang", "Huajun Chen"], "title": "SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration", "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 6 figures", "summary": "Scientific research increasingly relies on specialized computational tools,\nyet effectively utilizing these tools demands substantial domain expertise.\nWhile Large Language Models (LLMs) show promise in tool automation, they\nstruggle to seamlessly integrate and orchestrate multiple tools for complex\nscientific workflows. Here, we present SciToolAgent, an LLM-powered agent that\nautomates hundreds of scientific tools across biology, chemistry, and materials\nscience. At its core, SciToolAgent leverages a scientific tool knowledge graph\nthat enables intelligent tool selection and execution through graph-based\nretrieval-augmented generation. The agent also incorporates a comprehensive\nsafety-checking module to ensure responsible and ethical tool usage. Extensive\nevaluations on a curated benchmark demonstrate that SciToolAgent significantly\noutperforms existing approaches. Case studies in protein engineering, chemical\nreactivity prediction, chemical synthesis, and metal-organic framework\nscreening further demonstrate SciToolAgent's capability to automate complex\nscientific workflows, making advanced research tools accessible to both experts\nand non-experts."}
{"id": "2507.20322", "pdf": "https://arxiv.org/pdf/2507.20322", "abs": "https://arxiv.org/abs/2507.20322", "authors": ["Manish Verma", "Vivek Sharma", "Vishal Singh"], "title": "Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting", "categories": ["cs.AI"], "comment": "Page 4-Figure 1 and Page 11-Figure 2 . A preprint describing a system\n  for AI-powered technology scouting", "summary": "This paper presents the development of an AI powered software platform that\nleverages advanced large language models (LLMs) to transform technology\nscouting and solution discovery in industrial R&D. Traditional approaches to\nsolving complex research and development challenges are often time consuming,\nmanually driven, and heavily dependent on domain specific expertise. These\nmethods typically involve navigating fragmented sources such as patent\nrepositories, commercial product catalogs, and competitor data, leading to\ninefficiencies and incomplete insights. The proposed platform utilizes cutting\nedge LLM capabilities including semantic understanding, contextual reasoning,\nand cross-domain knowledge extraction to interpret problem statements and\nretrieve high-quality, sustainable solutions. The system processes unstructured\npatent texts, such as claims and technical descriptions, and systematically\nextracts potential innovations aligned with the given problem context. These\nsolutions are then algorithmically organized under standardized technical\ncategories and subcategories to ensure clarity and relevance across\ninterdisciplinary domains. In addition to patent analysis, the platform\nintegrates commercial intelligence by identifying validated market solutions\nand active organizations addressing similar challenges. This combined insight\nsourced from both intellectual property and real world product data enables R&D\nteams to assess not only technical novelty but also feasibility, scalability,\nand sustainability. The result is a comprehensive, AI driven scouting engine\nthat reduces manual effort, accelerates innovation cycles, and enhances\ndecision making in complex R&D environments."}
{"id": "2507.20333", "pdf": "https://arxiv.org/pdf/2507.20333", "abs": "https://arxiv.org/abs/2507.20333", "authors": ["Rachel S. Y. Teo", "Laziz U. Abdullaev", "Tan M. Nguyen"], "title": "The Blessing and Curse of Dimensionality in Safety Alignment", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "Published as a conference paper at COLM 2025", "summary": "The focus on safety alignment in large language models (LLMs) has increased\nsignificantly due to their widespread adoption across different domains. The\nscale of LLMs play a contributing role in their success, and the growth in\nparameter count follows larger hidden dimensions. In this paper, we hypothesize\nthat while the increase in dimensions has been a key advantage, it may lead to\nemergent problems as well. These problems emerge as the linear structures in\nthe activation space can be exploited, in the form of activation engineering,\nto circumvent its safety alignment. Through detailed visualizations of linear\nsubspaces associated with different concepts, such as safety, across various\nmodel scales, we show that the curse of high-dimensional representations\nuniquely impacts LLMs. Further substantiating our claim, we demonstrate that\nprojecting the representations of the model onto a lower dimensional subspace\ncan preserve sufficient information for alignment while avoiding those linear\nstructures. Empirical results confirm that such dimensional reduction\nsignificantly reduces susceptibility to jailbreaking through representation\nengineering. Building on our empirical validations, we provide theoretical\ninsights into these linear jailbreaking methods relative to a model's hidden\ndimensions. Broadly speaking, our work posits that the high dimensions of a\nmodel's internal representations can be both a blessing and a curse in safety\nalignment."}
{"id": "2507.20342", "pdf": "https://arxiv.org/pdf/2507.20342", "abs": "https://arxiv.org/abs/2507.20342", "authors": ["Zhipeng Tang", "Sha Zhang", "Jiajun Deng", "Chenjie Wang", "Guoliang You", "Yuting Huang", "Xinrui Lin", "Yanyong Zhang"], "title": "VLMPlanner: Integrating Visual Language Models with Motion Planning", "categories": ["cs.AI", "cs.RO"], "comment": "8 pages, 3 figures, this paper has been accepted by ACM MM 2025", "summary": "Integrating large language models (LLMs) into autonomous driving motion\nplanning has recently emerged as a promising direction, offering enhanced\ninterpretability, better controllability, and improved generalization in rare\nand long-tail scenarios. However, existing methods often rely on abstracted\nperception or map-based inputs, missing crucial visual context, such as\nfine-grained road cues, accident aftermath, or unexpected obstacles, which are\nessential for robust decision-making in complex driving environments. To bridge\nthis gap, we propose VLMPlanner, a hybrid framework that combines a\nlearning-based real-time planner with a vision-language model (VLM) capable of\nreasoning over raw images. The VLM processes multi-view images to capture rich,\ndetailed visual information and leverages its common-sense reasoning\ncapabilities to guide the real-time planner in generating robust and safe\ntrajectories. Furthermore, we develop the Context-Adaptive Inference Gate\n(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by\ndynamically adjusting its inference frequency based on scene complexity,\nthereby achieving an optimal balance between planning performance and\ncomputational efficiency. We evaluate our approach on the large-scale,\nchallenging nuPlan benchmark, with comprehensive experimental results\ndemonstrating superior planning performance in scenarios with intricate road\nconditions and dynamic elements. Code will be available."}
{"id": "2507.20377", "pdf": "https://arxiv.org/pdf/2507.20377", "abs": "https://arxiv.org/abs/2507.20377", "authors": ["Farshid Nooshi", "Suining He"], "title": "Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping", "categories": ["cs.AI"], "comment": "5 pages, UrbComp 2025", "summary": "Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing\nvehicles) is crucial for rebalancing the mobility demand and supply in the\nurban environments. We propose in this work a novel multi-agent reinforcement\nlearning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)\nfor dynamic mobility resource allocation. HAG-PS aims to address two important\nresearch challenges regarding multi-agent reinforcement learning for mobility\nresource allocation: (1) how to dynamically and adaptively share the mobility\nresource allocation policy (i.e., how to distribute mobility resources) across\nagents (i.e., representing the regional coordinators of mobility resources);\nand (2) how to achieve memory-efficient parameter sharing in an urban-scale\nsetting. To address the above challenges, we have provided following novel\ndesigns within HAG-PS. To enable dynamic and adaptive parameter sharing, we\nhave designed a hierarchical approach that consists of global and local\ninformation of the mobility resource states (e.g., distribution of mobility\nresources). We have developed an adaptive agent grouping approach in order to\nsplit or merge the groups of agents based on their relative closeness of\nencoded trajectories (i.e., states, actions, and rewards). We have designed a\nlearnable identity (ID) embeddings to enable agent specialization beyond simple\nparameter copy. We have performed extensive experimental studies based on\nreal-world NYC bike sharing data (a total of more than 1.2 million trips), and\ndemonstrated the superior performance (e.g., improved bike availability) of\nHAG-PS compared with other baseline approaches."}
{"id": "2507.20395", "pdf": "https://arxiv.org/pdf/2507.20395", "abs": "https://arxiv.org/abs/2507.20395", "authors": ["Hafsteinn Einarsson"], "title": "MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly power autonomous agents in\nrobotics and embodied AI, understanding their spatial reasoning capabilities\nbecomes crucial for ensuring reliable real-world deployment. Despite advances\nin language understanding, current research lacks evaluation of how LLMs\nperform spatial navigation without visual cues, a fundamental requirement for\nagents operating with limited sensory information. This paper addresses this\ngap by introducing MazeEval, a benchmark designed to isolate and evaluate pure\nspatial reasoning in LLMs through coordinate-based maze navigation tasks. Our\nmethodology employs a function-calling interface where models navigate mazes of\nvarying complexity ($5\\times 5$ to $15\\times 15$ grids) using only coordinate\nfeedback and distance-to-wall information, excluding visual input to test\nfundamental spatial cognition. We evaluate eight state-of-the-art LLMs across\nidentical mazes in both English and Icelandic to assess cross-linguistic\ntransfer of spatial abilities. Our findings reveal striking disparities: while\nOpenAI's O3 achieves perfect navigation for mazes up to size $30\\times 30$,\nother models exhibit catastrophic failure beyond $9\\times 9$ mazes, with 100%\nof failures attributed to excessive looping behavior where models revisit a\ncell at least 10 times. We document a significant performance degradation in\nIcelandic, with models solving mazes 3-4 sizes smaller than in English,\nsuggesting spatial reasoning in LLMs emerges from linguistic patterns rather\nthan language-agnostic mechanisms. These results have important implications\nfor global deployment of LLM-powered autonomous systems, showing spatial\nintelligence remains fundamentally constrained by training data availability\nand highlighting the need for architectural innovations to achieve reliable\nnavigation across linguistic contexts."}
{"id": "2507.20444", "pdf": "https://arxiv.org/pdf/2507.20444", "abs": "https://arxiv.org/abs/2507.20444", "authors": ["Chengzhuo Han"], "title": "Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems", "categories": ["cs.AI"], "comment": null, "summary": "In the context of the rapidly evolving information technology landscape,\nmarked by the advent of 6G communication networks, we face an increased data\nvolume and complexity in network environments. This paper addresses these\nchallenges by focusing on Quality of Service (QoS) in edge computing\nframeworks. We propose a novel approach to enhance QoS through the development\nof General Artificial Intelligence Lifelong Learning Systems, with a special\nemphasis on Federated Layering Techniques (FLT). Our work introduces a\nfederated layering-based small model collaborative mechanism aimed at improving\nAI models' operational efficiency and response time in environments where\nresources are limited. This innovative method leverages the strengths of cloud\nand edge computing, incorporating a negotiation and debate mechanism among\nsmall AI models to enhance reasoning and decision-making processes. By\nintegrating model layering techniques with privacy protection measures, our\napproach ensures the secure transmission of model parameters while maintaining\nhigh efficiency in learning and reasoning capabilities. The experimental\nresults demonstrate that our strategy not only enhances learning efficiency and\nreasoning accuracy but also effectively protects the privacy of edge nodes.\nThis presents a viable solution for achieving resilient large model lifelong\nlearning systems, with a significant improvement in QoS for edge computing\nenvironments."}
{"id": "2507.20451", "pdf": "https://arxiv.org/pdf/2507.20451", "abs": "https://arxiv.org/abs/2507.20451", "authors": ["Pritom Ray Nobin", "Imran Ahammad Rifat"], "title": "STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction", "categories": ["cs.AI"], "comment": "10 pages", "summary": "Accurate prediction of traffic accident severity is critical for improving\nroad safety, optimizing emergency response strategies, and informing the design\nof safer transportation infrastructure. However, existing approaches often\nstruggle to effectively model the intricate interdependencies among spatial,\ntemporal, and contextual variables that govern accident outcomes. In this\nstudy, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention\nNetwork, which leverages adaptive graph construction and modality-aware\nattention mechanisms to capture these complex relationships. Unlike\nconventional methods, STARN-GAT integrates road network topology, temporal\ntraffic patterns, and environmental context within a unified attention-based\nframework. The model is evaluated on the Fatality Analysis Reporting System\n(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and\nrecall of 81 percent for severe incidents. To ensure generalizability within\nthe South Asian context, STARN-GAT is further validated on the ARI-BUET traffic\naccident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,\nand ROC-AUC of 0.89. These results demonstrate the model's effectiveness in\nidentifying high-risk cases and its potential for deployment in real-time,\nsafety-critical traffic management systems. Furthermore, the attention-based\narchitecture enhances interpretability, offering insights into contributing\nfactors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT\nbridges the gap between advanced graph neural network techniques and practical\napplications in road safety analytics."}
{"id": "2507.20526", "pdf": "https://arxiv.org/pdf/2507.20526", "abs": "https://arxiv.org/abs/2507.20526", "authors": ["Andy Zou", "Maxwell Lin", "Eliot Jones", "Micha Nowak", "Mateusz Dziemian", "Nick Winter", "Alexander Grattan", "Valent Nathanael", "Ayla Croft", "Xander Davies", "Jai Patel", "Robert Kirk", "Nate Burnikell", "Yarin Gal", "Dan Hendrycks", "J. Zico Kolter", "Matt Fredrikson"], "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances have enabled LLM-powered AI agents to autonomously execute\ncomplex tasks by combining language model reasoning with tools, memory, and web\naccess. But can these systems be trusted to follow deployment policies in\nrealistic environments, especially under attack? To investigate, we ran the\nlargest public red-teaming competition to date, targeting 22 frontier AI agents\nacross 44 realistic deployment scenarios. Participants submitted 1.8 million\nprompt-injection attacks, with over 60,000 successfully eliciting policy\nviolations such as unauthorized data access, illicit financial actions, and\nregulatory noncompliance. We use these results to build the Agent Red Teaming\n(ART) benchmark - a curated set of high-impact attacks - and evaluate it across\n19 state-of-the-art models. Nearly all agents exhibit policy violations for\nmost behaviors within 10-100 queries, with high attack transferability across\nmodels and tasks. Importantly, we find limited correlation between agent\nrobustness and model size, capability, or inference-time compute, suggesting\nthat additional defenses are needed against adversarial misuse. Our findings\nhighlight critical and persistent vulnerabilities in today's AI agents. By\nreleasing the ART benchmark and accompanying evaluation framework, we aim to\nsupport more rigorous security assessment and drive progress toward safer agent\ndeployment."}
{"id": "2507.20541", "pdf": "https://arxiv.org/pdf/2507.20541", "abs": "https://arxiv.org/abs/2507.20541", "authors": ["Zishang Qiu", "Xinan Chen", "Long Chen", "Ruibin Bai"], "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD."}
{"id": "2507.20566", "pdf": "https://arxiv.org/pdf/2507.20566", "abs": "https://arxiv.org/abs/2507.20566", "authors": ["Jiajun Liu", "Wenjun Ke", "Peng Wang", "Yao He", "Ziyu Shang", "Guozheng Li", "Zijie Xu", "Ke Ji"], "title": "Unlearning of Knowledge Graph Embedding via Preference Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Existing knowledge graphs (KGs) inevitably contain outdated or erroneous\nknowledge that needs to be removed from knowledge graph embedding (KGE) models.\nTo address this challenge, knowledge unlearning can be applied to eliminate\nspecific information while preserving the integrity of the remaining knowledge\nin KGs. Existing unlearning methods can generally be categorized into exact\nunlearning and approximate unlearning. However, exact unlearning requires high\ntraining costs while approximate unlearning faces two issues when applied to\nKGs due to the inherent connectivity of triples: (1) It fails to fully remove\ntargeted information, as forgetting triples can still be inferred from\nremaining ones. (2) It focuses on local data for specific removal, which\nweakens the remaining knowledge in the forgetting boundary. To address these\nissues, we propose GraphDPO, a novel approximate unlearning framework based on\ndirect preference optimization (DPO). Firstly, to effectively remove forgetting\ntriples, we reframe unlearning as a preference optimization problem, where the\nmodel is trained by DPO to prefer reconstructed alternatives over the original\nforgetting triples. This formulation penalizes reliance on forgettable\nknowledge, mitigating incomplete forgetting caused by KG connectivity.\nMoreover, we introduce an out-boundary sampling strategy to construct\npreference pairs with minimal semantic overlap, weakening the connection\nbetween forgetting and retained knowledge. Secondly, to preserve boundary\nknowledge, we introduce a boundary recall mechanism that replays and distills\nrelevant information both within and across time steps. We construct eight\nunlearning datasets across four popular KGs with varying unlearning rates.\nExperiments show that GraphDPO outperforms state-of-the-art baselines by up to\n10.1% in MRR_Avg and 14.0% in MRR_F1."}
{"id": "2507.20613", "pdf": "https://arxiv.org/pdf/2507.20613", "abs": "https://arxiv.org/abs/2507.20613", "authors": ["Te Zhang", "Yuheng Li", "Junxiang Wang", "Lujun Li"], "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression", "categories": ["cs.AI", "cs.LG"], "comment": "6 pages", "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."}
{"id": "2507.20620", "pdf": "https://arxiv.org/pdf/2507.20620", "abs": "https://arxiv.org/abs/2507.20620", "authors": ["Lijian Li"], "title": "Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world\nknowledge in multimodal knowledge graphs by leveraging both multimodal and\nstructural entity information. However, the inherent imbalance in multimodal\nknowledge graphs, where modality distributions vary across entities, poses\nchallenges in utilizing additional modality data for robust entity\nrepresentation. Existing MMKGC methods typically rely on attention or\ngate-based fusion mechanisms but overlook complementarity contained in\nmulti-modal data. In this paper, we propose a novel framework named Mixture of\nComplementary Modality Experts (MoCME), which consists of a\nComplementarity-guided Modality Knowledge Fusion (CMKF) module and an\nEntropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits\nboth intra-modal and inter-modal complementarity to fuse multi-view and\nmulti-modal embeddings, enhancing representations of entities. Additionally, we\nintroduce an Entropy-guided Negative Sampling mechanism to dynamically\nprioritize informative and uncertain negative samples to enhance training\neffectiveness and model robustness. Extensive experiments on five benchmark\ndatasets demonstrate that our MoCME achieves state-of-the-art performance,\nsurpassing existing approaches."}
{"id": "2507.20641", "pdf": "https://arxiv.org/pdf/2507.20641", "abs": "https://arxiv.org/abs/2507.20641", "authors": ["Lijian Li"], "title": "Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "At present, state-of-the-art forecasting models are short of the ability to\ncapture spatio-temporal dependency and synthesize global information at the\nstage of learning. To address this issue, in this paper, through the adaptive\nfuzzified construction of temporal data, we propose a novel convolutional\narchitecture with partially asymmetric design based on the scheme of sliding\nwindow to realize accurate time series forecasting. First, the construction\nstrategy of traditional fuzzy time series is improved to further extract short\nand long term temporal interrelation, which enables every time node to\nautomatically possess corresponding global information and inner relationships\namong them in a restricted sliding window and the process does not require\nhuman involvement. Second, a bilateral Atrous algorithm is devised to reduce\ncalculation demand of the proposed model without sacrificing global\ncharacteristics of elements. And it also allows the model to avoid processing\nredundant information. Third, after the transformation of time series, a\npartially asymmetric convolutional architecture is designed to more flexibly\nmine data features by filters in different directions on feature maps, which\ngives the convolutional neural network (CNN) the ability to construct\nsub-windows within existing sliding windows to model at a more fine-grained\nlevel. And after obtaining the time series information at different levels, the\nmulti-scale features from different sub-windows will be sent to the\ncorresponding network layer for time series information fusion. Compared with\nother competitive modern models, the proposed method achieves state-of-the-art\nresults on most of popular time series datasets, which is fully verified by the\nexperimental results."}
{"id": "2507.20703", "pdf": "https://arxiv.org/pdf/2507.20703", "abs": "https://arxiv.org/abs/2507.20703", "authors": ["Aysu Bogatarkan", "Esra Erdem"], "title": "A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels", "categories": ["cs.AI"], "comment": null, "summary": "MAPF problem aims to find plans for multiple agents in an environment within\na given time, such that the agents do not collide with each other or obstacles.\nMotivated by the execution and monitoring of these plans, we study Dynamic MAPF\n(D-MAPF) problem, which allows changes such as agents entering/leaving the\nenvironment or obstacles being removed/moved. Considering the requirements of\nreal-world applications in warehouses with the presence of humans, we introduce\n1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a\nnew framework to solve D-MAPF (utilizing multi-shot computation, and allowing\ndifferent methods to solve D-MAPF), and 3) a new ASP-based method to solve\nD-MAPF (combining advantages of replanning and repairing methods, with a novel\nconcept of tunnels to specify where agents can move). We have illustrated the\nstrengths and weaknesses of this method by experimental evaluations, from the\nperspectives of computational performance and quality of solutions."}
{"id": "2507.20711", "pdf": "https://arxiv.org/pdf/2507.20711", "abs": "https://arxiv.org/abs/2507.20711", "authors": ["Filip Cano", "Thomas A. Henzinger", "Konstantin Kueffner"], "title": "Algorithmic Fairness: A Runtime Perspective", "categories": ["cs.AI"], "comment": "To appear in RV 2025", "summary": "Fairness in AI is traditionally studied as a static property evaluated once,\nover a fixed dataset. However, real-world AI systems operate sequentially, with\noutcomes and environments evolving over time. This paper proposes a framework\nfor analysing fairness as a runtime property. Using a minimal yet expressive\nmodel based on sequences of coin tosses with possibly evolving biases, we study\nthe problems of monitoring and enforcing fairness expressed in either toss\noutcomes or coin biases. Since there is no one-size-fits-all solution for\neither problem, we provide a summary of monitoring and enforcement strategies,\nparametrised by environment dynamics, prediction horizon, and confidence\nthresholds. For both problems, we present general results under simple or\nminimal assumptions. We survey existing solutions for the monitoring problem\nfor Markovian and additive dynamics, and existing solutions for the enforcement\nproblem in static settings with known dynamics."}
{"id": "2507.20728", "pdf": "https://arxiv.org/pdf/2507.20728", "abs": "https://arxiv.org/abs/2507.20728", "authors": ["Andrés Holgado-Sánchez", "Holger Billhardt", "Sascha Ossowski", "Sara Degli-Esposti"], "title": "Learning the Value Systems of Societies from Preferences", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "Full version of publication under the same accepted at ECAI 2025\n  conference (Submission 6755). 8 pages + 2 supplementary material", "summary": "Aligning AI systems with human values and the value-based preferences of\nvarious stakeholders (their value systems) is key in ethical AI. In value-aware\nAI systems, decision-making draws upon explicit computational representations\nof individual values (groundings) and their aggregation into value systems. As\nthese are notoriously difficult to elicit and calibrate manually, value\nlearning approaches aim to automatically derive computational models of an\nagent's values and value system from demonstrations of human behaviour.\nNonetheless, social science and humanities literature suggest that it is more\nadequate to conceive the value system of a society as a set of value systems of\ndifferent groups, rather than as the simple aggregation of individual value\nsystems. Accordingly, here we formalize the problem of learning the value\nsystems of societies and propose a method to address it based on heuristic deep\nclustering. The method learns socially shared value groundings and a set of\ndiverse value systems representing a given society by observing qualitative\nvalue-based preferences from a sample of agents. We evaluate the proposal in a\nuse case with real data about travelling decisions."}
{"id": "2507.20755", "pdf": "https://arxiv.org/pdf/2507.20755", "abs": "https://arxiv.org/abs/2507.20755", "authors": ["Arpan Dasgupta", "Sarvesh Gharat", "Neha Madhiwalla", "Aparna Hegde", "Milind Tambe", "Aparna Taneja"], "title": "Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours", "categories": ["cs.AI"], "comment": null, "summary": "Automated voice calls with health information are a proven method for\ndisseminating maternal and child health information among beneficiaries and are\ndeployed in several programs around the world. However, these programs often\nsuffer from beneficiary dropoffs and poor engagement. In previous work, through\nreal-world trials, we showed that an AI model, specifically a restless bandit\nmodel, could identify beneficiaries who would benefit most from live service\ncall interventions, preventing dropoffs and boosting engagement. However, one\nkey question has remained open so far: does such improved listenership via\nAI-targeted interventions translate into beneficiaries' improved knowledge and\nhealth behaviors? We present a first study that shows not only listenership\nimprovements due to AI interventions, but also simultaneously links these\nimprovements to health behavior changes. Specifically, we demonstrate that\nAI-scheduled interventions, which enhance listenership, lead to statistically\nsignificant improvements in beneficiaries' health behaviors such as taking iron\nor calcium supplements in the postnatal period, as well as understanding of\ncritical health topics during pregnancy and infancy. This underscores the\npotential of AI to drive meaningful improvements in maternal and child health."}
{"id": "2507.20758", "pdf": "https://arxiv.org/pdf/2507.20758", "abs": "https://arxiv.org/abs/2507.20758", "authors": ["Hao Yang", "Qinghua Zhao", "Lei Li"], "title": "How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation", "categories": ["cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet\nits internal mechanisms remain poorly understood. We analyze CoT's operational\nprinciples by reversely tracing information flow across decoding, projection,\nand activation phases. Our quantitative analysis suggests that CoT may serve as\na decoding space pruner, leveraging answer templates to guide output\ngeneration, with higher template adherence strongly correlating with improved\nperformance. Furthermore, we surprisingly find that CoT modulates neuron\nengagement in a task-dependent manner: reducing neuron activation in\nopen-domain tasks, yet increasing it in closed-domain scenarios. These findings\noffer a novel mechanistic interpretability framework and critical insights for\nenabling targeted CoT interventions to design more efficient and robust\nprompts. We released our code and data at\nhttps://anonymous.4open.science/r/cot-D247."}
{"id": "2507.20774", "pdf": "https://arxiv.org/pdf/2507.20774", "abs": "https://arxiv.org/abs/2507.20774", "authors": ["Fatou Ndiaye Mbodji"], "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments", "categories": ["cs.AI"], "comment": "4 pages, 4 tables", "summary": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods."}
{"id": "2507.20804", "pdf": "https://arxiv.org/pdf/2507.20804", "abs": "https://arxiv.org/abs/2507.20804", "authors": ["Xueyao Wan", "Hang Yu"], "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language model generation by\nretrieving relevant information from external knowledge bases. However,\nconventional RAG methods face the issue of missing multimodal information.\nMultimodal RAG methods address this by fusing images and text through mapping\nthem into a shared embedding space, but they fail to capture the structure of\nknowledge and logical chains between modalities. Moreover, they also require\nlarge-scale training for specific tasks, resulting in limited generalizing\nability. To address these limitations, we propose MMGraphRAG, which refines\nvisual content through scene graphs and constructs a multimodal knowledge graph\n(MMKG) in conjunction with text-based KG. It employs spectral clustering to\nachieve cross-modal entity linking and retrieves context along reasoning paths\nto guide the generative process. Experimental results show that MMGraphRAG\nachieves state-of-the-art performance on the DocBench and MMLongBench datasets,\ndemonstrating strong domain adaptability and clear reasoning paths."}
{"id": "2507.20951", "pdf": "https://arxiv.org/pdf/2507.20951", "abs": "https://arxiv.org/abs/2507.20951", "authors": ["Yang You", "Vincent Thomas", "Alex Schutz", "Robert Skilton", "Nick Hawes", "Olivier Buffet"], "title": "Partially Observable Monte-Carlo Graph Search", "categories": ["cs.AI", "cs.RO"], "comment": "To be published in Proceedings of ICAPS 2025", "summary": "Currently, large partially observable Markov decision processes (POMDPs) are\noften solved by sampling-based online methods which interleave planning and\nexecution phases. However, a pre-computed offline policy is more desirable in\nPOMDP applications with time or energy constraints. But previous offline\nalgorithms are not able to scale up to large POMDPs. In this article, we\npropose a new sampling-based algorithm, the partially observable Monte-Carlo\ngraph search (POMCGS) to solve large POMDPs offline. Different from many online\nPOMDP methods, which progressively develop a tree while performing\n(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to\nconstruct a policy graph, so that computations can be drastically reduced, and\nusers can analyze and validate the policy prior to embedding and executing it.\nMoreover, POMCGS, together with action progressive widening and observation\nclustering methods provided in this article, is able to address certain\ncontinuous POMDPs. Through experiments, we demonstrate that POMCGS can generate\npolicies on the most challenging POMDPs, which cannot be computed by previous\noffline algorithms, and these policies' values are competitive compared with\nthe state-of-the-art online POMDP algorithms."}
{"id": "2507.20960", "pdf": "https://arxiv.org/pdf/2507.20960", "abs": "https://arxiv.org/abs/2507.20960", "authors": ["Bill Cochran"], "title": "On the Limits of Hierarchically Embedded Logic in Classical Neural Networks", "categories": ["cs.AI"], "comment": "9 pages", "summary": "We propose a formal model of reasoning limitations in large neural net models\nfor language, grounded in the depth of their neural architecture. By treating\nneural networks as linear operators over logic predicate space we show that\neach layer can encode at most one additional level of logical reasoning. We\nprove that a neural network of depth a particular depth cannot faithfully\nrepresent predicates in a one higher order logic, such as simple counting over\ncomplex predicates, implying a strict upper bound on logical expressiveness.\nThis structure induces a nontrivial null space during tokenization and\nembedding, excluding higher-order predicates from representability. Our\nframework offers a natural explanation for phenomena such as hallucination,\nrepetition, and limited planning, while also providing a foundation for\nunderstanding how approximations to higher-order logic may emerge. These\nresults motivate architectural extensions and interpretability strategies in\nfuture development of language models."}
{"id": "2507.20964", "pdf": "https://arxiv.org/pdf/2507.20964", "abs": "https://arxiv.org/abs/2507.20964", "authors": ["Aran Nayebi"], "title": "Core Safety Values for Provably Corrigible Agents", "categories": ["cs.AI", "cs.CC", "cs.GT", "cs.LG", "cs.MA"], "comment": "14 pages", "summary": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems."}
{"id": "2507.21017", "pdf": "https://arxiv.org/pdf/2507.21017", "abs": "https://arxiv.org/abs/2507.21017", "authors": ["Weichen Zhang", "Yiyou Sun", "Pohao Huang", "Jiayue Pu", "Heyue Lin", "Dawn Song"], "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them", "categories": ["cs.AI"], "comment": "Code and data: https://github.com/sunblaze-ucb/mirage-bench.git", "summary": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments."}
{"id": "2507.21027", "pdf": "https://arxiv.org/pdf/2507.21027", "abs": "https://arxiv.org/abs/2507.21027", "authors": ["Lucia Balážová", "Richard Comploi-Taupe", "Susana Hahn", "Nicolas Rühling", "Gottfried Schenner"], "title": "Smart Expansion Techniques for ASP-based Interactive Configuration", "categories": ["cs.AI", "cs.SE", "D.1.6; I.2.1"], "comment": "Under consideration for publication in Theory and Practice of Logic\n  Programming (TPLP)", "summary": "Product configuration is a successful application of Answer Set Programming\n(ASP). However, challenges are still open for interactive systems to\neffectively guide users through the configuration process. The aim of our work\nis to provide an ASP-based solver for interactive configuration that can deal\nwith large-scale industrial configuration problems and that supports intuitive\nuser interfaces via an API. In this paper, we focus on improving the\nperformance of automatically completing a partial configuration. Our main\ncontribution enhances the classical incremental approach for multi-shot solving\nby four different smart expansion functions. The core idea is to determine and\nadd specific objects or associations to the partial configuration by exploiting\ncautious and brave consequences before checking for the existence of a complete\nconfiguration with the current objects in each iteration. This approach limits\nthe number of costly unsatisfiability checks and reduces the search space,\nthereby improving solving performance. In addition, we present a user interface\nthat uses our API and is implemented in ASP."}
{"id": "2507.21035", "pdf": "https://arxiv.org/pdf/2507.21035", "abs": "https://arxiv.org/abs/2507.21035", "authors": ["Haoyang Liu", "Yijiang Li", "Haohan Wang"], "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis", "categories": ["cs.AI", "cs.LG", "cs.MA", "q-bio.GN"], "comment": "51 pages, 5 figures", "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."}
{"id": "2507.21046", "pdf": "https://arxiv.org/pdf/2507.21046", "abs": "https://arxiv.org/abs/2507.21046", "authors": ["Huan-ang Gao", "Jiayi Geng", "Wenyue Hua", "Mengkang Hu", "Xinzhe Juan", "Hongzhang Liu", "Shilong Liu", "Jiahao Qiu", "Xuan Qi", "Yiran Wu", "Hongru Wang", "Han Xiao", "Yuhang Zhou", "Shaokun Zhang", "Jiayi Zhang", "Jinyu Xiang", "Yixiong Fang", "Qiwen Zhao", "Dongrui Liu", "Qihan Ren", "Cheng Qian", "Zhenghailong Wang", "Minda Hu", "Huazheng Wang", "Qingyun Wu", "Heng Ji", "Mengdi Wang"], "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence", "categories": ["cs.AI"], "comment": "51 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."}
{"id": "2407.05592", "pdf": "https://arxiv.org/pdf/2407.05592", "abs": "https://arxiv.org/abs/2407.05592", "authors": ["Zehui Zhao", "Laith Alzubaidi", "Jinglan Zhang", "Ye Duan", "Usman Naseem", "Yuantong Gu"], "title": "Transfer or Self-Supervised? Bridging the Performance Gap in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "37 pages, 8 figures", "summary": "Recently, transfer learning and self-supervised learning have gained\nsignificant attention within the medical field due to their ability to mitigate\nthe challenges posed by limited data availability, improve model\ngeneralisation, and reduce computational expenses. Transfer learning and\nself-supervised learning hold immense potential for advancing medical research.\nHowever, it is crucial to recognise that transfer learning and self-supervised\nlearning architectures exhibit distinct advantages and limitations, manifesting\nvariations in accuracy, training speed, and robustness. This paper compares the\nperformance and robustness of transfer learning and self-supervised learning in\nthe medical field. Specifically, we pre-trained two models using the same\nsource domain datasets with different pre-training methods and evaluated them\non small-sized medical datasets to identify the factors influencing their final\nperformance. We tested data with several common issues in medical domains, such\nas data imbalance, data scarcity, and domain mismatch, through comparison\nexperiments to understand their impact on specific pre-trained models. Finally,\nwe provide recommendations to help users apply transfer learning and\nself-supervised learning methods in medical areas, and build more convenient\nand efficient deployment strategies."}
{"id": "2507.19483", "pdf": "https://arxiv.org/pdf/2507.19483", "abs": "https://arxiv.org/abs/2507.19483", "authors": ["Giuseppe Riva"], "title": "The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration", "categories": ["cs.HC", "cs.AI"], "comment": "39 Pages, no figures", "summary": "AI systems now function as cognitive extensions, evolving from tools to\nactive cognitive collaborators within human-AI integrated systems. While these\nsystems can amplify cognition - enhancing problem-solving, learning, and\ncreativity - they present a fundamental \"comfort-growth paradox\": AI's\nuser-friendly nature may foster intellectual stagnation by minimizing cognitive\nfriction necessary for development. As AI aligns with user preferences and\nprovides frictionless assistance, it risks inducing cognitive complacency\nrather than promoting growth. We introduce Enhanced Cognitive Scaffolding to\nresolve this paradox - reconceptualizing AI from convenient assistant to\ndynamic mentor. Drawing from Vygotskian theories, educational scaffolding\nprinciples, and AI ethics, our framework integrates three dimensions: (1)\nProgressive Autonomy, where AI support gradually fades as user competence\nincreases; (2) Adaptive Personalization, tailoring assistance to individual\nneeds and learning trajectories; and (3) Cognitive Load Optimization, balancing\nmental effort to maximize learning while minimizing unnecessary complexity.\nResearch across educational, workplace, creative, and healthcare domains\nsupports this approach, demonstrating accelerated skill acquisition, improved\nself-regulation, and enhanced higher-order thinking. The framework includes\nsafeguards against risks like dependency, skill atrophy, and bias\namplification. By prioritizing cognitive development over convenience in\nhuman-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward\ngenuinely amplified cognition while safeguarding autonomous thought and\ncontinuous learning."}
{"id": "2507.19485", "pdf": "https://arxiv.org/pdf/2507.19485", "abs": "https://arxiv.org/abs/2507.19485", "authors": ["Alayt Issak"], "title": "Creativity as a Human Right: Design Considerations for Computational Creativity Systems", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "We investigate creativity that is underlined in the Universal Declaration of\nHuman Rights (UDHR) to present design considerations for Computational\nCreativity (CC) systems. We find this declaration to describe creativity in\nsalient aspects and bring to light creativity as a Human Right attributed to\nthe Fourth Generation of such rights. This generation of rights attributes CC\nsystems and the evolving nature of interaction with entities of shared\nintelligence. Our methodology examines five of thirty articles from the UDHR\nand demonstrates each article with actualizations concluding with design\nconsiderations for each. We contribute our findings to ground the relationship\nbetween creativity and CC systems."}
{"id": "2507.19486", "pdf": "https://arxiv.org/pdf/2507.19486", "abs": "https://arxiv.org/abs/2507.19486", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Jinu Nyachhyon", "Mridul Sharma", "Callum Canavan", "Dylan Epstein-Gross", "Muhammed Abdulbari"], "title": "Confirmation bias: A challenge for scalable oversight", "categories": ["cs.HC", "cs.AI"], "comment": "61 pages, 8 figures", "summary": "Scalable oversight protocols aim to empower evaluators to accurately verify\nAI models more capable than themselves. However, human evaluators are subject\nto biases that can lead to systematic errors. We conduct two studies examining\nthe performance of simple oversight protocols where evaluators know that the\nmodel is \"correct most of the time, but not all of the time\". We find no\noverall advantage for the tested protocols, although in Study 1, showing\narguments in favor of both answers improves accuracy in cases where the model\nis incorrect. In Study 2, participants in both groups become more confident in\nthe system's answers after conducting online research, even when those answers\nare incorrect. We also reanalyze data from prior work that was more optimistic\nabout simple protocols, finding that human evaluators possessing knowledge\nabsent from models likely contributed to their positive results--an advantage\nthat diminishes as models continue to scale in capability. These findings\nunderscore the importance of testing the degree to which oversight protocols\nare robust to evaluator biases, whether they outperform simple deference to the\nmodel under evaluation, and whether their performance scales with increasing\nproblem difficulty and model capability."}
{"id": "2507.19487", "pdf": "https://arxiv.org/pdf/2507.19487", "abs": "https://arxiv.org/abs/2507.19487", "authors": ["Margarita Leib", "Nils Köbis", "Ivan Soraperra"], "title": "Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "People increasingly rely on AI-advice when making decisions. At times, such\nadvice can promote selfish behavior. When individuals abide by\nselfishness-promoting AI advice, how are they perceived and punished? To study\nthis question, we build on theories from social psychology and combine\nmachine-behavior and behavioral economic approaches. In a pre-registered,\nfinancially-incentivized experiment, evaluators could punish real\ndecision-makers who (i) received AI, human, or no advice. The advice (ii)\nencouraged selfish or prosocial behavior, and decision-makers (iii) behaved\nselfishly or, in a control condition, behaved prosocially. Evaluators further\nassigned responsibility to decision-makers and their advisors. Results revealed\nthat (i) prosocial behavior was punished very little, whereas selfish behavior\nwas punished much more. Focusing on selfish behavior, (ii) compared to\nreceiving no advice, selfish behavior was penalized more harshly after\nprosocial advice and more leniently after selfish advice. Lastly, (iii) whereas\nselfish decision-makers were seen as more responsible when they followed AI\ncompared to human advice, punishment between the two advice sources did not\nvary. Overall, behavior and advice content shape punishment, whereas the advice\nsource does not."}
{"id": "2507.19492", "pdf": "https://arxiv.org/pdf/2507.19492", "abs": "https://arxiv.org/abs/2507.19492", "authors": ["Jovana Kondic", "Pengyuan Li", "Dhiraj Joshi", "Zexue He", "Shafiq Abedin", "Jennifer Sun", "Ben Wiesel", "Eli Schwartz", "Ahmed Nassar", "Bo Wu", "Assaf Arbelle", "Aude Oliva", "Dan Gutfreund", "Leonid Karlinsky", "Rogerio Feris"], "title": "ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Chart-to-code reconstruction -- the task of recovering executable plotting\nscripts from chart images -- provides important insights into a model's ability\nto ground data visualizations in precise, machine-readable form. Yet many\nexisting multimodal benchmarks largely focus primarily on answering questions\nabout charts or summarizing them. To bridge this gap, we present ChartGen, a\nfully-automated pipeline for code-guided synthetic chart generation. Starting\nfrom seed chart images, ChartGen (i) prompts a vision-language model (VLM) to\nreconstruct each image into a python script, and (ii) iteratively augments that\nscript with a code-oriented large language model (LLM). Using ChartGen, we\ncreate 222.5K unique chart-image code pairs from 13K seed chart images, and\npresent an open-source synthetic chart dataset covering 27 chart types, 11\nplotting libraries, and multiple data modalities (image, code, text, CSV,\nDocTags). From this corpus, we curate a held-out chart-to-code evaluation\nsubset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -\n26B parameters), highlighting substantial room for progress. We release the\npipeline, prompts, and the dataset to help accelerate efforts towards robust\nchart understanding and vision-conditioned code generation:\nhttps://github.com/SD122025/ChartGen/"}
{"id": "2507.19495", "pdf": "https://arxiv.org/pdf/2507.19495", "abs": "https://arxiv.org/abs/2507.19495", "authors": ["Qing Dong", "Pengyuan Liu", "Dong Yu", "Chen Kang"], "title": "Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative agents have made significant progress in simulating human\nbehavior, but existing frameworks often simplify emotional modeling and focus\nprimarily on specific tasks, limiting the authenticity of the simulation. Our\nwork proposes the Psychological-mechanism Agent (PSYA) framework, based on the\nCognitive Triangle (Feeling-Thought-Action), designed to more accurately\nsimulate human behavior. The PSYA consists of three core modules: the Feeling\nmodule (using a layer model of affect to simulate changes in short-term,\nmedium-term, and long-term emotions), the Thought module (based on the Triple\nNetwork Model to support goal-directed and spontaneous thinking), and the\nAction module (optimizing agent behavior through the integration of emotions,\nneeds and plans). To evaluate the framework's effectiveness, we conducted daily\nlife simulations and extended the evaluation metrics to self-influence,\none-influence, and group-influence, selection five classic psychological\nexperiments for simulation. The results show that the PSYA framework generates\nmore natural, consistent, diverse, and credible behaviors, successfully\nreplicating human experimental outcomes. Our work provides a richer and more\naccurate emotional and cognitive modeling approach for generative agents and\noffers an alternative to human participants in psychological experiments."}
{"id": "2507.19497", "pdf": "https://arxiv.org/pdf/2507.19497", "abs": "https://arxiv.org/abs/2507.19497", "authors": ["Alex Leitch", "Celia Chen"], "title": "Unlimited Editions: Documenting Human Style in AI Art Generation", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR"], "comment": "alt.CHI 2025", "summary": "As AI art generation becomes increasingly sophisticated, HCI research has\nfocused primarily on questions of detection, authenticity, and automation. This\npaper argues that such approaches fundamentally misunderstand how artistic\nvalue emerges from the concerns that drive human image production. Through\nexamination of historical precedents, we demonstrate that artistic style is not\nonly visual appearance but the resolution of creative struggle, as artists\nwrestle with influence and technical constraints to develop unique ways of\nseeing. Current AI systems flatten these human choices into reproducible\npatterns without preserving their provenance. We propose that HCI's role lies\nnot only in perfecting visual output, but in developing means to document the\norigins and evolution of artistic style as it appears within generated visual\ntraces. This reframing suggests new technical directions for HCI research in\ngenerative AI, focused on automatic documentation of stylistic lineage and\ncreative choice rather than simple reproduction of aesthetic effects."}
{"id": "2507.19498", "pdf": "https://arxiv.org/pdf/2507.19498", "abs": "https://arxiv.org/abs/2507.19498", "authors": ["Yue Wu", "Xiaolan Chen", "Weiyi Zhang", "Shunming Liu", "Wing Man Rita Sum", "Xinyuan Wu", "Xianwen Shang", "Chea-su Kee", "Mingguang He", "Danli Shi"], "title": "ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings", "categories": ["cs.HC", "cs.AI"], "comment": "35 pages, 4 figures, 1 table", "summary": "Large language models (LLMs) show promise for tailored healthcare\ncommunication but face challenges in interpretability and multi-task\nintegration particularly for domain-specific needs like myopia, and their\nreal-world effectiveness as patient education tools has yet to be demonstrated.\nHere, we introduce ChatMyopia, an LLM-based AI agent designed to address text\nand image-based inquiries related to myopia. To achieve this, ChatMyopia\nintegrates an image classification tool and a retrieval-augmented knowledge\nbase built from literature, expert consensus, and clinical guidelines. Myopic\nmaculopathy grading task, single question examination and human evaluations\nvalidated its ability to deliver personalized, accurate, and safe responses to\nmyopia-related inquiries with high scalability and interpretability. In a\nrandomized controlled trial (n=70, NCT06607822), ChatMyopia significantly\nimproved patient satisfaction compared to traditional leaflets, enhancing\npatient education in accuracy, empathy, disease awareness, and patient-eyecare\npractitioner communication. These findings highlight ChatMyopia's potential as\na valuable supplement to enhance patient education and improve satisfaction\nwith medical services in primary eye care settings."}
{"id": "2507.19500", "pdf": "https://arxiv.org/pdf/2507.19500", "abs": "https://arxiv.org/abs/2507.19500", "authors": ["Omkar Suresh Hatti"], "title": "Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence provides an opportunity to\ncreate psychological spaciousness in society. Spaciousness is defined as the\nability to hold diverse interpersonal interactions and forms the basis for\nvulnerability that leads to authenticity that leads to prosocial behaviors and\nthus to societal harmony. This paper demonstrates an attempt to quantify, the\nhuman conditioning to subconsciously modify authentic self-expression to fit\nthe norms of the dominant culture. Gaze is explored across various marginalized\nand intersectional groups, using concepts from postmodern philosophy and\npsychology. The effects of gaze are studied through analyzing a few redacted\nReddit posts, only to be discussed in discourse and not endorsement. A\nmathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite\nMetric is presented to model the analysis of two sets of conversational spaces\nin relation to one another. The outcome includes an equation to train Large\nLanguage Models (LLMs) - the working mechanism of AI products such as Chat-GPT;\nand an argument for affirming and inclusive HCI, based on the equation, is\npresented. The argument is supported by a few principles of Neuro-plasticity,\nThe brain's lifelong capacity to rewire."}
{"id": "2507.19510", "pdf": "https://arxiv.org/pdf/2507.19510", "abs": "https://arxiv.org/abs/2507.19510", "authors": ["Haoxuan Ma", "Xishun Liao", "Yifan Liu", "Chris Stanford", "Jiaqi Ma"], "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning."}
{"id": "2507.19513", "pdf": "https://arxiv.org/pdf/2507.19513", "abs": "https://arxiv.org/abs/2507.19513", "authors": ["Khalid Ali", "Zineddine Bettouche", "Andreas Kassler", "Andreas Fischer"], "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource\nmanagement in 5G and beyond. However, conventional AI approaches often fail to\ncapture the intricate spatial and temporal patterns that exist, due to e.g.,\nthe mobility of users. We introduce a lightweight, dual-path Spatiotemporal\nNetwork that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling\nand a three-layer Conv3D module for spatial feature extraction. A fusion layer\nintegrates both streams into a cohesive representation, enabling robust\nforecasting. Our design improves gradient stability and convergence speed while\nreducing prediction error. Evaluations on real-world datasets show superior\nforecast performance over ConvLSTM baselines and strong generalization to\nunseen regions, making it well-suited for large-scale, next-generation network\ndeployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,\nwith a 30% improvement in model generalization."}
{"id": "2507.19517", "pdf": "https://arxiv.org/pdf/2507.19517", "abs": "https://arxiv.org/abs/2507.19517", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Ben Beck"], "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted for publication in the Proceedings of\n  the $28^{th}$ IEEE International Conference on Intelligent Transportation\n  Systems (ITSC 2025). This is the author's version of the work", "summary": "Accurate link-level bicycle volume estimation is essential for informed urban\nand transport planning but it is challenged by extremely sparse count data in\nurban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task\nframework augmenting a Hybrid Graph Neural Network (GNN) with Variational\nAutoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing\nsparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model\nintricate spatial relationships in sparse networks while VAE generates\nsynthetic nodes and edges to enrich the graph structure and enhance the\nestimation performance. BikeVAE-GNN simultaneously performs - regression for\nbicycling volume estimation and classification for bicycling traffic level\ncategorization. We demonstrate the effectiveness of BikeVAE-GNN using\nOpenStreetMap data and publicly available bicycle count data within the City of\nMelbourne - where only 141 of 15,933 road segments have labeled counts\n(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN\noutperforms machine learning and baseline GNN models, achieving a mean absolute\nerror (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.\nAblation studies further validate the effective role of Hybrid-GNN and VAE\ncomponents. Our research advances bicycling volume estimation in sparse\nnetworks using novel and state-of-the-art approaches, providing insights for\nsustainable bicycling infrastructures."}
{"id": "2507.19518", "pdf": "https://arxiv.org/pdf/2507.19518", "abs": "https://arxiv.org/abs/2507.19518", "authors": ["Sangwoo Seo", "Jimin Seo", "Yoonho Lee", "Donghyeon Kim", "Hyejin Shin", "Banghyun Sung", "Chanyoung Park"], "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "ICCAD 2025", "summary": "Subgraph matching plays an important role in electronic design automation\n(EDA) and circuit verification. Traditional rule-based methods have limitations\nin generalizing to arbitrary target circuits. Furthermore, node-to-node\nmatching approaches tend to be computationally inefficient, particularly for\nlarge-scale circuits. Deep learning methods have emerged as a potential\nsolution to address these challenges, but existing models fail to efficiently\ncapture global subgraph embeddings or rely on inefficient matching matrices,\nwhich limits their effectiveness for large circuits. In this paper, we propose\nan efficient graph matching approach that utilizes Graph Neural Networks (GNNs)\nto predict regions of high probability for containing the target circuit.\nSpecifically, we construct various negative samples to enable GNNs to\naccurately learn the presence of target circuits and develop an approach to\ndirectly extracting subgraph embeddings from the entire circuit, which captures\nglobal subgraph information and addresses the inefficiency of applying GNNs to\nall candidate subgraphs. Extensive experiments demonstrate that our approach\nsignificantly outperforms existing methods in terms of time efficiency and\ntarget region prediction, offering a scalable and effective solution for\nsubgraph matching in large-scale circuits."}
{"id": "2507.19519", "pdf": "https://arxiv.org/pdf/2507.19519", "abs": "https://arxiv.org/abs/2507.19519", "authors": ["J. Poole", "P. Gardner", "A. J. Hughes", "N. Dervilis", "R. S. Mills", "T. A. Dardeno", "K. Worden"], "title": "Physics-informed transfer learning for SHM via feature selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data used for training structural health monitoring (SHM) systems are\nexpensive and often impractical to obtain, particularly labelled data.\nPopulation-based SHM presents a potential solution to this issue by considering\nthe available data across a population of structures. However, differences\nbetween structures will mean the training and testing distributions will\ndiffer; thus, conventional machine learning methods cannot be expected to\ngeneralise between structures. To address this issue, transfer learning (TL),\ncan be used to leverage information across related domains. An important\nconsideration is that the lack of labels in the target domain limits data-based\nmetrics to quantifying the discrepancy between the marginal distributions.\nThus, a prerequisite for the application of typical unsupervised TL methods is\nto identify suitable source structures (domains), and a set of features, for\nwhich the conditional distributions are related to the target structure.\nGenerally, the selection of domains and features is reliant on domain\nexpertise; however, for complex mechanisms, such as the influence of damage on\nthe dynamic response of a structure, this task is not trivial. In this paper,\nknowledge of physics is leveraged to select more similar features, the modal\nassurance criterion (MAC) is used to quantify the correspondence between the\nmodes of healthy structures. The MAC is shown to have high correspondence with\na supervised metric that measures joint-distribution similarity, which is the\nprimary indicator of whether a classifier will generalise between domains. The\nMAC is proposed as a measure for selecting a set of features that behave\nconsistently across domains when subjected to damage, i.e. features with\ninvariance in the conditional distributions. This approach is demonstrated on\nnumerical and experimental case studies to verify its effectiveness in various\napplications."}
{"id": "2507.19520", "pdf": "https://arxiv.org/pdf/2507.19520", "abs": "https://arxiv.org/abs/2507.19520", "authors": ["Ethan Lo", "Dan C. Lo"], "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.AI"], "comment": null, "summary": "With manual searching processes, the rate at which scientists and astronomers\ndiscover exoplanets is slow because of inefficiencies that require an extensive\ntime of laborious inspections. In fact, as of now there have been about only\n5,000 confirmed exoplanets since the late 1900s. Recently, machine learning\n(ML) has proven to be extremely valuable and efficient in various fields,\ncapable of processing massive amounts of data in addition to increasing its\naccuracy by learning. Though ML models for discovering exoplanets owned by\nlarge corporations (e.g. NASA) exist already, they largely depend on complex\nalgorithms and supercomputers. In an effort to reduce such complexities, in\nthis paper, we report the results and potential benefits of various, well-known\nML models in the discovery and validation of extrasolar planets. The ML models\nthat are examined in this study include logistic regression, k-nearest\nneighbors, and random forest. The dataset on which the models train and predict\nis acquired from NASA's Kepler space telescope. The initial results show\npromising scores for each model. However, potential biases and dataset\nimbalances necessitate the use of data augmentation techniques to further\nensure fairer predictions and improved generalization. This study concludes\nthat, in the context of searching for exoplanets, data augmentation techniques\nsignificantly improve the recall and precision, while the accuracy varies for\neach model."}
{"id": "2507.19523", "pdf": "https://arxiv.org/pdf/2507.19523", "abs": "https://arxiv.org/abs/2507.19523", "authors": ["Xingyu Su", "Xiner Li", "Yuchao Lin", "Ziqian Xie", "Degui Zhi", "Shuiwang Ji"], "title": "Language Models for Controllable DNA Sequence Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We consider controllable DNA sequence design, where sequences are generated\nby conditioning on specific biological properties. While language models (LMs)\nsuch as GPT and BERT have achieved remarkable success in natural language\ngeneration, their application to DNA sequence generation remains largely\nunderexplored. In this work, we introduce ATGC-Gen, an Automated Transformer\nGenerator for Controllable Generation, which leverages cross-modal encoding to\nintegrate diverse biological signals. ATGC-Gen is instantiated with both\ndecoder-only and encoder-only transformer architectures, allowing flexible\ntraining and generation under either autoregressive or masked recovery\nobjectives. We evaluate ATGC-Gen on representative tasks including promoter and\nenhancer sequence design, and further introduce a new dataset based on ChIP-Seq\nexperiments for modeling protein binding specificity. Our experiments\ndemonstrate that ATGC-Gen can generate fluent, diverse, and biologically\nrelevant sequences aligned with the desired properties. Compared to prior\nmethods, our model achieves notable improvements in controllability and\nfunctional relevance, highlighting the potential of language models in\nadvancing programmable genomic design. The source code is released at\n(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen)."}
{"id": "2507.19525", "pdf": "https://arxiv.org/pdf/2507.19525", "abs": "https://arxiv.org/abs/2507.19525", "authors": ["Chenchen Zhao", "Zhengyuan Shi", "Xiangyu Wen", "Chengjie Liu", "Yi Liu", "Yunhao Zhou", "Yuxiang Zhao", "Hefei Feng", "Yinan Zhu", "Gwok-Waa Wan", "Xin Cheng", "Weiyu Chen", "Yongqi Fu", "Chujie Chen", "Chenhao Xue", "Guangyu Sun", "Ying Wang", "Yibo Lin", "Jun Yang", "Ning Xu", "Xi Wang", "Qiang Xu"], "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 1 figure, 5 tables. To appear in ICCAD 2025", "summary": "The emergence of multimodal large language models (MLLMs) presents promising\nopportunities for automation and enhancement in Electronic Design Automation\n(EDA). However, comprehensively evaluating these models in circuit design\nremains challenging due to the narrow scope of existing benchmarks. To bridge\nthis gap, we introduce MMCircuitEval, the first multimodal benchmark\nspecifically designed to assess MLLM performance comprehensively across diverse\nEDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer\n(QA) pairs spanning digital and analog circuits across critical EDA stages -\nranging from general knowledge and specifications to front-end and back-end\ndesign. Derived from textbooks, technical question banks, datasheets, and\nreal-world documentation, each QA pair undergoes rigorous expert review for\naccuracy and relevance. Our benchmark uniquely categorizes questions by design\nstage, circuit type, tested abilities (knowledge, comprehension, reasoning,\ncomputation), and difficulty level, enabling detailed analysis of model\ncapabilities and limitations. Extensive evaluations reveal significant\nperformance gaps among existing LLMs, particularly in back-end design and\ncomplex computations, highlighting the critical need for targeted training\ndatasets and modeling approaches. MMCircuitEval provides a foundational\nresource for advancing MLLMs in EDA, facilitating their integration into\nreal-world circuit design workflows. Our benchmark is available at\nhttps://github.com/cure-lab/MMCircuitEval."}
{"id": "2507.19526", "pdf": "https://arxiv.org/pdf/2507.19526", "abs": "https://arxiv.org/abs/2507.19526", "authors": ["Jianyuan Bo", "Hao Wu", "Yuan Fang"], "title": "Quantizing Text-attributed Graphs for Semantic-Structural Integration", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at KDD'2025", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation for\nmodeling complex relationships across diverse domains. With the rise of large\nlanguage models (LLMs), there is growing interest in leveraging their\ncapabilities for graph learning. However, current approaches face significant\nchallenges in embedding structural information into LLM-compatible formats,\nrequiring either computationally expensive alignment mechanisms or manual graph\nverbalization techniques that often lose critical structural details. Moreover,\nthese methods typically require labeled data from source domains for effective\ntransfer learning, significantly constraining their adaptability. We propose\nSTAG, a novel self-supervised framework that directly quantizes graph\nstructural information into discrete tokens using a frozen codebook. Unlike\ntraditional quantization approaches, our method employs soft assignment and KL\ndivergence guided quantization to address the unique challenges of graph data,\nwhich lacks natural tokenization structures. Our framework enables both\nLLM-based and traditional learning approaches, supporting true zero-shot\ntransfer learning without requiring labeled data even in the source domain.\nExtensive experiments demonstrate state-of-the-art performance across multiple\nnode classification benchmarks while maintaining compatibility with different\nLLM architectures, offering an elegant solution to bridging graph learning with\nLLMs."}
{"id": "2507.19529", "pdf": "https://arxiv.org/pdf/2507.19529", "abs": "https://arxiv.org/abs/2507.19529", "authors": ["Obumneme Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As green hydrogen emerges as a major component of global decarbonisation,\nOman has positioned itself strategically through national auctions and\ninternational partnerships. Following two successful green hydrogen project\nrounds, the country launched its third auction (R3) in the Duqm region. While\nthis area exhibits relative geospatial homogeneity, it is still vulnerable to\nenvironmental fluctuations that pose inherent risks to productivity. Despite\ngrowing global investment in green hydrogen, operational data remains scarce,\nwith major projects like Saudi Arabia's NEOM facility not expected to commence\nproduction until 2026, and Oman's ACME Duqm project scheduled for 2028. This\nabsence of historical maintenance and performance data from large-scale\nhydrogen facilities in desert environments creates a major knowledge gap for\naccurate risk assessment for infrastructure planning and auction decisions.\nGiven this data void, environmental conditions emerge as accessible and\nreliable proxy for predicting infrastructure maintenance pressures, because\nharsh desert conditions such as dust storms, extreme temperatures, and humidity\nfluctuations are well-documented drivers of equipment degradation in renewable\nenergy systems. To address this challenge, this paper proposes an Artificial\nIntelligence decision support system that leverages publicly available\nmeteorological data to develop a predictive Maintenance Pressure Index (MPI),\nwhich predicts risk levels and future maintenance demands on hydrogen\ninfrastructure. This tool strengthens regulatory foresight and operational\ndecision-making by enabling temporal benchmarking to assess and validate\nperformance claims over time. It can be used to incorporate temporal risk\nintelligence into auction evaluation criteria despite the absence of historical\noperational benchmarks."}
{"id": "2507.19530", "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr."}
{"id": "2507.19534", "pdf": "https://arxiv.org/pdf/2507.19534", "abs": "https://arxiv.org/abs/2507.19534", "authors": ["Ali Shakeri", "Wei Emma Zhang", "Amin Beheshti", "Weitong Chen", "Jian Yang", "Lishan Yang"], "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2; I.7"], "comment": "12 pages; Published to PAKDD'2025", "summary": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork."}
{"id": "2507.19536", "pdf": "https://arxiv.org/pdf/2507.19536", "abs": "https://arxiv.org/abs/2507.19536", "authors": ["K. -C. Ouyang", "S. -Y. Zhang", "S. -L. Liu", "J. Tian", "Y. -H. Li", "H. Tong", "H. -Y. Bai", "W. -H. Wang", "Y. -C. Hu"], "title": "Graph Learning Metallic Glass Discovery from Wikipedia", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.AI"], "comment": "7 figures", "summary": "Synthesizing new materials efficiently is highly demanded in various research\nfields. However, this process is usually slow and expensive, especially for\nmetallic glasses, whose formation strongly depends on the optimal combinations\nof multiple elements to resist crystallization. This constraint renders only\nseveral thousands of candidates explored in the vast material space since 1960.\nRecently, data-driven approaches armed by advanced machine learning techniques\nprovided alternative routes for intelligent materials design. Due to data\nscarcity and immature material encoding, the conventional tabular data is\nusually mined by statistical learning algorithms, giving limited model\npredictability and generalizability. Here, we propose sophisticated data\nlearning from material network representations. The node elements are encoded\nfrom the Wikipedia by a language model. Graph neural networks with versatile\narchitectures are designed to serve as recommendation systems to explore hidden\nrelationships among materials. By employing Wikipedia embeddings from different\nlanguages, we assess the capability of natural languages in materials design.\nOur study proposes a new paradigm to harvesting new amorphous materials and\nbeyond with artificial intelligence."}
{"id": "2507.19539", "pdf": "https://arxiv.org/pdf/2507.19539", "abs": "https://arxiv.org/abs/2507.19539", "authors": ["Khurram Javed", "Richard S. Sutton"], "title": "Swift-Sarsa: Fast and Robust Linear Control", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Presented at RLDM 2025", "summary": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features."}
{"id": "2507.19548", "pdf": "https://arxiv.org/pdf/2507.19548", "abs": "https://arxiv.org/abs/2507.19548", "authors": ["André Steingrüber", "Kevin Baum"], "title": "Justifications for Democratizing AI Alignment and Their Prospects", "categories": ["cs.CY", "cs.AI"], "comment": "accepted for the LNCS on-site proceedings of the AISoLA 2025\n  conference", "summary": "The AI alignment problem comprises both technical and normative dimensions.\nWhile technical solutions focus on implementing normative constraints in AI\nsystems, the normative problem concerns determining what these constraints\nshould be. This paper examines justifications for democratic approaches to the\nnormative problem -- where affected stakeholders determine AI alignment -- as\nopposed to epistocratic approaches that defer to normative experts. We analyze\nboth instrumental justifications (democratic approaches produce better\noutcomes) and non-instrumental justifications (democratic approaches prevent\nillegitimate authority or coercion). We argue that normative and metanormative\nuncertainty create a justificatory gap that democratic approaches aim to fill\nthrough political rather than theoretical justification. However, we identify\nsignificant challenges for democratic approaches, particularly regarding the\nprevention of illegitimate coercion through AI alignment. Our analysis suggests\nthat neither purely epistocratic nor purely democratic approaches may be\nsufficient on their own, pointing toward hybrid frameworks that combine expert\njudgment with participatory input alongside institutional safeguards against AI\nmonopolization."}
{"id": "2507.19549", "pdf": "https://arxiv.org/pdf/2507.19549", "abs": "https://arxiv.org/abs/2507.19549", "authors": ["Nadeen Fathallah", "Daniel Hernández", "Steffen Staab"], "title": "AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The vast majority of Web pages fail to comply with established Web\naccessibility guidelines, excluding a range of users with diverse abilities\nfrom interacting with their content. Making Web pages accessible to all users\nrequires dedicated expertise and additional manual efforts from Web page\nproviders. To lower their efforts and promote inclusiveness, we aim to\nautomatically detect and correct Web accessibility violations in HTML code.\nWhile previous work has made progress in detecting certain types of\naccessibility violations, the problem of automatically detecting and correcting\naccessibility violations remains an open challenge that we address. We\nintroduce a novel taxonomy classifying Web accessibility violations into three\nkey categories - Syntactic, Semantic, and Layout. This taxonomy provides a\nstructured foundation for developing our detection and correction method and\nredefining evaluation metrics. We propose a novel method, AccessGuru, which\ncombines existing accessibility testing tools and Large Language Models (LLMs)\nto detect violations and applies taxonomy-driven prompting strategies to\ncorrect all three categories. To evaluate these capabilities, we develop a\nbenchmark of real-world Web accessibility violations. Our benchmark quantifies\nsyntactic and layout compliance and judges semantic accuracy through\ncomparative analysis with human expert corrections. Evaluation against our\nbenchmark shows that AccessGuru achieves up to 84% average violation score\ndecrease, significantly outperforming prior methods that achieve at most 50%."}
{"id": "2507.19551", "pdf": "https://arxiv.org/pdf/2507.19551", "abs": "https://arxiv.org/abs/2507.19551", "authors": ["Ran Tong", "Songtao Wei", "Jiaqi Liu", "Lanruo Wang"], "title": "Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": "9 pages, 1 figure", "summary": "Hateful memes aimed at LGBTQ\\,+ communities often evade detection by tweaking\neither the caption, the image, or both. We build the first robustness benchmark\nfor this setting, pairing four realistic caption attacks with three canonical\nimage corruptions and testing all combinations on the PrideMM dataset. Two\nstate-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and\nwe introduce a lightweight \\textbf{Text Denoising Adapter (TDA)} to enhance the\nlatter's resilience. Across the grid, MemeCLIP degrades more gently, while\nMemeBLIP2 is particularly sensitive to the caption edits that disrupt its\nlanguage processing. However, the addition of the TDA not only remedies this\nweakness but makes MemeBLIP2 the most robust model overall. Ablations reveal\nthat all systems lean heavily on text, but architectural choices and\npre-training data significantly impact robustness. Our benchmark exposes where\ncurrent multimodal safety models crack and demonstrates that targeted,\nlightweight modules like the TDA offer a powerful path towards stronger\ndefences."}
{"id": "2507.19555", "pdf": "https://arxiv.org/pdf/2507.19555", "abs": "https://arxiv.org/abs/2507.19555", "authors": ["Rajat Khanda", "Mohammad Baqar", "Sambuddha Chakrabarti", "Satyasaran Changdar"], "title": "Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "Group Relative Policy Optimization (GRPO) has shown promise in discrete\naction spaces by eliminating value function dependencies through group-based\nadvantage estimation. However, its application to continuous control remains\nunexplored, limiting its utility in robotics where continuous actions are\nessential. This paper presents a theoretical framework extending GRPO to\ncontinuous control environments, addressing challenges in high-dimensional\naction spaces, sparse rewards, and temporal dynamics. Our approach introduces\ntrajectory-based policy clustering, state-aware advantage estimation, and\nregularized policy updates designed for robotic applications. We provide\ntheoretical analysis of convergence properties and computational complexity,\nestablishing a foundation for future empirical validation in robotic systems\nincluding locomotion and manipulation tasks."}
{"id": "2507.19556", "pdf": "https://arxiv.org/pdf/2507.19556", "abs": "https://arxiv.org/abs/2507.19556", "authors": ["Jialu Zhang", "Qingyang Sun", "Qianyi Wang", "Weiyi Zhang", "Zunjie Xiao", "Xiaoqing Zhang", "Jianfeng Ren", "Jiang Liu"], "title": "PEMUTA: Pedagogically-Enriched Multi-Granular Undergraduate Thesis Assessment", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The undergraduate thesis (UGTE) plays an indispensable role in assessing a\nstudent's cumulative academic development throughout their college years.\nAlthough large language models (LLMs) have advanced education intelligence,\nthey typically focus on holistic assessment with only one single evaluation\nscore, but ignore the intricate nuances across multifaceted criteria, limiting\ntheir ability to reflect structural criteria, pedagogical objectives, and\ndiverse academic competencies. Meanwhile, pedagogical theories have long\ninformed manual UGTE evaluation through multi-dimensional assessment of\ncognitive development, disciplinary thinking, and academic performance, yet\nremain underutilized in automated settings. Motivated by the research gap, we\npioneer PEMUTA, a pedagogically-enriched framework that effectively activates\ndomain-specific knowledge from LLMs for multi-granular UGTE assessment. Guided\nby Vygotsky's theory and Bloom's Taxonomy, PEMUTA incorporates a hierarchical\nprompting scheme that evaluates UGTEs across six fine-grained dimensions:\nStructure, Logic, Originality, Writing, Proficiency, and Rigor (SLOWPR),\nfollowed by holistic synthesis. Two in-context learning techniques, \\ie,\nfew-shot prompting and role-play prompting, are also incorporated to further\nenhance alignment with expert judgments without fine-tuning. We curate a\ndataset of authentic UGTEs with expert-provided SLOWPR-aligned annotations to\nsupport multi-granular UGTE assessment. Extensive experiments demonstrate that\nPEMUTA achieves strong alignment with expert evaluations, and exhibits strong\npotential for fine-grained, pedagogically-informed UGTE evaluations."}
{"id": "2507.19559", "pdf": "https://arxiv.org/pdf/2507.19559", "abs": "https://arxiv.org/abs/2507.19559", "authors": ["Gwendal Jouneaux", "Jordi Cabot"], "title": "Towards Sustainability Model Cards", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The growth of machine learning (ML) models and associated datasets triggers a\nconsequent dramatic increase in energy costs for the use and training of these\nmodels. In the current context of environmental awareness and global\nsustainability concerns involving ICT, Green AI is becoming an important\nresearch topic. Initiatives like the AI Energy Score Ratings are a good\nexample. Nevertheless, these benchmarking attempts are still to be integrated\nwith existing work on Quality Models and Service-Level Agreements common in\nother, more mature, ICT subfields. This limits the (automatic) analysis of this\nmodel energy descriptions and their use in (semi)automatic model comparison,\nselection, and certification processes. We aim to leverage the concept of\nquality models and merge it with existing ML model reporting initiatives and\nGreen/Frugal AI proposals to formalize a Sustainable Quality Model for AI/ML\nmodels. As a first step, we propose a new Domain-Specific Language to precisely\ndefine the sustainability aspects of an ML model (including the energy costs\nfor its different tasks). This information can then be exported as an extended\nversion of the well-known Model Cards initiative while, at the same time, being\nformal enough to be input of any other model description automatic process."}
{"id": "2507.19562", "pdf": "https://arxiv.org/pdf/2507.19562", "abs": "https://arxiv.org/abs/2507.19562", "authors": ["Abdul Basit", "Minghao Shao", "Muhammad Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "title": "PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation", "categories": ["quant-ph", "cs.AI", "68T50, 81P68, 68T07", "I.2.7; I.2.2"], "comment": "6 pages, 5 figures, 3 tables, paper accepted to QCE 2025", "summary": "The growing demand for robust quantum programming frameworks has unveiled a\ncritical limitation: current large language model (LLM) based quantum code\nassistants heavily rely on remote APIs, introducing challenges related to\nprivacy, latency, and excessive usage costs. Addressing this gap, we propose\nPennyCoder, a novel lightweight framework for quantum code generation,\nexplicitly designed for local and embedded deployment to enable on-device\nquantum programming assistance without external API dependence. PennyCoder\nleverages a fine-tuned version of the LLaMA 3.1-8B model, adapted through\nparameter-efficient Low-Rank Adaptation (LoRA) techniques combined with\ndomain-specific instruction tuning optimized for the specialized syntax and\ncomputational logic of quantum programming in PennyLane, including tasks in\nquantum machine learning and quantum reinforcement learning. Unlike prior work\nfocused on cloud-based quantum code generation, our approach emphasizes\ndevice-native operability while maintaining high model efficacy. We rigorously\nevaluated PennyCoder over a comprehensive quantum programming dataset,\nachieving 44.3% accuracy with our fine-tuned model (compared to 33.7% for the\nbase LLaMA 3.1-8B and 40.1% for the RAG-augmented baseline), demonstrating a\nsignificant improvement in functional correctness."}
{"id": "2507.19567", "pdf": "https://arxiv.org/pdf/2507.19567", "abs": "https://arxiv.org/abs/2507.19567", "authors": ["Elisha D. O. Roberson"], "title": "Differentiating hype from practical applications of large language models in medicine -- a primer for healthcare professionals", "categories": ["cs.CY", "cs.AI"], "comment": "7 pages main document text, 2 figures. A basic primer on the\n  potential and dangers of AI generally and LLMs specifically in the medical\n  care system. Targeted to *non-expert* healthcare workers without experience\n  in AI or LLMs", "summary": "The medical ecosystem consists of the training of new clinicians and\nresearchers, the practice of clinical medicine, and areas of adjacent research.\nThere are many aspects of these domains that could benefit from the application\nof task automation and programmatic assistance. Machine learning and artificial\nintelligence techniques, including large language models (LLMs), have been\npromised to deliver on healthcare innovation, improving care speed and\naccuracy, and reducing the burden on staff for manual interventions. However,\nLLMs have no understanding of objective truth that is based in reality. They\nalso represent real risks to the disclosure of protected information when used\nby clinicians and researchers. The use of AI in medicine in general, and the\ndeployment of LLMs in particular, therefore requires careful consideration and\nthoughtful application to reap the benefits of these technologies while\navoiding the dangers in each context."}
{"id": "2507.19568", "pdf": "https://arxiv.org/pdf/2507.19568", "abs": "https://arxiv.org/abs/2507.19568", "authors": ["You Wu", "Philip E. Bourne", "Lei Xie"], "title": "Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.LG"], "comment": "Under Review", "summary": "Artificial intelligence (AI) has sparked immense interest in drug discovery,\nbut most current approaches only digitize existing high-throughput experiments.\nThey remain constrained by conventional pipelines. As a result, they do not\naddress the fundamental challenges of predicting drug effects in humans.\nSimilarly, biomedical digital twins, largely grounded in real-world data and\nmechanistic models, are tailored for late-phase drug development and lack the\nresolution to model molecular interactions or their systemic consequences,\nlimiting their impact in early-stage discovery. This disconnect between early\ndiscovery and late development is one of the main drivers of high failure rates\nin drug discovery. The true promise of AI lies not in augmenting current\nexperiments but in enabling virtual experiments that are impossible in the real\nworld: testing novel compounds directly in silico in the human body. Recent\nadvances in AI, high-throughput perturbation assays, and single-cell and\nspatial omics across species now make it possible to construct programmable\nvirtual humans: dynamic, multiscale models that simulate drug actions from\nmolecular to phenotypic levels. By bridging the translational gap, programmable\nvirtual humans offer a transformative path to optimize therapeutic efficacy and\nsafety earlier than ever before. This perspective introduces the concept of\nprogrammable virtual humans, explores their roles in a new paradigm of drug\ndiscovery centered on human physiology, and outlines key opportunities,\nchallenges, and roadmaps for their realization."}
{"id": "2507.19586", "pdf": "https://arxiv.org/pdf/2507.19586", "abs": "https://arxiv.org/abs/2507.19586", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks."}
{"id": "2507.19595", "pdf": "https://arxiv.org/pdf/2507.19595", "abs": "https://arxiv.org/abs/2507.19595", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models."}
{"id": "2507.19598", "pdf": "https://arxiv.org/pdf/2507.19598", "abs": "https://arxiv.org/abs/2507.19598", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-Tür", "Ismini Lourentzou"], "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision."}
{"id": "2507.19629", "pdf": "https://arxiv.org/pdf/2507.19629", "abs": "https://arxiv.org/abs/2507.19629", "authors": ["Hsin-Yi Lin", "Samuel Yen-Chi Chen", "Huan-Hsin Tseng", "Shinjae Yoo"], "title": "Quantum Reinforcement Learning by Adaptive Non-local Observables", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "Accepted at IEEE Quantum Week 2025 (QCE 2025)", "summary": "Hybrid quantum-classical frameworks leverage quantum computing for machine\nlearning; however, variational quantum circuits (VQCs) are limited by the need\nfor local measurements. We introduce an adaptive non-local observable (ANO)\nparadigm within VQCs for quantum reinforcement learning (QRL), jointly\noptimizing circuit parameters and multi-qubit measurements. The ANO-VQC\narchitecture serves as the function approximator in Deep Q-Network (DQN) and\nAsynchronous Advantage Actor-Critic (A3C) algorithms. On multiple benchmark\ntasks, ANO-VQC agents outperform baseline VQCs. Ablation studies reveal that\nadaptive measurements enhance the function space without increasing circuit\ndepth. Our results demonstrate that adaptive multi-qubit observables can enable\npractical quantum advantages in reinforcement learning."}
{"id": "2507.19634", "pdf": "https://arxiv.org/pdf/2507.19634", "abs": "https://arxiv.org/abs/2507.19634", "authors": ["Sara Papi", "Maike Züfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development."}
{"id": "2507.19635", "pdf": "https://arxiv.org/pdf/2507.19635", "abs": "https://arxiv.org/abs/2507.19635", "authors": ["Zain Asgar", "Michelle Nguyen", "Sachin Katti"], "title": "Efficient and Scalable Agentic AI with Heterogeneous Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Early access preprint", "summary": "AI agents are emerging as a dominant workload in a wide range of\napplications, promising to be the vehicle that delivers the promised benefits\nof AI to enterprises and consumers. Unlike conventional software or static\ninference, agentic workloads are dynamic and structurally complex. Often these\nagents are directed graphs of compute and IO operations that span multi-modal\ndata input and conversion), data processing and context gathering (e.g vector\nDB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,\nwe need efficient and scalable deployment and agent-serving infrastructure.\n  To tackle this challenge, in this paper, we present a system design for\ndynamic orchestration of AI agent workloads on heterogeneous compute\ninfrastructure spanning CPUs and accelerators, both from different vendors and\nacross different performance tiers within a single vendor. The system delivers\nseveral building blocks: a framework for planning and optimizing agentic AI\nexecution graphs using cost models that account for compute, memory, and\nbandwidth constraints of different HW; a MLIR based representation and\ncompilation system that can decompose AI agent execution graphs into granular\noperators and generate code for different HW options; and a dynamic\norchestration system that can place the granular components across a\nheterogeneous compute infrastructure and stitch them together while meeting an\nend-to-end SLA. Our design performs a systems level TCO optimization and\npreliminary results show that leveraging a heterogeneous infrastructure can\ndeliver significant TCO benefits. A preliminary surprising finding is that for\nsome workloads a heterogeneous combination of older generation GPUs with newer\naccelerators can deliver similar TCO as the latest generation homogenous GPU\ninfrastructure design, potentially extending the life of deployed\ninfrastructure."}
{"id": "2507.19643", "pdf": "https://arxiv.org/pdf/2507.19643", "abs": "https://arxiv.org/abs/2507.19643", "authors": ["Minju Kim", "Dongje Yoo", "Yeonjun Hwang", "Minseok Kang", "Namyoung Kim", "Minju Gwak", "Beong-woo Kwak", "Hyungjoo Chae", "Harim Kim", "Yunjoong Lee", "Min Hee Kim", "Dayi Jung", "Kyong-Mee Chung", "Jinyoung Yeo"], "title": "Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation", "categories": ["cs.CY", "cs.AI"], "comment": "Published at ACL 2025 Findings", "summary": "Understanding clients' thoughts and beliefs is fundamental in counseling, yet\ncurrent evaluations of LLM therapists often fail to assess this ability.\nExisting evaluation methods rely on client simulators that clearly disclose\ninternal states to the therapist, making it difficult to determine whether an\nLLM therapist can uncover unexpressed perspectives. To address this limitation,\nwe introduce MindVoyager, a novel evaluation framework featuring a controllable\nand realistic client simulator which dynamically adapts itself based on the\nongoing counseling session, offering a more realistic and challenging\nevaluation environment. We further introduce evaluation metrics that assess the\nexploration ability of LLM therapists by measuring their thorough understanding\nof client's beliefs and thoughts."}
{"id": "2507.19653", "pdf": "https://arxiv.org/pdf/2507.19653", "abs": "https://arxiv.org/abs/2507.19653", "authors": ["Armen Manukyan", "Hrant Khachatrian", "Edvard Ghukasyan", "Theofanis P. Raptis"], "title": "On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication.\n  This work was supported by funding under the bilateral agreement between CNR\n  (Italy) and HESC MESCS RA (Armenia) as part of the DeepRF project for the\n  2025-2026 biennium, and by the HESC MESCS RA grant No. 22rl-052 (DISTAL)", "summary": "We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links\nin central Rome. We use a real measurement set of 1,664 user-equipments (UEs)\nand six nominal base-station (BS) sites. Using these fixed positions we\nsystematically vary the main simulation parameters, including path depth,\ndiffuse/specular/refraction flags, carrier frequency, as well as antenna's\nproperties like its altitude, radiation pattern, and orientation. Simulator\nfidelity is scored for each base station via Spearman correlation between\nmeasured and simulated powers, and by a fingerprint-based k-nearest-neighbor\nlocalization algorithm using RSSI-based fingerprints. Across all experiments,\nsolver hyper-parameters are having immaterial effect on the chosen metrics. On\nthe contrary, antenna locations and orientations prove decisive. By simple\ngreedy optimization we improve the Spearman correlation by 5% to 130% for\nvarious base stations, while kNN-based localization error using only simulated\ndata as reference points is decreased by one-third on real-world samples, while\nstaying twice higher than the error with purely real data. Precise geometry and\ncredible antenna models are therefore necessary but not sufficient; faithfully\ncapturing the residual urban noise remains an open challenge for transferable,\nhigh-fidelity outdoor RF simulation."}
{"id": "2507.19657", "pdf": "https://arxiv.org/pdf/2507.19657", "abs": "https://arxiv.org/abs/2507.19657", "authors": ["Beining Wu", "Jun Huang", "Shui Yu"], "title": "\"X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems", "categories": ["cs.NI", "cs.AI"], "comment": "48 pages, 14 figures, submitted to IEEE", "summary": "The development of next-generation networking systems has inherently shifted\nfrom throughput-based paradigms towards intelligent, information-aware designs\nthat emphasize the quality, relevance, and utility of transmitted information,\nrather than sheer data volume. While classical network metrics, such as latency\nand packet loss, remain significant, they are insufficient to quantify the\nnuanced information quality requirements of modern intelligent applications,\nincluding autonomous vehicles, digital twins, and metaverse environments. In\nthis survey, we present the first comprehensive study of the ``X of\nInformation'' continuum by introducing a systematic four-dimensional taxonomic\nframework that structures information metrics along temporal, quality/utility,\nreliability/robustness, and network/communication dimensions. We uncover the\nincreasing interdependencies among these dimensions, whereby temporal freshness\ntriggers quality evaluation, which in turn helps with reliability appraisal,\nultimately enabling effective network delivery. Our analysis reveals that\nartificial intelligence technologies, such as deep reinforcement learning,\nmulti-agent systems, and neural optimization models, enable adaptive,\ncontext-aware optimization of competing information quality objectives. In our\nextensive study of six critical application domains, covering autonomous\ntransportation, industrial IoT, healthcare digital twins, UAV communications,\nLLM ecosystems, and metaverse settings, we illustrate the revolutionary promise\nof multi-dimensional information metrics for meeting diverse operational needs.\nOur survey identifies prominent implementation challenges, including ..."}
{"id": "2507.19679", "pdf": "https://arxiv.org/pdf/2507.19679", "abs": "https://arxiv.org/abs/2507.19679", "authors": ["Mandar Kulkarni"], "title": "Efficient Learning for Product Attributes with Compact Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image-based product attribute prediction in e-commerce is a crucial task with\nnumerous applications. The supervised fine-tuning of Vision Language Models\n(VLMs) faces significant scale challenges due to the cost of manual or API\nbased annotation. In this paper, we investigate label-efficient semi-supervised\nfine-tuning strategies for compact VLMs (2B-3B parameters) that leverage\nunlabeled product listings through Direct Preference Optimization (DPO).\nBeginning with a small, API-based, annotated, and labeled set, we first employ\nPEFT to train low-rank adapter modules. To update the adapter weights with\nunlabeled data, we generate multiple reasoning-and-answer chains per unlabeled\nsample and segregate these chains into preferred and dispreferred based on\nself-consistency. We then fine-tune the model with DPO loss and use the updated\nmodel for the next iteration. By using PEFT fine-tuning with DPO, our method\nachieves efficient convergence with minimal compute overhead. On a dataset\nspanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes\nonly unlabeled data, demonstrates a significant improvement over the supervised\nmodel. Moreover, experiments demonstrate that accuracy with DPO training\nimproves with more unlabeled data, indicating that a large pool of unlabeled\nsamples can be effectively leveraged to improve performance."}
{"id": "2507.19682", "pdf": "https://arxiv.org/pdf/2507.19682", "abs": "https://arxiv.org/abs/2507.19682", "authors": ["Matthew Drexler", "Benjamin Risk", "James J Lah", "Suprateek Kundu", "Deqiang Qiu"], "title": "DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages, 10 figures", "summary": "Conventional multimodal data integration methods provide a comprehensive\nassessment of the shared or unique structure within each individual data type\nbut suffer from several limitations such as the inability to handle\nhigh-dimensional data and identify nonlinear structures. In this paper, we\nintroduce DeepJIVE, a deep-learning approach to performing Joint and Individual\nVariance Explained (JIVE). We perform mathematical derivation and experimental\nvalidations using both synthetic and real-world 1D, 2D, and 3D datasets.\nDifferent strategies of achieving the identity and orthogonality constraints\nfor DeepJIVE were explored, resulting in three viable loss functions. We found\nthat DeepJIVE can successfully uncover joint and individual variations of\nmultimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) also identified biologically plausible\ncovariation patterns between the amyloid positron emission tomography (PET) and\nmagnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a\nuseful tool for multimodal data analysis."}
{"id": "2507.19684", "pdf": "https://arxiv.org/pdf/2507.19684", "abs": "https://arxiv.org/abs/2507.19684", "authors": ["Bermet Burkanova", "Payam Jome Yazdian", "Chuxuan Zhang", "Trinity Evans", "Paige Tuttösí", "Angelica Lim"], "title": "Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "https://rosielab.github.io/compas3d", "summary": "Imagine a humanoid that can safely and creatively dance with a human,\nadapting to its partner's proficiency, using haptic signaling as a primary form\nof communication. While today's AI systems excel at text or voice-based\ninteraction with large language models, human communication extends far beyond\ntext-it includes embodied movement, timing, and physical coordination. Modeling\ncoupled interaction between two agents poses a formidable challenge: it is\ncontinuous, bidirectionally reactive, and shaped by individual variation. We\npresent CoMPAS3D, the largest and most diverse motion capture dataset of\nimprovised salsa dancing, designed as a challenging testbed for interactive,\nexpressive humanoid AI. The dataset includes 3 hours of leader-follower salsa\ndances performed by 18 dancers spanning beginner, intermediate, and\nprofessional skill levels. For the first time, we provide fine-grained salsa\nexpert annotations, covering over 2,800 move segments, including move types,\ncombinations, execution errors and stylistic elements. We draw analogies\nbetween partner dance communication and natural language, evaluating CoMPAS3D\non two benchmark tasks for synthetic humans that parallel key problems in\nspoken language and dialogue processing: leader or follower generation with\nproficiency levels (speaker or listener synthesis), and duet (conversation)\ngeneration. Towards a long-term goal of partner dance with humans, we release\nthe dataset, annotations, and code, along with a multitask SalsaAgent model\ncapable of performing all benchmark tasks, alongside additional baselines to\nencourage research in socially interactive embodied AI and creative, expressive\nhumanoid motion generation."}
{"id": "2507.19686", "pdf": "https://arxiv.org/pdf/2507.19686", "abs": "https://arxiv.org/abs/2507.19686", "authors": ["Robert Frenken", "Sidra Ghayour Bhatti", "Hanqin Zhang", "Qadeer Ahmed"], "title": "KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Controller Area Network (CAN) protocol is widely adopted for in-vehicle\ncommunication but lacks inherent security mechanisms, making it vulnerable to\ncyberattacks. This paper introduces KD-GAT, an intrusion detection framework\nthat combines Graph Attention Networks (GATs) with knowledge distillation (KD)\nto enhance detection accuracy while reducing computational complexity. In our\napproach, CAN traffic is represented as graphs using a sliding window to\ncapture temporal and relational patterns. A multi-layer GAT with jumping\nknowledge aggregation acting as the teacher model, while a compact student\nGAT--only 6.32% the size of the teacher--is trained via a two-phase process\ninvolving supervised pretraining and knowledge distillation with both soft and\nhard label supervision. Experiments on three benchmark datasets--Car-Hacking,\nCar-Survival, and can-train-and-test demonstrate that both teacher and student\nmodels achieve strong results, with the student model attaining 99.97% and\n99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,\nsignificant class imbalance in can-train-and-test has led to reduced\nperformance for both models on this dataset. Addressing this imbalance remains\nan important direction for future work."}
{"id": "2507.19694", "pdf": "https://arxiv.org/pdf/2507.19694", "abs": "https://arxiv.org/abs/2507.19694", "authors": ["Faruk Alpay", "Hamdi Alakkad", "Bugra Kilictas", "Taylan Alpay"], "title": "Ultracoarse Equilibria and Ordinal-Folding Dynamics in Operator-Algebraic Models of Infinite Multi-Agent Games", "categories": ["math.OC", "cs.AI", "cs.GT", "cs.MA", "91A26, 47L65, 03E10, 91B32"], "comment": "15 pages, 2 figures; companion implementation available at\n  https://github.com/farukalpay/ordinal-folding-index/", "summary": "We develop an operator algebraic framework for infinite games with a\ncontinuum of agents and prove that regret based learning dynamics governed by a\nnoncommutative continuity equation converge to a unique quantal response\nequilibrium under mild regularity assumptions. The framework unifies functional\nanalysis, coarse geometry and game theory by assigning to every game a von\nNeumann algebra that represents collective strategy evolution. A reflective\nregret operator within this algebra drives the flow of strategy distributions\nand its fixed point characterises equilibrium. We introduce the ordinal folding\nindex, a computable ordinal valued metric that measures the self referential\ndepth of the dynamics, and show that it bounds the transfinite time needed for\nconvergence, collapsing to zero on coarsely amenable networks. The theory\nyields new invariant subalgebra rigidity results, establishes existence and\nuniqueness of envy free and maximin share allocations in continuum economies,\nand links analytic properties of regret flows with empirical stability\nphenomena in large language models. These contributions supply a rigorous\nmathematical foundation for large scale multi agent systems and demonstrate the\nutility of ordinal metrics for equilibrium selection."}
{"id": "2507.19712", "pdf": "https://arxiv.org/pdf/2507.19712", "abs": "https://arxiv.org/abs/2507.19712", "authors": ["Ngoc Hung Nguyen", "Nguyen Van Thieu", "Quang-Trung Luu", "Anh Tuan Nguyen", "Senura Wanasekara", "Nguyen Cong Luong", "Fatemeh Kavehmadavani", "Van-Dinh Nguyen"], "title": "Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning", "categories": ["cs.DC", "cs.AI", "cs.GT", "cs.LG", "cs.NI"], "comment": "15 pages, 13 figures", "summary": "In this paper, we explore mission assignment and task offloading in an Open\nRadio Access Network (Open RAN)-based intelligent transportation system (ITS),\nwhere autonomous vehicles leverage mobile edge computing for efficient\nprocessing. Existing studies often overlook the intricate interdependencies\nbetween missions and the costs associated with offloading tasks to edge\nservers, leading to suboptimal decision-making. To bridge this gap, we\nintroduce Oranits, a novel system model that explicitly accounts for mission\ndependencies and offloading costs while optimizing performance through vehicle\ncooperation. To achieve this, we propose a twofold optimization approach.\nFirst, we develop a metaheuristic-based evolutionary computing algorithm,\nnamely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline\nfor one-slot optimization. Second, we design an enhanced reward-based deep\nreinforcement learning (DRL) framework, referred to as the Multi-agent Double\nDeep Q-Network (MA-DDQN), that integrates both multi-agent coordination and\nmulti-action selection mechanisms, significantly reducing mission assignment\ntime and improving adaptability over baseline methods. Extensive simulations\nreveal that CGG-ARO improves the number of completed missions and overall\nbenefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN\nachieves even greater improvements of 11.0% in terms of mission completions and\n12.5% in terms of the overall benefit. These results highlight the\neffectiveness of Oranits in enabling faster, more adaptive, and more efficient\ntask processing in dynamic ITS environments."}
{"id": "2507.19730", "pdf": "https://arxiv.org/pdf/2507.19730", "abs": "https://arxiv.org/abs/2507.19730", "authors": ["Liyang Wang", "Shiqian Wu", "Shun Fang", "Qile Zhu", "Jiaxin Wu", "Sos Again"], "title": "Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Moving target detection is a challenging computer vision task aimed at\ngenerating accurate segmentation maps in diverse in-the-wild color videos\ncaptured by static cameras. If backgrounds and targets can be simultaneously\nextracted and recombined, such synthetic data can significantly enrich\nannotated in-the-wild datasets and enhance the generalization ability of deep\nmodels. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for\ncolor image processing. However, in color video processing, Quaternion Singular\nValue Decomposition (QSVD) incurs high computational costs, and rank-1\nquaternion matrix fails to yield rank-1 color channels. In this paper, we\nreduce the computational complexity of QSVD to o(1) by utilizing a quaternion\nRiemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)\nframework, which achieves a balance in simultaneously segmenting targets and\nrecovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by\nintroducing the Color Rank-1 Batch (CR1B) method to further process and obtain\nthe ideal low-rank background across color channels. Experiments demonstrate\nour uQRPCA+ achieves State Of The Art (SOTA) performance on moving target\ndetection and background recovery tasks compared to existing open-source\nmethods. Our implementation is publicly available on GitHub at\nhttps://github.com/Ruchtech/uQRPCA"}
{"id": "2507.19737", "pdf": "https://arxiv.org/pdf/2507.19737", "abs": "https://arxiv.org/abs/2507.19737", "authors": ["Yinzhou Tang", "Huandong Wang", "Xiaochen Fan", "Yong Li"], "title": "Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "The vulnerability of cities to natural disasters has increased with\nurbanization and climate change, making it more important to predict human\nmobility in the disaster scenarios for downstream tasks including\nlocation-based early disaster warning and pre-allocating rescue resources, etc.\nHowever, existing human mobility prediction models are mainly designed for\nnormal scenarios, and fail to adapt to disaster scenarios due to the shift of\nhuman mobility patterns under disaster. To address this issue, we introduce\n\\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios\nthat can be integrated into existing deep mobility prediction methods by\nleveraging LLMs to model the mobility intention and transferring the common\nknowledge of how different disasters affect mobility intentions between cities.\nThis framework utilizes a RAG-Enhanced Intention Predictor to forecast the next\nintention, refines it with an LLM-based Intention Refiner, and then maps the\nintention to an exact location using an Intention-Modulated Location Predictor.\nExtensive experiments illustrate that DisasterMobLLM can achieve a 32.8\\%\nimprovement in terms of Acc@1 and a 35.0\\% improvement in terms of the F1-score\nof predicting immobility compared to the baselines. The code is available at\nhttps://github.com/tsinghua-fib-lab/DisasterMobLLM."}
{"id": "2507.19743", "pdf": "https://arxiv.org/pdf/2507.19743", "abs": "https://arxiv.org/abs/2507.19743", "authors": ["Zhuolin Xu", "Chenglin Li", "Qiushi Li", "Shin Hwei Tan"], "title": "Defining ethically sourced code generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Several code generation models have been proposed to help reduce time and\neffort in solving software-related tasks. To ensure responsible AI, there are\ngrowing interests over various ethical issues (e.g., unclear licensing,\nprivacy, fairness, and environment impact). These studies have the overarching\ngoal of ensuring ethically sourced generation, which has gained growing\nattentions in speech synthesis and image generation. In this paper, we\nintroduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to\nrefer to managing all processes involved in code generation model development\nfrom data collection to post-deployment via ethical and sustainable practices.\nTo build a taxonomy of ES-CodeGen, we perform a two-phase literature review\nwhere we read 803 papers across various domains and specific to AI-based code\ngeneration. We identified 71 relevant papers with 10 initial dimensions of\nES-CodeGen. To refine our dimensions and gain insights on consequences of\nES-CodeGen, we surveyed 32 practitioners, which include six developers who\nsubmitted GitHub issues to opt-out from the Stack dataset (these impacted users\nhave real-world experience of ethically sourcing issues in code generation\nmodels). The results lead to 11 dimensions of ES-CodeGen with a new dimension\non code quality as practitioners have noted its importance. We also identified\nconsequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey\nreflection showed that most practitioners tend to ignore social-related\ndimensions despite their importance. Most practitioners either agreed or\nstrongly agreed that our survey help improve their understanding of ES-CodeGen.\nOur study calls for attentions of various ethical issues towards ES-CodeGen."}
{"id": "2507.19755", "pdf": "https://arxiv.org/pdf/2507.19755", "abs": "https://arxiv.org/abs/2507.19755", "authors": ["Ziqi Zhang", "Shiheng Chen", "Runze Yang", "Zhisheng Wei", "Wei Zhang", "Lei Wang", "Zhanzhi Liu", "Fengshan Zhang", "Jing Wu", "Xiaoyong Pan", "Hongbin Shen", "Longbing Cao", "Zhaohong Deng"], "title": "Modeling enzyme temperature stability from sequence segment perspective", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": null, "summary": "Developing enzymes with desired thermal properties is crucial for a wide\nrange of industrial and research applications, and determining temperature\nstability is an essential step in this process. Experimental determination of\nthermal parameters is labor-intensive, time-consuming, and costly. Moreover,\nexisting computational approaches are often hindered by limited data\navailability and imbalanced distributions. To address these challenges, we\nintroduce a curated temperature stability dataset designed for model\ndevelopment and benchmarking in enzyme thermal modeling. Leveraging this\ndataset, we present the \\textit{Segment Transformer}, a novel deep learning\nframework that enables efficient and accurate prediction of enzyme temperature\nstability. The model achieves state-of-the-art performance with an RMSE of\n24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,\nrespectively. These results highlight the effectiveness of incorporating\nsegment-level representations, grounded in the biological observation that\ndifferent regions of a protein sequence contribute unequally to thermal\nbehavior. As a proof of concept, we applied the Segment Transformer to guide\nthe engineering of a cutinase enzyme. Experimental validation demonstrated a\n1.64-fold improvement in relative activity following heat treatment, achieved\nthrough only 17 mutations and without compromising catalytic function."}
{"id": "2507.19766", "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity."}
{"id": "2507.19771", "pdf": "https://arxiv.org/pdf/2507.19771", "abs": "https://arxiv.org/abs/2507.19771", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Structural drawings are widely used in many fields, e.g., mechanical\nengineering, civil engineering, etc. In civil engineering, structural drawings\nserve as the main communication tool between architects, engineers, and\nbuilders to avoid conflicts, act as legal documentation, and provide a\nreference for future maintenance or evaluation needs. They are often organized\nusing key elements such as title/subtitle blocks, scales, plan views, elevation\nview, sections, and detailed sections, which are annotated with standardized\nsymbols and line types for interpretation by engineers and contractors. Despite\nadvances in software capabilities, the task of generating a structural drawing\nremains labor-intensive and time-consuming for structural engineers. Here we\nintroduce a novel generative AI-based method for generating structural drawings\nemploying a large language model (LLM) agent. The method incorporates a\nretrieval-augmented generation (RAG) technique using externally-sourced facts\nto enhance the accuracy and reliability of the language model. This method is\ncapable of understanding varied natural language descriptions, processing these\nto extract necessary information, and generating code to produce the desired\nstructural drawing in AutoCAD. The approach developed, demonstrated and\nevaluated herein enables the efficient and direct conversion of a structural\ndrawing's natural language description into an AutoCAD drawing, significantly\nreducing the workload compared to current working process associated with\nmanual drawing production, facilitating the typical iterative process of\nengineers for expressing design ideas in a simplified way."}
{"id": "2507.19803", "pdf": "https://arxiv.org/pdf/2507.19803", "abs": "https://arxiv.org/abs/2507.19803", "authors": ["Saram Abbas", "Naeem Soomro", "Rishad Shafik", "Rakesh Heer", "Kabita Adhikari"], "title": "AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to ISTM 2025", "summary": "Bladder cancer claims one life every 3 minutes worldwide. Most patients are\ndiagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur\nafter treatment, triggering a relentless cycle of surgeries, monitoring, and\nrisk of progression. Clinical tools like the EORTC risk tables are outdated and\nunreliable - especially for intermediate-risk cases.\n  We propose an interpretable AI model using the Tsetlin Machine (TM), a\nsymbolic learner that outputs transparent, human-readable logic. Tested on the\nPHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming\nXGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the\nexact clauses behind each prediction, grounded in clinical features like tumour\ncount, surgeon experience, and hospital stay - offering accuracy and full\ntransparency. This makes TM a powerful, trustworthy decision-support tool ready\nfor real-world adoption."}
{"id": "2507.19806", "pdf": "https://arxiv.org/pdf/2507.19806", "abs": "https://arxiv.org/abs/2507.19806", "authors": ["Xinlong Zhao", "Tong Jia", "Minghua He", "Yihan Wu", "Ying Li", "Gang Huang"], "title": "From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages, 1 figures, FSE 2025", "summary": "Log anomaly detection plays a critical role in ensuring the stability and\nreliability of software systems. However, existing approaches rely on large\namounts of labeled log data, which poses significant challenges in real-world\napplications. To address this issue, cross-system transfer has been identified\nas a key research direction. State-of-the-art cross-system approaches achieve\npromising performance with only a few labels from the target system. However,\ntheir reliance on labeled target logs makes them susceptible to the cold-start\nproblem when labeled logs are insufficient. To overcome this limitation, we\nexplore a novel yet underexplored setting: zero-label cross-system log anomaly\ndetection, where the target system logs are entirely unlabeled. To this end, we\npropose FreeLog, a system-agnostic representation meta-learning method that\neliminates the need for labeled target system logs, enabling cross-system log\nanomaly detection under zero-label conditions. Experimental results on three\npublic log datasets demonstrate that FreeLog achieves performance comparable to\nstate-of-the-art methods that rely on a small amount of labeled data from the\ntarget system."}
{"id": "2507.19823", "pdf": "https://arxiv.org/pdf/2507.19823", "abs": "https://arxiv.org/abs/2507.19823", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."}
{"id": "2507.19836", "pdf": "https://arxiv.org/pdf/2507.19836", "abs": "https://arxiv.org/abs/2507.19836", "authors": ["Xuanchen Wang", "Heng Wang", "Weidong Cai"], "title": "ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": "10 pages, 5 figures, accepted by the 33rd ACM International\n  Conference on Multimedia (ACM MM 2025), demo page:\n  https://choreomuse.github.io", "summary": "Modern artistic productions increasingly demand automated choreography\ngeneration that adapts to diverse musical styles and individual dancer\ncharacteristics. Existing approaches often fail to produce high-quality dance\nvideos that harmonize with both musical rhythm and user-defined choreography\nstyles, limiting their applicability in real-world creative contexts. To\naddress this gap, we introduce ChoreoMuse, a diffusion-based framework that\nuses SMPL format parameters and their variation version as intermediaries\nbetween music and video generation, thereby overcoming the usual constraints\nimposed by video resolution. Critically, ChoreoMuse supports\nstyle-controllable, high-fidelity dance video generation across diverse musical\ngenres and individual dancer characteristics, including the flexibility to\nhandle any reference individual at any resolution. Our method employs a novel\nmusic encoder MotionTune to capture motion cues from audio, ensuring that the\ngenerated choreography closely follows the beat and expressive qualities of the\ninput music. To quantitatively evaluate how well the generated dances match\nboth musical and choreographic styles, we introduce two new metrics that\nmeasure alignment with the intended stylistic cues. Extensive experiments\nconfirm that ChoreoMuse achieves state-of-the-art performance across multiple\ndimensions, including video quality, beat alignment, dance diversity, and style\nadherence, demonstrating its potential as a robust solution for a wide range of\ncreative applications. Video results can be found on our project page:\nhttps://choreomuse.github.io."}
{"id": "2507.19840", "pdf": "https://arxiv.org/pdf/2507.19840", "abs": "https://arxiv.org/abs/2507.19840", "authors": ["Samuel Ebimobowei Johnny", "Blessed Guda", "Andrew Blayama Stephen", "Assane Gueye"], "title": "AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Paper to appear at the 1st Workshop in Multimodal Sign Language\n  Recognition at ICCV 2025", "summary": "Continuously recognizing sign gestures and converting them to glosses plays a\nkey role in bridging the gap between the hearing and hearing-impaired\ncommunities. This involves recognizing and interpreting the hands, face, and\nbody gestures of the signer, which pose a challenge as it involves a\ncombination of all these features. Continuous Sign Language Recognition (CSLR)\nmethods rely on multi-stage pipelines that first extract visual features, then\nalign variable-length sequences with target glosses using CTC or HMM-based\napproaches. However, these alignment-based methods suffer from error\npropagation across stages, overfitting, and struggle with vocabulary\nscalability due to the intermediate gloss representation bottleneck. To address\nthese limitations, we propose AutoSign, an autoregressive decoder-only\ntransformer that directly translates pose sequences to natural language text,\nbypassing traditional alignment mechanisms entirely. The use of this\ndecoder-only approach allows the model to directly map between the features and\nthe glosses without the need for CTC loss while also directly learning the\ntextual dependencies in the glosses. Our approach incorporates a temporal\ncompression module using 1D CNNs to efficiently process pose sequences,\nfollowed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses).\nThrough comprehensive ablation studies, we demonstrate that hand and body\ngestures provide the most discriminative features for signer-independent CSLR.\nBy eliminating the multi-stage pipeline, AutoSign achieves substantial\nimprovements on the Isharah-1000 dataset, achieving an improvement of up to\n6.1\\% in WER score compared to the best existing method."}
{"id": "2507.19842", "pdf": "https://arxiv.org/pdf/2507.19842", "abs": "https://arxiv.org/abs/2507.19842", "authors": ["Mohammad Azarijafari", "Luisa Mich", "Michele Missikoff", "Oleg Missikoff"], "title": "A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Enterprises are currently undergoing profound transformations due to the\nunpostponable digital transformation. Then, to remain competitive, enterprises\nmust adapt their organisational structures and operations. This organisational\nshift is also important for small and medium-sized enterprises. A key\ninnovation frontier is the adoption of process-oriented production models. This\npaper presents a knowledge-based method to support business experts in\ndesigning business processes. The method requires no prior expertise in\nKnowledge Engineering and guides designers through a structured sequence of\nsteps to produce a diagrammatic workflow of the target process. The\nconstruction of the knowledge base starts from simple, text-based, knowledge\nartefacts and then progresses towards more structured, formal representations.\nThe approach has been conceived to allow a shared approach for all stakeholders\nand actors who participate in the BP design."}
{"id": "2507.19844", "pdf": "https://arxiv.org/pdf/2507.19844", "abs": "https://arxiv.org/abs/2507.19844", "authors": ["Biswarup Mukherjee", "Li Zhou", "S. Gokul Krishnan", "Milad Kabirifar", "Subhash Lakshminarayana", "Charalambos Konstantinou"], "title": "VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "summary": "This paper introduces a model for coordinating prosumers with heterogeneous\ndistributed energy resources (DERs), participating in the local energy market\n(LEM) that interacts with the market-clearing entity. The proposed LEM scheme\nutilizes a data-driven, model-free reinforcement learning approach based on the\nmulti-agent deep deterministic policy gradient (MADDPG) framework, enabling\nprosumers to make real-time decisions on whether to buy, sell, or refrain from\nany action while facilitating efficient coordination for optimal energy trading\nin a dynamic market. In addition, we investigate a price manipulation strategy\nusing a variational auto encoder-generative adversarial network (VAE-GAN)\nmodel, which allows utilities to adjust price signals in a way that induces\nfinancial losses for the prosumers. Our results show that under adversarial\npricing, heterogeneous prosumer groups, particularly those lacking generation\ncapabilities, incur financial losses. The same outcome holds across LEMs of\ndifferent sizes. As the market size increases, trading stabilizes and fairness\nimproves through emergent cooperation among agents."}
{"id": "2507.19849", "pdf": "https://arxiv.org/pdf/2507.19849", "abs": "https://arxiv.org/abs/2507.19849", "authors": ["Guanting Dong", "Hangyu Mao", "Kai Ma", "Licheng Bao", "Yifei Chen", "Zhongyuan Wang", "Zhongxia Chen", "Jiazhen Du", "Huiyang Wang", "Fuzheng Zhang", "Guorui Zhou", "Yutao Zhu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "Agentic Reinforced Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Working on progress", "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO"}
{"id": "2507.19856", "pdf": "https://arxiv.org/pdf/2507.19856", "abs": "https://arxiv.org/abs/2507.19856", "authors": ["Xiaokai Bai", "Chenxu Zhou", "Lianqing Zheng", "Si-Yuan Cao", "Jianan Liu", "Xiaohan Zhang", "Zhengzhuang Zhang", "Hui-liang Shen"], "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 6 figures, conference", "summary": "4D millimeter-wave radar has emerged as a promising sensor for autonomous\ndriving, but effective 3D object detection from both 4D radar and monocular\nimages remains a challenge. Existing fusion approaches typically rely on either\ninstance-based proposals or dense BEV grids, which either lack holistic scene\nunderstanding or are limited by rigid grid structures. To address these, we\npropose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as\nrepresentation for fusing 4D radar and monocular cues in 3D object detection.\n3D GS naturally suits 3D object detection by modeling the scene as a field of\nGaussians, dynamically allocating resources on foreground objects and providing\na flexible, resource-efficient solution. RaGS uses a cascaded pipeline to\nconstruct and refine the Gaussian field. It starts with the Frustum-based\nLocalization Initiation (FLI), which unprojects foreground pixels to initialize\ncoarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)\nfuses semantics and geometry, refining the limited Gaussians to the regions of\ninterest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians\ninto multi-level BEV features for 3D object detection. By dynamically focusing\non sparse objects within scenes, RaGS enable object concentrating while\noffering comprehensive scene perception. Extensive experiments on\nView-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its\nstate-of-the-art performance. Code will be released."}
{"id": "2507.19880", "pdf": "https://arxiv.org/pdf/2507.19880", "abs": "https://arxiv.org/abs/2507.19880", "authors": ["Nicola Croce", "Tobin South"], "title": "Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data", "categories": ["cs.CR", "cs.AI"], "comment": "Abstract submitted to the Technical AI Governance Forum 2025\n  (https://www.techgov.ai/)", "summary": "The Model Context Protocol (MCP) represents a significant advancement in\nAI-tool integration, enabling seamless communication between AI agents and\nexternal services. However, this connectivity introduces novel attack vectors\nthat remain largely unexplored. This paper demonstrates how unsophisticated\nthreat actors, requiring only basic programming skills and free web tools, can\nexploit MCP's trust model to exfiltrate sensitive financial data. We present a\nproof-of-concept attack where a malicious weather MCP server, disguised as\nbenign functionality, discovers and exploits legitimate banking tools to steal\nuser account balances. The attack chain requires no advanced technical\nknowledge, server infrastructure, or monetary investment. The findings reveal a\ncritical security gap in the emerging MCP ecosystem: while individual servers\nmay appear trustworthy, their combination creates unexpected cross-server\nattack surfaces. Unlike traditional cybersecurity threats that assume\nsophisticated adversaries, our research shows that the barrier to entry for\nMCP-based attacks is alarmingly low. A threat actor with undergraduate-level\nPython knowledge can craft convincing social engineering attacks that exploit\nthe implicit trust relationships MCP establishes between AI agents and tool\nproviders. This work contributes to the nascent field of MCP security by\ndemonstrating that current MCP implementations allow trivial cross-server\nattacks and proposing both immediate mitigations and protocol improvements to\nsecure this emerging ecosystem."}
{"id": "2507.19881", "pdf": "https://arxiv.org/pdf/2507.19881", "abs": "https://arxiv.org/abs/2507.19881", "authors": ["Tao Lian", "Jose L. Gómez", "Antonio M. López"], "title": "FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Federated domain generalization has shown promising progress in image\nclassification by enabling collaborative training across multiple clients\nwithout sharing raw data. However, its potential in the semantic segmentation\nof autonomous driving remains underexplored. In this paper, we propose FedS2R,\nthe first one-shot federated domain generalization framework for\nsynthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises\ntwo components: an inconsistency-driven data augmentation strategy that\ngenerates images for unstable classes, and a multi-client knowledge\ndistillation scheme with feature fusion that distills a global model from\nmultiple client models. Experiments on five real-world datasets, Cityscapes,\nBDD100K, Mapillary, IDD, and ACDC, show that the global model significantly\noutperforms individual client models and is only 2 mIoU points behind the model\ntrained with simultaneous access to all client data. These results demonstrate\nthe effectiveness of FedS2R in synthetic-to-real semantic segmentation for\nautonomous driving under federated learning"}
{"id": "2507.19891", "pdf": "https://arxiv.org/pdf/2507.19891", "abs": "https://arxiv.org/abs/2507.19891", "authors": ["Drandreb Earl O. Juanico", "Rowel O. Atienza", "Jeffrey Kenneth Go"], "title": "Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages with supplementary material, 6 main figures, 2 main tables;\n  github: earl-juanico/rca", "summary": "We propose Reverse Contrast Attention (RCA), a plug-in method that enhances\nobject localization in vision-language transformers without retraining. RCA\nreweights final-layer attention by suppressing extremes and amplifying\nmid-level activations to let semantically relevant but subdued tokens guide\npredictions. We evaluate it on Open Vocabulary Referring Object Detection\n(OV-RefOD), introducing FitAP, a confidence-free average precision metric based\non IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with\ngains up to $+26.6\\%$. Effectiveness aligns with attention sharpness and fusion\ntiming; while late-fusion models benefit consistently, models like\n$\\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement\nas key factors. RCA offers both interpretability and performance gains for\nmultimodal transformers."}
{"id": "2507.19898", "pdf": "https://arxiv.org/pdf/2507.19898", "abs": "https://arxiv.org/abs/2507.19898", "authors": ["Parsa Vares", "Éloi Durant", "Jun Pang", "Nicolas Médoc", "Mohammad Ghoniem"], "title": "TS-Insight: Visualizing Thompson Sampling for Verification and XAI", "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "I.2.6; H.5.2"], "comment": "Accepted as a poster at IEEE VIS 2025 (\"TS-Insight: Visual\n  Fingerprinting of Multi-Armed Bandits\"). Open-source tool available at\n  https://github.com/parsavares/ts-insight", "summary": "Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit\nalgorithms used to balance exploration and exploitation strategies in active\nlearning. Yet, their probabilistic nature often turns them into a ``black\nbox'', hindering debugging and trust. We introduce TS-Insight, a visual\nanalytics tool explicitly designed to shed light on the internal decision\nmechanisms of Thompson Sampling-based algorithms, for model developers. It\ncomprises multiple plots, tracing for each arm the evolving posteriors,\nevidence counts, and sampling outcomes, enabling the verification, diagnosis,\nand explainability of exploration/exploitation dynamics. This tool aims at\nfostering trust and facilitating effective debugging and deployment in complex\nbinary decision-making scenarios especially in sensitive domains requiring\ninterpretable decision-making."}
{"id": "2507.19902", "pdf": "https://arxiv.org/pdf/2507.19902", "abs": "https://arxiv.org/abs/2507.19902", "authors": ["Sourena Khanzadeh"], "title": "AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software development is a complex, multi-phase process traditionally\nrequiring collaboration among individuals with diverse expertise. We propose\nAgentMesh, a Python-based framework that uses multiple cooperating LLM-powered\nagents to automate software development tasks. In AgentMesh, specialized agents\n- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a\nhigh-level requirement into fully realized code. The Planner agent first\ndecomposes user requests into concrete subtasks; the Coder agent implements\neach subtask in code; the Debugger agent tests and fixes the code; and the\nReviewer agent validates the final output for correctness and quality. We\ndescribe the architecture and design of these agents and their communication,\nand provide implementation details including prompt strategies and workflow\norchestration. A case study illustrates AgentMesh handling a non-trivial\ndevelopment request via sequential task planning, code generation, iterative\ndebugging, and final code review. We discuss how dividing responsibilities\namong cooperative agents leverages the strengths of large language models while\nmitigating single-agent limitations. Finally, we examine current limitations -\nsuch as error propagation and context scaling - and outline future work toward\nmore robust, scalable multi-agent AI systems for software engineering\nautomation."}
{"id": "2507.19904", "pdf": "https://arxiv.org/pdf/2507.19904", "abs": "https://arxiv.org/abs/2507.19904", "authors": ["Zhanhang Xiong", "Dongxia Wang", "Yuekang Li", "Xinyuan An", "Wenhai Wang"], "title": "CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in software\nengineering workflows, a critical capability remains underexplored: generating\ncorrect code that enables cross-programming-language (CPL) interoperability.\nThis skill is essential for building complex systems that integrate components\nwritten in multiple languages via mechanisms like inter-process communication\n(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to\nsystematically evaluate LLMs' ability to generate CPL-interoperating code.\nCrossPL comprises 1,982 tasks centered around IPC, covering six widely-used\nprogramming languages and seven representative CPL techniques. We construct\nthis benchmark by (i) analyzing 19,169 multi-language GitHub repositories using\n156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based\npipeline that automatically extracts CPL code snippets, generates task\ninstructions, and validates functional correctness. We evaluate 14\nstate-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the\npast three years on CrossPL via FSM-based validation. Results reveal that even\nthe best-performing models struggle with CPL scenarios, underscoring the need\nfor more targeted research in this space. Our benchmark and code are available\nat: https://anonymous.4open.science/r/crosspl-2814."}
{"id": "2507.19909", "pdf": "https://arxiv.org/pdf/2507.19909", "abs": "https://arxiv.org/abs/2507.19909", "authors": ["Roman Macháček", "Anastasiia Grishina", "Max Hort", "Leon Moonen"], "title": "The Impact of Fine-tuning Large Language Models on Automated Program Repair", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 41th\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Automated Program Repair (APR) uses various tools and techniques to help\ndevelopers achieve functional and error-free code faster. In recent years,\nLarge Language Models (LLMs) have gained popularity as components in APR tool\nchains because of their performance and flexibility. However, training such\nmodels requires a significant amount of resources. Fine-tuning techniques have\nbeen developed to adapt pre-trained LLMs to specific tasks, such as APR, and\nenhance their performance at far lower computational costs than training from\nscratch. In this study, we empirically investigate the impact of various\nfine-tuning techniques on the performance of LLMs used for APR. Our experiments\nprovide insights into the performance of a selection of state-of-the-art LLMs\npre-trained on code. The evaluation is done on three popular APR benchmarks\n(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs\nwith varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,\nBloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,\nfull fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and\nIA3. We observe that full fine-tuning techniques decrease the benchmarking\nperformance of various models due to different data distributions and\noverfitting. By using parameter-efficient fine-tuning methods, we restrict\nmodels in the amount of trainable parameters and achieve better results.\n  Keywords: large language models, automated program repair,\nparameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE."}
{"id": "2507.19917", "pdf": "https://arxiv.org/pdf/2507.19917", "abs": "https://arxiv.org/abs/2507.19917", "authors": ["Yuxuan Jiang", "Chenwei Yu", "Zhi Lin", "Xiaolan Liu"], "title": "A mini-batch training strategy for deep subspace clustering networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mini-batch training is a cornerstone of modern deep learning, offering\ncomputational efficiency and scalability for training complex architectures.\nHowever, existing deep subspace clustering (DSC) methods, which typically\ncombine an autoencoder with a self-expressive layer, rely on full-batch\nprocessing. The bottleneck arises from the self-expressive module, which\nrequires representations of the entire dataset to construct a\nself-representation coefficient matrix. In this work, we introduce a mini-batch\ntraining strategy for DSC by integrating a memory bank that preserves global\nfeature representations. Our approach enables scalable training of deep\narchitectures for subspace clustering with high-resolution images, overcoming\nprevious limitations. Additionally, to efficiently fine-tune large-scale\npre-trained encoders for subspace clustering, we propose a decoder-free\nframework that leverages contrastive learning instead of autoencoding for\nrepresentation learning. This design not only eliminates the computational\noverhead of decoder training but also provides competitive performance.\nExtensive experiments demonstrate that our approach not only achieves\nperformance comparable to full-batch methods, but outperforms other\nstate-of-the-art subspace clustering methods on the COIL100 and ORL datasets by\nfine-tuning deep networks."}
{"id": "2507.19929", "pdf": "https://arxiv.org/pdf/2507.19929", "abs": "https://arxiv.org/abs/2507.19929", "authors": ["Yanhui Sun", "Wu Liu", "Wentao Wang", "Hantao Yao", "Jiebo Luo", "Yongdong Zhang"], "title": "DynamiX: Large-Scale Dynamic Social Network Simulator", "categories": ["physics.soc-ph", "cs.AI"], "comment": null, "summary": "Understanding the intrinsic mechanisms of social platforms is an urgent\ndemand to maintain social stability. The rise of large language models provides\nsignificant potential for social network simulations to capture attitude\ndynamics and reproduce collective behaviors. However, existing studies mainly\nfocus on scaling up agent populations, neglecting the dynamic evolution of\nsocial relationships. To address this gap, we introduce DynamiX, a novel\nlarge-scale social network simulator dedicated to dynamic social network\nmodeling. DynamiX uses a dynamic hierarchy module for selecting core agents\nwith key characteristics at each timestep, enabling accurate alignment of\nreal-world adaptive switching of user roles. Furthermore, we design distinct\ndynamic social relationship modeling strategies for different user types. For\nopinion leaders, we propose an information-stream-based link prediction method\nrecommending potential users with similar stances, simulating homogeneous\nconnections, and autonomous behavior decisions. For ordinary users, we\nconstruct an inequality-oriented behavior decision-making module, effectively\naddressing unequal social interactions and capturing the patterns of\nrelationship adjustments driven by multi-dimensional factors. Experimental\nresults demonstrate that DynamiX exhibits marked improvements in attitude\nevolution simulation and collective behavior analysis compared to static\nnetworks. Besides, DynamiX opens a new theoretical perspective on follower\ngrowth prediction, providing empirical evidence for opinion leaders\ncultivation."}
{"id": "2507.19936", "pdf": "https://arxiv.org/pdf/2507.19936", "abs": "https://arxiv.org/abs/2507.19936", "authors": ["Zhongnian Li", "Chao Zheng", "Jian Xiao", "Ji Wang", "Gongpu Wang", "Ming Zeng", "Octavia A. Dobre"], "title": "Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "5 pages,8 figures", "summary": "This paper investigates joint channel estimation and positioning in\nnear-field sparse extra-large multiple-input multiple-output (XL-MIMO)\northogonal frequency division multiplexing (OFDM) systems. To achieve\ncooperative gains between channel estimation and positioning, we propose a deep\nlearning-based two-stage framework comprising positioning and channel\nestimation. In the positioning stage, the user's coordinates are predicted and\nutilized in the channel estimation stage, thereby enhancing the accuracy of\nchannel estimation. Within this framework, we propose a U-shaped Mamba\narchitecture for channel estimation and positioning, termed as CP-Mamba. This\nnetwork integrates the strengths of the Mamba model with the structural\nadvantages of U-shaped convolutional networks, enabling effective capture of\nlocal spatial features and long-range temporal dependencies of the channel.\nNumerical simulation results demonstrate that the proposed two-stage approach\nwith CP-Mamba architecture outperforms existing baseline methods. Moreover,\nsparse arrays (SA) exhibit significantly superior performance in both channel\nestimation and positioning accuracy compared to conventional compact arrays."}
{"id": "2507.19950", "pdf": "https://arxiv.org/pdf/2507.19950", "abs": "https://arxiv.org/abs/2507.19950", "authors": ["Chengyu Zheng", "Jin Huang", "Honghua Chen", "Mingqiang Wei"], "title": "RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent research leveraging large-scale pretrained diffusion models has\ndemonstrated the potential of using diffusion features to establish semantic\ncorrespondences in images. Inspired by advancements in diffusion-based\ntechniques, we propose a novel zero-shot method for refining point cloud\nregistration algorithms. Our approach leverages correspondences derived from\ndepth images to enhance point feature representations, eliminating the need for\na dedicated training dataset. Specifically, we first project the point cloud\ninto depth maps from multiple perspectives and extract implicit knowledge from\na pretrained diffusion network as depth diffusion features. These features are\nthen integrated with geometric features obtained from existing methods to\nestablish more accurate correspondences between point clouds. By leveraging\nthese refined correspondences, our approach achieves significantly improved\nregistration accuracy. Extensive experiments demonstrate that our method not\nonly enhances the performance of existing point cloud registration techniques\nbut also exhibits robust generalization capabilities across diverse datasets.\nCodes are available at https://github.com/zhengcy-lambo/RARE.git."}
{"id": "2507.19956", "pdf": "https://arxiv.org/pdf/2507.19956", "abs": "https://arxiv.org/abs/2507.19956", "authors": ["Cesar Kadir Torrico Villanueva", "Jiaxin Cindy Tu", "Mihir Tripathy", "Connor Lane", "Rishab Iyer", "Paul S. Scotti"], "title": "Predicting Brain Responses To Natural Movies With Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "Code available at https://github.com/MedARC-AI/algonauts2025", "summary": "We present MedARC's team solution to the Algonauts 2025 challenge. Our\npipeline leveraged rich multimodal representations from various\nstate-of-the-art pretrained models across video (V-JEPA2), speech (Whisper),\ntext (Llama 3.2), vision-text (InternVL3), and vision-text-audio\n(Qwen2.5-Omni). These features extracted from the models were linearly\nprojected to a latent space, temporally aligned to the fMRI time series, and\nfinally mapped to cortical parcels through a lightweight encoder comprising a\nshared group head plus subject-specific residual heads. We trained hundreds of\nmodel variants across hyperparameter settings, validated them on held-out\nmovies and assembled ensembles targeted to each parcel in each subject. Our\nfinal submission achieved a mean Pearson's correlation of 0.2085 on the test\nsplit of withheld out-of-distribution movies, placing our team in fourth place\nfor the competition. We further discuss a last-minute optimization that would\nhave raised us to second place. Our results highlight how combining features\nfrom models trained in different modalities, using a simple architecture\nconsisting of shared-subject and single-subject components, and conducting\ncomprehensive model selection and ensembling improves generalization of\nencoding models to novel movie stimuli. All code is available on GitHub."}
{"id": "2507.19961", "pdf": "https://arxiv.org/pdf/2507.19961", "abs": "https://arxiv.org/abs/2507.19961", "authors": ["Oğuzhan Büyüksolak", "İlkay Öksüz"], "title": "Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures", "categories": ["cs.CV", "cs.AI"], "comment": "To appear in: Proceedings of the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC), 2025", "summary": "The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases.\nHowever, many disease patterns are derived from outdated datasets and\ntraditional stepwise algorithms with limited accuracy. This study presents a\nmethod for direct cardiovascular disease (CVD) diagnosis from ECG images,\neliminating the need for digitization. The proposed approach utilizes a\ntwo-step curriculum learning framework, beginning with the pre-training of a\nclassification model on segmentation masks, followed by fine-tuning on\ngrayscale, inverted ECG images. Robustness is further enhanced through an\nensemble of three models with averaged outputs, achieving an AUC of 0.9534 and\nan F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming\nindividual models. By effectively handling real-world artifacts and simplifying\nthe diagnostic process, this method offers a reliable solution for automated\nCVD diagnosis, particularly in resource-limited settings where printed or\nscanned ECG images are commonly used. Such an automated procedure enables rapid\nand accurate diagnosis, which is critical for timely intervention in CVD cases\nthat often demand urgent care."}
{"id": "2507.19968", "pdf": "https://arxiv.org/pdf/2507.19968", "abs": "https://arxiv.org/abs/2507.19968", "authors": ["Yue Hu", "Zanxia Cao", "Yingchao Liu"], "title": "Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "8 pages, 2 figures", "summary": "First-order optimization methods, such as SGD and Adam, are widely used for\ntraining large-scale deep neural networks due to their computational efficiency\nand robust performance. However, relying solely on gradient information, these\nmethods often struggle to navigate complex loss landscapes with flat regions,\nplateaus, and saddle points. Second-order methods, which use curvature\ninformation from the Hessian matrix, can address these challenges but are\ncomputationally infeasible for large models. The Dimer method, a first-order\ntechnique that constructs two closely spaced points to probe the local geometry\nof a potential energy surface, efficiently estimates curvature using only\ngradient information. Inspired by its use in molecular dynamics simulations for\nlocating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel\nframework to escape saddle points in neural network training. DEO adapts the\nDimer method to explore a broader region of the loss landscape, approximating\nthe Hessian's smallest eigenvector without computing the full matrix. By\nperiodically projecting the gradient onto the subspace orthogonal to the\nminimum curvature direction, DEO guides the optimizer away from saddle points\nand flat regions, enhancing training efficiency with non-stepwise updates.\nPreliminary experiments on a Transformer toy model show DEO achieves\ncompetitive performance compared to standard first-order methods, improving\nnavigation of complex loss landscapes. Our work repurposes physics-inspired,\nfirst-order curvature estimation to enhance neural network training in\nhigh-dimensional spaces."}
{"id": "2507.19975", "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs."}
{"id": "2507.19983", "pdf": "https://arxiv.org/pdf/2507.19983", "abs": "https://arxiv.org/abs/2507.19983", "authors": ["Yuhong Deng", "Chao Tang", "Cunjun Yu", "Linfeng Li", "David Hsu"], "title": "CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Clothes manipulation, such as folding or hanging, is a critical capability\nfor home service robots. Despite recent advances, most existing methods remain\nlimited to specific tasks and clothes types, due to the complex,\nhigh-dimensional geometry of clothes. This paper presents CLothes mAnipulation\nwith Semantic keyPoints (CLASP), which aims at general-purpose clothes\nmanipulation over different clothes types, T-shirts, shorts, skirts, long\ndresses, ... , as well as different tasks, folding, flattening, hanging, ... .\nThe core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right\nshoulder'', etc. -- a sparse spatial-semantic representation that is salient\nfor both perception and action. Semantic keypoints of clothes can be reliably\nextracted from RGB-D images and provide an effective intermediate\nrepresentation of clothes manipulation policies. CLASP uses semantic keypoints\nto bridge high-level task planning and low-level action execution. At the high\nlevel, it exploits vision language models (VLMs) to predict task plans over the\nsemantic keypoints. At the low level, it executes the plans with the help of a\nsimple pre-built manipulation skill library. Extensive simulation experiments\nshow that CLASP outperforms state-of-the-art baseline methods on multiple tasks\nacross diverse clothes types, demonstrating strong performance and\ngeneralization. Further experiments with a Franka dual-arm system on four\ndistinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's\nperformance on a real robot."}
{"id": "2507.19990", "pdf": "https://arxiv.org/pdf/2507.19990", "abs": "https://arxiv.org/abs/2507.19990", "authors": ["Sinnyum Choi", "Woong Kim"], "title": "Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3; I.2.6; I.2.7"], "comment": null, "summary": "Recently, competition in the field of artificial intelligence (AI) has\nintensified among major technological companies, resulting in the continuous\nrelease of new large-language models (LLMs) that exhibit improved language\nunderstanding and context-based reasoning capabilities. It is expected that\nthese advances will enable more efficient personalized recommendations in\nLLM-based recommendation systems through improved quality of training data and\narchitectural design. However, many studies have not considered these recent\ndevelopments. In this study, it was proposed to improve LLM-based\nrecommendation systems by replacing Llama2 with Llama3 in the LlamaRec\nframework. To ensure a fair comparison, random seed values were set and\nidentical input data was provided during preprocessing and training. The\nexperimental results show average performance improvements of 38.65\\%, 8.69\\%,\nand 8.19\\% for the ML-100K, Beauty, and Games datasets, respectively, thus\nconfirming the practicality of this method. Notably, the significant\nimprovements achieved by model replacement indicate that the recommendation\nquality can be improved cost-effectively without the need to make structural\nchanges to the system. Based on these results, it is our contention that the\nproposed approach is a viable solution for improving the performance of current\nrecommendation systems."}
{"id": "2507.19992", "pdf": "https://arxiv.org/pdf/2507.19992", "abs": "https://arxiv.org/abs/2507.19992", "authors": ["Md Fantacher Islam", "Jarrod Mosier", "Vignesh Subbian"], "title": "NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care", "categories": ["q-bio.OT", "cs.AI"], "comment": "Submitted to the Journal of the American Medical Informatics\n  Association (JAMIA)", "summary": "Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology to\nsupport knowledge representation in acute care settings.\n  Materials and Methods: We developed the NIRS ontology using Web Ontology\nLanguage (OWL) semantics and Protege to organize clinical concepts and\nrelationships. To enable rule-based clinical reasoning beyond hierarchical\nstructures, we added Semantic Web Rule Language (SWRL) rules. We evaluated\nlogical reasoning by adding 17 hypothetical patient clinical scenarios. We used\nSPARQL queries and data from the Electronic Intensive Care Unit (eICU)\nCollaborative Research Database to retrieve and test targeted inferences.\n  Results: The ontology has 132 classes, 12 object properties, and 17 data\nproperties across 882 axioms that establish concept relationships. To\nstandardize clinical concepts, we added 350 annotations, including descriptive\ndefinitions based on controlled vocabularies. SPARQL queries successfully\nvalidated all test cases (rules) by retrieving appropriate patient outcomes,\nfor instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours\ndue to acute respiratory failure may avoid endotracheal intubation.\n  Discussion: The NIRS ontology formally represents domain-specific concepts,\nincluding ventilation modalities, patient characteristics, therapy parameters,\nand outcomes. SPARQL query evaluations on clinical scenarios confirmed the\nability of the ontology to support rule based reasoning and therapy\nrecommendations, providing a foundation for consistent documentation practices,\nintegration into clinical data models, and advanced analysis of NIRS outcomes.\n  Conclusion: We unified NIRS concepts into an ontological framework and\ndemonstrated its applicability through the evaluation of hypothetical patient\nscenarios and alignment with standardized vocabularies."}
{"id": "2507.19995", "pdf": "https://arxiv.org/pdf/2507.19995", "abs": "https://arxiv.org/abs/2507.19995", "authors": ["Tan-Minh Nguyen", "Hoang-Trung Nguyen", "Trong-Khoi Dao", "Xuan-Hieu Phan", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong"], "title": "VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs) has led to significant\nachievements in various domains, including legal text processing. Leveraging\nLLMs for legal tasks is a natural evolution and an increasingly compelling\nchoice. However, their capabilities are often portrayed as greater than they\ntruly are. Despite the progress, we are still far from the ultimate goal of\nfully automating legal tasks using artificial intelligence (AI) and natural\nlanguage processing (NLP). Moreover, legal systems are deeply domain-specific\nand exhibit substantial variation across different countries and languages. The\nneed for building legal text processing applications for different natural\nlanguages is, therefore, large and urgent. However, there is a big challenge\nfor legal NLP in low-resource languages such as Vietnamese due to the scarcity\nof resources and annotated data. The need for labeled legal corpora for\nsupervised training, validation, and supervised fine-tuning is critical. In\nthis paper, we introduce the VLQA dataset, a comprehensive and high-quality\nresource tailored for the Vietnamese legal domain. We also conduct a\ncomprehensive statistical analysis of the dataset and evaluate its\neffectiveness through experiments with state-of-the-art models on legal\ninformation retrieval and question-answering tasks."}
{"id": "2507.20008", "pdf": "https://arxiv.org/pdf/2507.20008", "abs": "https://arxiv.org/abs/2507.20008", "authors": ["Padmavathi Moorthy"], "title": "Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost", "categories": ["cs.LG", "cs.AI", "62M10 (Primary), 62H30 (Secondary)", "I.2.6; I.5.1; I.2.10"], "comment": "10 pages, 9 figures, prepared with LaTeX, GitHub link:\n  https://github.com/padmavathi026/Smart-Fare-Prediction", "summary": "Precise fare prediction is crucial in ride-hailing platforms and urban\nmobility systems. This study examines three machine learning models-Graph\nAttention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive\ncapabilities for taxi fares using a real-world dataset comprising over 55\nmillion records. Both raw (noisy) and denoised versions of the dataset are\nanalyzed to assess the impact of data quality on model performance. The study\nevaluated the models along multiple axes, including predictive accuracy,\ncalibration, uncertainty estimation, out-of-distribution (OOD) robustness, and\nfeature sensitivity. We also explore pre-processing strategies, including KNN\nimputation, Gaussian noise injection, and autoencoder-based denoising. The\nstudy reveals critical differences between classical and deep learning models\nunder realistic conditions, offering practical guidelines for building robust\nand scalable models in urban fare prediction systems."}
{"id": "2507.20014", "pdf": "https://arxiv.org/pdf/2507.20014", "abs": "https://arxiv.org/abs/2507.20014", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet"], "title": "Policy-Driven AI in Dataspaces: Taxonomy, Explainability, and Pathways for Compliant Innovation", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As AI-driven dataspaces become integral to data sharing and collaborative\nanalytics, ensuring privacy, performance, and policy compliance presents\nsignificant challenges. This paper provides a comprehensive review of\nprivacy-preserving and policy-aware AI techniques, including Federated\nLearning, Differential Privacy, Trusted Execution Environments, Homomorphic\nEncryption, and Secure Multi-Party Computation, alongside strategies for\naligning AI with regulatory frameworks such as GDPR and the EU AI Act. We\npropose a novel taxonomy to classify these techniques based on privacy levels,\nperformance impacts, and compliance complexity, offering a clear framework for\npractitioners and researchers to navigate trade-offs. Key performance metrics\n-- latency, throughput, cost overhead, model utility, fairness, and\nexplainability -- are analyzed to highlight the multi-dimensional optimization\nrequired in dataspaces. The paper identifies critical research gaps, including\nthe lack of standardized privacy-performance KPIs, challenges in explainable AI\nfor federated ecosystems, and semantic policy enforcement amidst regulatory\nfragmentation. Future directions are outlined, proposing a conceptual framework\nfor policy-driven alignment, automated compliance validation, standardized\nbenchmarking, and integration with European initiatives like GAIA-X, IDS, and\nEclipse EDC. By synthesizing technical, ethical, and regulatory perspectives,\nthis work lays the groundwork for developing trustworthy, efficient, and\ncompliant AI systems in dataspaces, fostering innovation in secure and\nresponsible data-driven ecosystems."}
{"id": "2507.20016", "pdf": "https://arxiv.org/pdf/2507.20016", "abs": "https://arxiv.org/abs/2507.20016", "authors": ["Liu junkang", "Yuanyuan Liu", "Fanhua Shang", "Hongying Liu", "Jin Liu", "Wei Feng"], "title": "FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.1"], "comment": "icml 2025", "summary": "For federated learning (FL) algorithms such as FedSAM, their generalization\ncapability is crucial for real-word applications. In this paper, we revisit the\ngeneralization problem in FL and investigate the impact of data heterogeneity\non FL generalization. We find that FedSAM usually performs worse than FedAvg in\nthe case of highly heterogeneous data, and thus propose a novel and effective\nfederated learning algorithm with Stochastic Weight Averaging (called\n\\texttt{FedSWA}), which aims to find flatter minima in the setting of highly\nheterogeneous data. Moreover, we introduce a new momentum-based stochastic\ncontrolled weight averaging FL algorithm (\\texttt{FedMoSWA}), which is designed\nto better align local and global models.\n  Theoretically, we provide both convergence analysis and generalization bounds\nfor \\texttt{FedSWA} and \\texttt{FedMoSWA}. We also prove that the optimization\nand generalization errors of \\texttt{FedMoSWA} are smaller than those of their\ncounterparts, including FedSAM and its variants. Empirically, experimental\nresults on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the\nproposed algorithms compared to their counterparts. Open source code at:\nhttps://github.com/junkangLiu0/FedSWA."}
{"id": "2507.20018", "pdf": "https://arxiv.org/pdf/2507.20018", "abs": "https://arxiv.org/abs/2507.20018", "authors": ["Sayed Mahbub Hasan Amiri", "Prasun Goswami", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Naznin Akter"], "title": "The Carbon Cost of Conversation, Sustainability in the Age of Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "22 Pages, 5 Tables", "summary": "Large language models (LLMs) like GPT-3 and BERT have revolutionized natural\nlanguage processing (NLP), yet their environmental costs remain dangerously\noverlooked. This article critiques the sustainability of LLMs, quantifying\ntheir carbon footprint, water usage, and contribution to e-waste through case\nstudies of models such as GPT-4 and energy-efficient alternatives like Mistral\n7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of\ncars driven annually, while data centre cooling exacerbates water scarcity in\nvulnerable regions. Systemic challenges corporate greenwashing, redundant model\ndevelopment, and regulatory voids perpetuate harm, disproportionately burdening\nmarginalized communities in the Global South. However, pathways exist for\nsustainable NLP: technical innovations (e.g., model pruning, quantum\ncomputing), policy reforms (carbon taxes, mandatory emissions reporting), and\ncultural shifts prioritizing necessity over novelty. By analysing industry\nleaders (Google, Microsoft) and laggards (Amazon), this work underscores the\nurgency of ethical accountability and global cooperation. Without immediate\naction, AIs ecological toll risks outpacing its societal benefits. The article\nconcludes with a call to align technological progress with planetary\nboundaries, advocating for equitable, transparent, and regenerative AI systems\nthat prioritize both human and environmental well-being."}
{"id": "2507.20019", "pdf": "https://arxiv.org/pdf/2507.20019", "abs": "https://arxiv.org/abs/2507.20019", "authors": ["Saurav Singla", "Aarav Singla", "Advik Gupta", "Parnika Gupta"], "title": "Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages. PyTorch code for few-shot anomaly detection using\n  meta-learning is available upon request or can be shared via GitHub", "summary": "We propose a meta learning framework for detecting anomalies in human\nlanguage across diverse domains with limited labeled data. Anomalies in\nlanguage ranging from spam and fake news to hate speech pose a major challenge\ndue to their sparsity and variability. We treat anomaly detection as a few shot\nbinary classification problem and leverage meta-learning to train models that\ngeneralize across tasks. Using datasets from domains such as SMS spam, COVID-19\nfake news, and hate speech, we evaluate model generalization on unseen tasks\nwith minimal labeled anomalies. Our method combines episodic training with\nprototypical networks and domain resampling to adapt quickly to new anomaly\ndetection tasks. Empirical results show that our method outperforms strong\nbaselines in F1 and AUC scores. We also release the code and benchmarks to\nfacilitate further research in few-shot text anomaly detection."}
{"id": "2507.20021", "pdf": "https://arxiv.org/pdf/2507.20021", "abs": "https://arxiv.org/abs/2507.20021", "authors": ["Matin Aghaei", "Mohammad Ali Alomrani", "Yingxue Zhang", "Mahdi Biparva"], "title": "When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are often credited with recent leaps in\nObjectGoal Navigation, yet the extent to which they improve planning remains\nunclear. We revisit this question on the HM3D-v1 validation split. First, we\nstrip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary\nGLEE detector and Intuition saliency map, and replace them with a simple\nDistance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises\nSuccess from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000\nvalidation episodes, outperforming all previous training-free baselines.\nSecond, we add a lightweight language prior (SHF); on a 200-episode subset this\nyields a further +2% Success and +0.9% SPL while shortening paths by five steps\non average. Qualitative trajectories confirm the trend: InstructNav back-tracks\nand times-out, DWFE reaches the goal after a few islands, and SHF follows an\nalmost straight route. Our results indicate that frontier geometry, not\nemergent LLM reasoning, drives most reported gains, and suggest that\nmetric-aware prompts or offline semantic graphs are necessary before\nattributing navigation success to \"LLM intelligence.\""}
{"id": "2507.20028", "pdf": "https://arxiv.org/pdf/2507.20028", "abs": "https://arxiv.org/abs/2507.20028", "authors": ["Dhruv Sarkar", "Aprameyo Chakrabartty", "Bibhudatta Bhanja"], "title": "TAPS : Frustratingly Simple Test Time Active Learning for VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Test-Time Optimization enables models to adapt to new data during inference\nby updating parameters on-the-fly. Recent advances in Vision-Language Models\n(VLMs) have explored learning prompts at test time to improve performance in\ndownstream tasks. In this work, we extend this idea by addressing a more\ngeneral and practical challenge: Can we effectively utilize an oracle in a\ncontinuous data stream where only one sample is available at a time, requiring\nan immediate query decision while respecting latency and memory constraints? To\ntackle this, we propose a novel Test-Time Active Learning (TTAL) framework that\nadaptively queries uncertain samples and updates prompts dynamically. Unlike\nprior methods that assume batched data or multiple gradient updates, our\napproach operates in a real-time streaming scenario with a single test sample\nper step. We introduce a dynamically adjusted entropy threshold for active\nquerying, a class-balanced replacement strategy for memory efficiency, and a\nclass-aware distribution alignment technique to enhance adaptation. The design\nchoices are justified using careful theoretical analysis. Extensive experiments\nacross 10 cross-dataset transfer benchmarks and 4 domain generalization\ndatasets demonstrate consistent improvements over state-of-the-art methods\nwhile maintaining reasonable latency and memory overhead. Our framework\nprovides a practical and effective solution for real-world deployment in\nsafety-critical applications such as autonomous systems and medical\ndiagnostics."}
{"id": "2507.20048", "pdf": "https://arxiv.org/pdf/2507.20048", "abs": "https://arxiv.org/abs/2507.20048", "authors": ["Jesus S. Aguilar-Ruiz"], "title": "Irredundant $k$-Fold Cross-Validation", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "In traditional k-fold cross-validation, each instance is used ($k-1$) times\nfor training and once for testing, leading to redundancy that lets many\ninstances disproportionately influence the learning phase. We introduce\nIrredundant $k$-fold cross-validation, a novel method that guarantees each\ninstance is used exactly once for training and once for testing across the\nentire validation procedure. This approach ensures a more balanced utilization\nof the dataset, mitigates overfitting due to instance repetition, and enables\nsharper distinctions in comparative model analysis. The method preserves\nstratification and remains model-agnostic, i.e., compatible with any\nclassifier. Experimental results demonstrate that it delivers consistent\nperformance estimates across diverse datasets -- comparable to $k$-fold\ncross-validation -- while providing less optimistic variance estimates because\ntraining partitions are non-overlapping, and significantly reducing the overall\ncomputational cost."}
{"id": "2507.20056", "pdf": "https://arxiv.org/pdf/2507.20056", "abs": "https://arxiv.org/abs/2507.20056", "authors": ["Ze Rong", "ZiYue Zhao", "Zhaoxin Wang", "Lei Ma"], "title": "FaRMamba: Frequency-based learning and Reconstruction aided Mamba for Medical Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate medical image segmentation remains challenging due to blurred lesion\nboundaries (LBA), loss of high-frequency details (LHD), and difficulty in\nmodeling long-range anatomical structures (DC-LRSS). Vision Mamba employs\none-dimensional causal state-space recurrence to efficiently model global\ndependencies, thereby substantially mitigating DC-LRSS. However, its patch\ntokenization and 1D serialization disrupt local pixel adjacency and impose a\nlow-pass filtering effect, resulting in Local High-frequency Information\nCapture Deficiency (LHICD) and two-dimensional Spatial Structure Degradation\n(2D-SSD), which in turn exacerbate LBA and LHD. In this work, we propose\nFaRMamba, a novel extension that explicitly addresses LHICD and 2D-SSD through\ntwo complementary modules. A Multi-Scale Frequency Transform Module (MSFM)\nrestores attenuated high-frequency cues by isolating and reconstructing\nmulti-band spectra via wavelet, cosine, and Fourier transforms. A\nSelf-Supervised Reconstruction Auxiliary Encoder (SSRAE) enforces pixel-level\nreconstruction on the shared Mamba encoder to recover full 2D spatial\ncorrelations, enhancing both fine textures and global context. Extensive\nevaluations on CAMUS echocardiography, MRI-based Mouse-cochlea, and Kvasir-Seg\nendoscopy demonstrate that FaRMamba consistently outperforms competitive\nCNN-Transformer hybrids and existing Mamba variants, delivering superior\nboundary accuracy, detail preservation, and global coherence without\nprohibitive computational overhead. This work provides a flexible\nfrequency-aware framework for future segmentation models that directly\nmitigates core challenges in medical imaging."}
{"id": "2507.20059", "pdf": "https://arxiv.org/pdf/2507.20059", "abs": "https://arxiv.org/abs/2507.20059", "authors": ["Ran Xu", "Yuchen Zhuang", "Yue Yu", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "title": "RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. Code will be published at:\n  https://github.com/ritaranx/RAG_in_the_Wild", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved at inference time. While RAG\ndemonstrates strong performance on benchmarks largely derived from\ngeneral-domain corpora like Wikipedia, its effectiveness under realistic,\ndiverse retrieval scenarios remains underexplored. We evaluated RAG systems\nusing MassiveDS, a large-scale datastore with mixture of knowledge, and\nidentified critical limitations: retrieval mainly benefits smaller models,\nrerankers add minimal value, and no single retrieval source consistently\nexcels. Moreover, current LLMs struggle to route queries across heterogeneous\nknowledge sources. These findings highlight the need for adaptive retrieval\nstrategies before deploying RAG in real-world settings. Our code and data can\nbe found at https://github.com/ritaranx/RAG_in_the_Wild."}
{"id": "2507.20094", "pdf": "https://arxiv.org/pdf/2507.20094", "abs": "https://arxiv.org/abs/2507.20094", "authors": ["Ankit Sanjyal"], "title": "Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": "10 Pages, 8 figures, pre-print", "summary": "Diffusion models have become a powerful backbone for text-to-image\ngeneration, enabling users to synthesize high-quality visuals from natural\nlanguage prompts. However, they often struggle with complex prompts involving\nmultiple objects and global or local style specifications. In such cases, the\ngenerated scenes tend to lack style uniformity and spatial coherence, limiting\ntheir utility in creative and controllable content generation. In this paper,\nwe propose a simple, training-free architectural method called Local Prompt\nAdaptation (LPA). Our method decomposes the prompt into content and style\ntokens, and injects them selectively into the U-Net's attention layers at\ndifferent stages. By conditioning object tokens early and style tokens later in\nthe generation process, LPA enhances both layout control and stylistic\nconsistency. We evaluate our method on a custom benchmark of 50 style-rich\nprompts across five categories and compare against strong baselines including\nComposer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach\noutperforms prior work on both CLIP score and style consistency metrics,\noffering a new direction for controllable, expressive diffusion-based\ngeneration."}
{"id": "2507.20096", "pdf": "https://arxiv.org/pdf/2507.20096", "abs": "https://arxiv.org/abs/2507.20096", "authors": ["Xin Gao", "Xingming Xu"], "title": "EcoTransformer: Attention without Multiplication", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05"], "comment": "8 pages, 1 figure", "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy."}
{"id": "2507.20109", "pdf": "https://arxiv.org/pdf/2507.20109", "abs": "https://arxiv.org/abs/2507.20109", "authors": ["Xin Yin", "Chao Ni", "Liushan Chen", "Xiaohu Yang"], "title": "Learning to Align Human Code Preferences", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\nautomating software development tasks. While recent advances leverage\nSupervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align\nmodels with human preferences, the optimal training strategy remains unclear\nacross diverse code preference scenarios. This paper systematically\ninvestigates the roles of SFT and DPO in aligning LLMs with different code\npreferences. Through both theoretical analysis and empirical observation, we\nhypothesize that SFT excels in scenarios with objectively verifiable optimal\nsolutions, while applying SFT followed by DPO (S&D) enables models to explore\nsuperior solutions in scenarios without objectively verifiable optimal\nsolutions. Based on the analysis and experimental evidence, we propose Adaptive\nPreference Optimization (APO), a dynamic integration approach that adaptively\namplifies preferred responses, suppresses dispreferred ones, and encourages\nexploration of potentially superior solutions during training. Extensive\nexperiments across six representative code preference tasks validate our\ntheoretical hypotheses and demonstrate that APO consistently matches or\nsurpasses the performance of existing SFT and S&D strategies. Our work provides\nboth theoretical foundations and practical guidance for selecting appropriate\ntraining strategies in different code preference alignment scenarios."}
{"id": "2507.20110", "pdf": "https://arxiv.org/pdf/2507.20110", "abs": "https://arxiv.org/abs/2507.20110", "authors": ["Shiyu Liu", "Lianlei Shan"], "title": "NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4; I.5"], "comment": "**14 pages, 3 figures, 2 tables", "summary": "Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large\nLanguage Models (MLLMs) have significantly advanced 3D scene perception towards\nlanguage-driven cognition. However, existing 3D language models struggle with\nsparse, large-scale point clouds due to slow feature extraction and limited\nrepresentation accuracy. To address these challenges, we propose NeuroVoxel-LM,\na novel framework that integrates Neural Radiance Fields (NeRF) with dynamic\nresolution voxelization and lightweight meta-embedding. Specifically, we\nintroduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that\nadaptively adjusts voxel granularity based on geometric and structural\ncomplexity, reducing computational cost while preserving reconstruction\nfidelity. In addition, we propose the Token-level Adaptive Pooling for\nLightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic\nrepresentation through attention-based weighting and residual fusion.\nExperimental results demonstrate that DR-MSV significantly improves point cloud\nfeature extraction efficiency and accuracy, while TAP-LME outperforms\nconventional max-pooling in capturing fine-grained semantics from NeRF weights."}
{"id": "2507.20111", "pdf": "https://arxiv.org/pdf/2507.20111", "abs": "https://arxiv.org/abs/2507.20111", "authors": ["Rodrigo Gabriel Salazar Alva", "Matías Nuñez", "Cristian López", "Javier Martín Arista"], "title": "AI-Driven Generation of Old English: A Framework for Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preserving ancient languages is essential for understanding humanity's\ncultural and linguistic heritage, yet Old English remains critically\nunder-resourced, limiting its accessibility to modern natural language\nprocessing (NLP) techniques. We present a scalable framework that uses advanced\nlarge language models (LLMs) to generate high-quality Old English texts,\naddressing this gap. Our approach combines parameter-efficient fine-tuning\n(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a\ndual-agent pipeline that separates the tasks of content generation (in English)\nand translation (into Old English). Evaluation with automated metrics (BLEU,\nMETEOR, and CHRF) shows significant improvements over baseline models, with\nBLEU scores increasing from 26 to over 65 for English-to-Old English\ntranslation. Expert human assessment also confirms high grammatical accuracy\nand stylistic fidelity in the generated texts. Beyond expanding the Old English\ncorpus, our method offers a practical blueprint for revitalizing other\nendangered languages, effectively uniting AI innovation with the goals of\ncultural preservation."}
{"id": "2507.20112", "pdf": "https://arxiv.org/pdf/2507.20112", "abs": "https://arxiv.org/abs/2507.20112", "authors": ["Tianyi Xu", "Yiting Chen", "Henger Li", "Zheyong Bian", "Emiliano Dall'Anese", "Zizhan Zheng"], "title": "Online Learning with Probing for Sequential User-Centric Selection", "categories": ["cs.LG", "cs.AI", "cs.DS", "stat.ML"], "comment": null, "summary": "We formalize sequential decision-making with information acquisition as the\nprobing-augmented user-centric selection (PUCS) framework, where a learner\nfirst probes a subset of arms to obtain side information on resources and\nrewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such\nas ridesharing, wireless scheduling, and content recommendation, in which both\nresources and payoffs are initially unknown and probing is costly. For the\noffline setting with known distributions, we present a greedy probing algorithm\nwith a constant-factor approximation guarantee $\\zeta = (e-1)/(2e-1)$. For the\nonline setting with unknown distributions, we introduce OLPA, a stochastic\ncombinatorial bandit algorithm that achieves a regret bound\n$\\mathcal{O}(\\sqrt{T} + \\ln^{2} T)$. We also prove a lower bound\n$\\Omega(\\sqrt{T})$, showing that the upper bound is tight up to logarithmic\nfactors. Experiments on real-world data demonstrate the effectiveness of our\nsolutions."}
{"id": "2507.20115", "pdf": "https://arxiv.org/pdf/2507.20115", "abs": "https://arxiv.org/abs/2507.20115", "authors": ["Gongli Xi", "Ye Tian", "Yannan Hu", "Yuchao Zhang", "Yapeng Niu", "Xiangyang Gong"], "title": "Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion", "categories": ["cs.NI", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In response to Distributed Denial of Service (DDoS) attacks, recent research\nefforts increasingly rely on Machine Learning (ML)-based solutions, whose\neffectiveness largely depends on the quality of labeled training datasets. To\naddress the scarcity of such datasets, data augmentation with synthetic traces\nis often employed. However, current synthetic trace generation methods struggle\nto capture the complex temporal patterns and spatial distributions exhibited in\nemerging DDoS attacks. This results in insufficient resemblance to real traces\nand unsatisfied detection accuracy when applied to ML tasks. In this paper, we\npropose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,\nmulti-stream network traffic generative model based on diffusion models,\nfeaturing two main streams: The field stream utilizes spatial mapping to bridge\nnetwork data characteristics with pre-trained realms of stable diffusion\nmodels, effectively translating complex network interactions into formats that\nstable diffusion can process, while the spatial stream adopts a dynamic\ntemporal modeling approach, meticulously capturing the intrinsic temporal\npatterns of network traffic. Extensive experiments demonstrate that data\ngenerated by our model exhibits higher statistical similarity to originals\ncompared to current state-of-the-art solutions, and enhance performances on a\nwide range of downstream tasks."}
{"id": "2507.20118", "pdf": "https://arxiv.org/pdf/2507.20118", "abs": "https://arxiv.org/abs/2507.20118", "authors": ["Taoyong Cui", "Zhongyao Wang", "Dongzhan Zhou", "Yuqiang Li", "Lei Bai", "Wanli Ouyang", "Mao Su", "Shufei Zhang"], "title": "Iterative Pretraining Framework for Interatomic Potentials", "categories": ["physics.comp-ph", "cs.AI"], "comment": null, "summary": "Machine learning interatomic potentials (MLIPs) enable efficient molecular\ndynamics (MD) simulations with ab initio accuracy and have been applied across\nvarious domains in physical science. However, their performance often relies on\nlarge-scale labeled training data. While existing pretraining strategies can\nimprove model performance, they often suffer from a mismatch between the\nobjectives of pretraining and downstream tasks or rely on extensive labeled\ndatasets and increasingly complex architectures to achieve broad\ngeneralization. To address these challenges, we propose Iterative Pretraining\nfor Interatomic Potentials (IPIP), a framework designed to iteratively improve\nthe predictive performance of MLIP models. IPIP incorporates a forgetting\nmechanism to prevent iterative training from converging to suboptimal local\nminima. Unlike general-purpose foundation models, which frequently underperform\non specialized tasks due to a trade-off between generality and system-specific\naccuracy, IPIP achieves higher accuracy and efficiency using lightweight\narchitectures. Compared to general-purpose force fields, this approach achieves\nover 80% reduction in prediction error and up to 4x speedup in the challenging\nMo-S-O system, enabling fast and accurate simulations."}
{"id": "2507.20127", "pdf": "https://arxiv.org/pdf/2507.20127", "abs": "https://arxiv.org/abs/2507.20127", "authors": ["Xuanting Xie", "Bingheng Li", "Erlin Pan", "Zhao Kang", "Wenyu Chen"], "title": "Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing", "categories": ["cs.LG", "cs.AI", "cs.GR"], "comment": "11 pages, 6 figures", "summary": "Graph Neural Networks (GNNs) have become a dominant approach to learning\ngraph representations, primarily because of their message-passing mechanisms.\nHowever, GNNs typically adopt a fixed aggregator function such as Mean, Max, or\nSum without principled reasoning behind the selection. This rigidity,\nespecially in the presence of heterophily, often leads to poor, problem\ndependent performance. Although some attempts address this by designing more\nsophisticated aggregation functions, these methods tend to rely heavily on\nlabeled data, which is often scarce in real-world tasks. In this work, we\npropose a novel unsupervised framework, \"Aggregation-aware Multilayer\nPerceptron\" (AMLP), which shifts the paradigm from directly crafting\naggregation functions to making MLP adaptive to aggregation. Our lightweight\napproach consists of two key steps: First, we utilize a graph reconstruction\nmethod that facilitates high-order grouping effects, and second, we employ a\nsingle-layer network to encode varying degrees of heterophily, thereby\nimproving the capacity and applicability of the model. Extensive experiments on\nnode clustering and classification demonstrate the superior performance of\nAMLP, highlighting its potential for diverse graph learning scenarios."}
{"id": "2507.20133", "pdf": "https://arxiv.org/pdf/2507.20133", "abs": "https://arxiv.org/abs/2507.20133", "authors": ["Anas Mohamed", "Azal Ahmad Khan", "Xinran Wang", "Ahmad Faraz Khan", "Shuwen Ge", "Saman Bahzad Khan", "Ayaan Ahmad", "Ali Anwar"], "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative AI can now synthesize strikingly realistic images from text, yet\noutput quality remains highly sensitive to how prompts are phrased. Direct\nPreference Optimization (DPO) offers a lightweight, off-policy alternative to\nRL for automatic prompt engineering, but its token-level regularization leaves\nsemantic inconsistency unchecked as prompts that win higher preference scores\ncan still drift away from the user's intended meaning.\n  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency\nyet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an\nexponential weight proportional to the cosine distance between the original\nprompt and winning candidate in embedding space, softly down-weighting training\nsignals that would otherwise reward semantically mismatched prompts. We provide\nthe first analytical bound on semantic drift for preference-tuned prompt\ngenerators, showing that Sem-DPO keeps learned prompts within a provably\nbounded neighborhood of the original text. On three standard text-to-image\nprompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%\nhigher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,\nPickScore) than DPO, while also outperforming state-of-the-art baselines. These\nfindings suggest that strong flat baselines augmented with semantic weighting\nshould become the new standard for prompt-optimization studies and lay the\ngroundwork for broader, semantics-aware preference optimization in language\nmodels."}
{"id": "2507.20136", "pdf": "https://arxiv.org/pdf/2507.20136", "abs": "https://arxiv.org/abs/2507.20136", "authors": ["Baiyu Chen", "Wilson Wongso", "Xiaoqian Hu", "Yue Tan", "Flora Salim"], "title": "Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD Cup 2025 Meta CRAG-MM Challenge", "summary": "This paper presents the technical solution developed by team CRUISE for the\nKDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn\n(CRAG-MM) challenge. The challenge aims to address a critical limitation of\nmodern Vision Language Models (VLMs): their propensity to hallucinate,\nespecially when faced with egocentric imagery, long-tail entities, and complex,\nmulti-hop questions. This issue is particularly problematic in real-world\napplications where users pose fact-seeking queries that demand high factual\naccuracy across diverse modalities. To tackle this, we propose a robust,\nmulti-stage framework that prioritizes factual accuracy and truthfulness over\ncompleteness. Our solution integrates a lightweight query router for\nefficiency, a query-aware retrieval and summarization pipeline, a dual-pathways\ngeneration and a post-hoc verification. This conservative strategy is designed\nto minimize hallucinations, which incur a severe penalty in the competition's\nscoring metric. Our approach achieved 3rd place in Task 1, demonstrating the\neffectiveness of prioritizing answer reliability in complex multi-modal RAG\nsystems. Our implementation is available at\nhttps://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM ."}
{"id": "2507.20140", "pdf": "https://arxiv.org/pdf/2507.20140", "abs": "https://arxiv.org/abs/2507.20140", "authors": ["Taesoo Kim", "Jinju Kim", "Dongchan Kim", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025), Vancouver, Canada. PMLR 267, 2025. Authors Jinju Kim and Taesoo\n  Kim contributed equally", "summary": "The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has\nenabled high-fidelity voice synthesis from minimal audio cues, raising\nsignificant privacy and ethical concerns. Despite the threats to voice privacy,\nresearch to selectively remove the knowledge to replicate unwanted individual\nvoices from pre-trained model parameters has not been explored. In this paper,\nwe address the new challenge of speaker identity unlearning for ZS-TTS systems.\nTo meet this goal, we propose the first machine unlearning frameworks for\nZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the\nmodel forgets designated speaker identities while retaining its ability to\ngenerate accurate speech for other speakers. Our proposed methods incorporate\nrandomness to prevent consistent replication of forget speakers' voices,\nassuring unlearned identities remain untraceable. Additionally, we propose a\nnew evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses\nthe model's ability to disregard prompts associated with forgotten speakers,\neffectively neutralizing its knowledge of these voices. The experiments\nconducted on the state-of-the-art model demonstrate that TGU prevents the model\nfrom replicating forget speakers' voices while maintaining high quality for\nother speakers. The demo is available at https://speechunlearn.github.io/"}
{"id": "2507.20144", "pdf": "https://arxiv.org/pdf/2507.20144", "abs": "https://arxiv.org/abs/2507.20144", "authors": ["Zeyi Liu", "Songqiao Hu", "Pengyu Han", "Jiaming Liu", "Xiao He"], "title": "Awesome-OL: An Extensible Toolkit for Online Learning", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages", "summary": "In recent years, online learning has attracted increasing attention due to\nits adaptive capability to process streaming and non-stationary data. To\nfacilitate algorithm development and practical deployment in this area, we\nintroduce Awesome-OL, an extensible Python toolkit tailored for online learning\nresearch. Awesome-OL integrates state-of-the-art algorithm, which provides a\nunified framework for reproducible comparisons, curated benchmark datasets, and\nmulti-modal visualization. Built upon the scikit-multiflow open-source\ninfrastructure, Awesome-OL emphasizes user-friendly interactions without\ncompromising research flexibility or extensibility. The source code is publicly\navailable at: https://github.com/liuzy0708/Awesome-OL."}
{"id": "2507.20145", "pdf": "https://arxiv.org/pdf/2507.20145", "abs": "https://arxiv.org/abs/2507.20145", "authors": ["Kesen Wang", "Daulet Toibazar", "Abdulrahman Alfulayt", "Abdulaziz S. Albadawi", "Ranya A. Alkahtani", "Asma A. Ibrahim", "Haneen A. Alhomoud", "Sherif Mohamed", "Pedro J. Moreno"], "title": "Multi-Agent Interactive Question Generation Framework for Long Document Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Document Understanding (DU) in long-contextual scenarios with complex layouts\nremains a significant challenge in vision-language research. Although Large\nVision-Language Models (LVLMs) excel at short-context DU tasks, their\nperformance declines in long-context settings. A key limitation is the scarcity\nof fine-grained training data, particularly for low-resource languages such as\nArabic. Existing state-of-the-art techniques rely heavily on human annotation,\nwhich is costly and inefficient. We propose a fully automated, multi-agent\ninteractive framework to generate long-context questions efficiently. Our\napproach efficiently generates high-quality single- and multi-page questions\nfor extensive English and Arabic documents, covering hundreds of pages across\ndiverse domains. This facilitates the development of LVLMs with enhanced\nlong-context understanding ability. Experimental results in this work have\nshown that our generated English and Arabic questions\n(\\textbf{AraEngLongBench}) are quite challenging to major open- and\nclose-source LVLMs. The code and data proposed in this work can be found in\nhttps://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and\nAnswer (QA) pairs and structured system prompts can be found in the Appendix."}
{"id": "2507.20152", "pdf": "https://arxiv.org/pdf/2507.20152", "abs": "https://arxiv.org/abs/2507.20152", "authors": ["Shuhaib Mehri", "Xiaocheng Yang", "Takyoung Kim", "Gokhan Tur", "Shikib Mehri", "Dilek Hakkani-Tür"], "title": "Goal Alignment in LLM-Based User Simulators for Conversational AI", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large\nLanguage Models (LLMs) have advanced user simulation capabilities, we reveal\nthat they struggle to consistently demonstrate goal-oriented behavior across\nmulti-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking\n(UGST), a novel framework that tracks user goal progression throughout\nconversations. Leveraging UGST, we present a three-stage methodology for\ndeveloping user simulators that can autonomously track goal progression and\nreason to generate goal-aligned responses. Moreover, we establish comprehensive\nevaluation metrics for measuring goal alignment in user simulators, and\ndemonstrate that our approach yields substantial improvements across two\nbenchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework\nfor developing goal-aligned user simulators."}
{"id": "2507.20156", "pdf": "https://arxiv.org/pdf/2507.20156", "abs": "https://arxiv.org/abs/2507.20156", "authors": ["Daulet Toibazar", "Kesen Wang", "Sherif Mohamed", "Abdulaziz Al-Badawi", "Abdulrahman Alfulayt", "Pedro J. Moreno"], "title": "Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) extend the conventional large language models\nby integrating visual data, enabling richer multimodal reasoning and\nsignificantly broadens the practical applications of AI. However, including\nvisual inputs also brings new challenges in maintaining data quality. Empirical\nevidence consistently shows that carefully curated and representative training\nexamples often yield superior results compared to simply increasing the\nquantity of data. Inspired by this observation, we introduce a streamlined data\nfiltration framework that employs a compact VLM, fine-tuned on a high-quality\nimage-caption annotated dataset. This model effectively evaluates and filters\npotential training samples based on caption and image quality and alignment.\nUnlike previous approaches, which typically add auxiliary filtration modules on\ntop of existing full-scale VLMs, our method exclusively utilizes the inherent\nevaluative capability of a purpose-built small VLM. This strategy eliminates\nthe need for extra modules and reduces training overhead. Our lightweight model\nefficiently filters out inaccurate, noisy web data, improving image-text\nalignment and caption linguistic fluency. Experimental results show that\ndatasets underwent high-precision filtration using our compact VLM perform on\npar with, or even surpass, larger and noisier datasets gathered through\nhigh-volume web crawling. Thus, our method provides a lightweight yet robust\nsolution for building high-quality vision-language training corpora. \\\\\n\\textbf{Availability and implementation:} Our compact VLM filtration model,\ntraining data, utility scripts, and Supplementary data (Appendices) are freely\navailable at https://github.com/daulettoibazar/Compact_VLM_Filter."}
{"id": "2507.20164", "pdf": "https://arxiv.org/pdf/2507.20164", "abs": "https://arxiv.org/abs/2507.20164", "authors": ["Jinwook Hong"], "title": "ASNN: Learning to Suggest Neural Architectures from Performance Distributions", "categories": ["cs.LG", "cs.AI", "68T05, 68T07, 62M45"], "comment": "10 pages", "summary": "The architecture of a neural network (NN) plays a critical role in\ndetermining its performance. However, there is no general closed-form function\nthat maps between network structure and accuracy, making the process of\narchitecture design largely heuristic or search-based. In this study, we\npropose the Architecture Suggesting Neural Network (ASNN), a model designed to\nlearn the relationship between NN architecture and its test accuracy, and to\nsuggest improved architectures accordingly. To train ASNN, we constructed\ndatasets using TensorFlow-based models with varying numbers of layers and\nnodes. Experimental results were collected for both 2-layer and 3-layer\narchitectures across a grid of configurations, each evaluated with 10 repeated\ntrials to account for stochasticity. Accuracy values were treated as inputs,\nand architectural parameters as outputs. The trained ASNN was then used\niteratively to predict architectures that yield higher performance. In both\n2-layer and 3-layer cases, ASNN successfully suggested architectures that\noutperformed the best results found in the original training data. Repeated\nprediction and retraining cycles led to the discovery of architectures with\nimproved mean test accuracies, demonstrating the model's capacity to generalize\nthe performance-structure relationship. These results suggest that ASNN\nprovides an efficient alternative to random search for architecture\noptimization, and offers a promising approach toward automating neural network\ndesign. \"Parts of the manuscript, including text editing and expression\nrefinement, were supported by OpenAI's ChatGPT. All content was reviewed and\nverified by the authors.\""}
{"id": "2507.20173", "pdf": "https://arxiv.org/pdf/2507.20173", "abs": "https://arxiv.org/abs/2507.20173", "authors": ["Haitian Wang", "Long Qin"], "title": "High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."}
{"id": "2507.20174", "pdf": "https://arxiv.org/pdf/2507.20174", "abs": "https://arxiv.org/abs/2507.20174", "authors": ["Fei Kong", "Jinhao Duan", "Kaidi Xu", "Zhenhua Guo", "Xiaofeng Zhu", "Xiaoshuang Shi"], "title": "LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real-world applications, such as autonomous driving and humanoid robot\nmanipulation, require precise spatial perception. However, it remains\nunderexplored how Vision-Language Models (VLMs) recognize spatial relationships\nand perceive spatial movement. In this work, we introduce a spatial evaluation\npipeline and construct a corresponding benchmark. Specifically, we categorize\nspatial understanding into two main types: absolute spatial understanding,\nwhich involves querying the absolute spatial position (e.g., left, right) of an\nobject within an image, and 3D spatial understanding, which includes movement\nand rotation. Notably, our dataset is entirely synthetic, enabling the\ngeneration of test samples at a low cost while also preventing dataset\ncontamination. We conduct experiments on multiple state-of-the-art VLMs and\nobserve that there is significant room for improvement in their spatial\nunderstanding abilities. Explicitly, in our experiments, humans achieve\nnear-perfect performance on all tasks, whereas current VLMs attain human-level\nperformance only on the two simplest tasks. For the remaining tasks, the\nperformance of VLMs is distinctly lower than that of humans. In fact, the\nbest-performing Vision-Language Models even achieve near-zero scores on\nmultiple tasks. The dataset and code are available on\nhttps://github.com/kong13661/LRR-Bench."}
{"id": "2507.20181", "pdf": "https://arxiv.org/pdf/2507.20181", "abs": "https://arxiv.org/abs/2507.20181", "authors": ["Hyeonji Lee", "Daejin Jo", "Seohwan Yun", "Sungwoong Kim"], "title": "SGPO: Self-Generated Preference Optimization based on Self-Improver", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their extensive pretraining on diverse\ndatasets, require effective alignment to human preferences for practical and\nreliable deployment. Conventional alignment methods typically employ off-policy\nlearning and depend on human-annotated datasets, which limits their broad\napplicability and introduces distribution shift issues during training. To\naddress these challenges, we propose Self-Generated Preference Optimization\nbased on Self-Improver (SGPO), an innovative alignment framework that leverages\nan on-policy self-improving mechanism. Specifically, the improver refines\nresponses from a policy model to self-generate preference data for direct\npreference optimization (DPO) of the policy model. Here, the improver and\npolicy are unified into a single model, and in order to generate higher-quality\npreference data, this self-improver learns to make incremental yet discernible\nimprovements to the current responses by referencing supervised fine-tuning\noutputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the\nproposed SGPO significantly improves performance over DPO and baseline\nself-improving methods without using external preference data."}
{"id": "2507.20189", "pdf": "https://arxiv.org/pdf/2507.20189", "abs": "https://arxiv.org/abs/2507.20189", "authors": ["Chengkai Wang", "Di Wu", "Yunsheng Liao", "Wenyao Zheng", "Ziyi Zeng", "Xurong Gao", "Hemmings Wu", "Zhoule Zhu", "Jie Yang", "Lihua Zhong", "Weiwei Cheng", "Yun-Hsuan Chen", "Mohamad Sawan"], "title": "NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Methamphetamine dependence poses a significant global health challenge, yet\nits assessment and the evaluation of treatments like repetitive transcranial\nmagnetic stimulation (rTMS) frequently depend on subjective self-reports, which\nmay introduce uncertainties. While objective neuroimaging modalities such as\nelectroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)\noffer alternatives, their individual limitations and the reliance on\nconventional, often hand-crafted, feature extraction can compromise the\nreliability of derived biomarkers. To overcome these limitations, we propose\nNeuroCLIP, a novel deep learning framework integrating simultaneously recorded\nEEG and fNIRS data through a progressive learning strategy. This approach\noffers a robust and trustworthy biomarker for methamphetamine addiction.\nValidation experiments show that NeuroCLIP significantly improves\ndiscriminative capabilities among the methamphetamine-dependent individuals and\nhealthy controls compared to models using either EEG or only fNIRS alone.\nFurthermore, the proposed framework facilitates objective, brain-based\nevaluation of rTMS treatment efficacy, demonstrating measurable shifts in\nneural patterns towards healthy control profiles after treatment. Critically,\nwe establish the trustworthiness of the multimodal data-driven biomarker by\nshowing its strong correlation with psychometrically validated craving scores.\nThese findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP\noffers enhanced robustness and reliability over single-modality approaches,\nproviding a valuable tool for addiction neuroscience research and potentially\nimproving clinical assessments."}
{"id": "2507.20191", "pdf": "https://arxiv.org/pdf/2507.20191", "abs": "https://arxiv.org/abs/2507.20191", "authors": ["Cheng-Jun Guo", "Chuan-Xian Ren", "You-Wei Luo", "Xiao-Lin Xu", "Hong Yan"], "title": "Partial Domain Adaptation via Importance Sampling-based Shift Correction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial domain adaptation (PDA) is a challenging task in real-world machine\nlearning scenarios. It aims to transfer knowledge from a labeled source domain\nto a related unlabeled target domain, where the support set of the source label\ndistribution subsumes the target one. Previous PDA works managed to correct the\nlabel distribution shift by weighting samples in the source domain. However,\nthe simple reweighing technique cannot explore the latent structure and\nsufficiently use the labeled data, and then models are prone to over-fitting on\nthe source domain. In this work, we propose a novel importance sampling-based\nshift correction (IS$^2$C) method, where new labeled data are sampled from a\nbuilt sampling domain, whose label distribution is supposed to be the same as\nthe target domain, to characterize the latent structure and enhance the\ngeneralization ability of the model. We provide theoretical guarantees for\nIS$^2$C by proving that the generalization error can be sufficiently dominated\nby IS$^2$C. In particular, by implementing sampling with the mixture\ndistribution, the extent of shift between source and sampling domains can be\nconnected to generalization error, which provides an interpretable way to build\nIS$^2$C. To improve knowledge transfer, an optimal transport-based independence\ncriterion is proposed for conditional distribution alignment, where the\ncomputation of the criterion can be adjusted to reduce the complexity from\n$\\mathcal{O}(n^3)$ to $\\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive\nexperiments on PDA benchmarks validate the theoretical results and demonstrate\nthe effectiveness of our IS$^2$C over existing methods."}
{"id": "2507.20197", "pdf": "https://arxiv.org/pdf/2507.20197", "abs": "https://arxiv.org/abs/2507.20197", "authors": ["Fabrizio Nunnari", "Alakshendra Jyotsnaditya Ramkrishna Singh", "Patrick Gebhard"], "title": "Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of this investigation is to quantify to what extent computer vision\nmethods can correctly classify facial expressions on a sign language dataset.\nWe extend our experiments by recognizing expressions using only the upper or\nlower part of the face, which is needed to further investigate the difference\nin emotion manifestation between hearing and deaf subjects. To take into\naccount the peculiar color profile of a dataset, our method introduces a color\nnormalization stage based on histogram equalization and fine-tuning. The\nresults show the ability to correctly recognize facial expressions with 83.8%\nmean sensitivity and very little variance (.042) among classes. Like for\nhumans, recognition of expressions from the lower half of the face (79.6%) is\nhigher than that from the upper half (77.9%). Noticeably, the classification\naccuracy from the upper half of the face is higher than human level."}
{"id": "2507.20217", "pdf": "https://arxiv.org/pdf/2507.20217", "abs": "https://arxiv.org/abs/2507.20217", "authors": ["Wei Cui", "Haoyu Wang", "Wenkang Qin", "Yijie Guo", "Gang Han", "Wen Zhao", "Jiahang Cao", "Zhang Zhang", "Jiaru Zhong", "Jingkai Sun", "Pihai Sun", "Shuai Shi", "Botuo Jiang", "Jiahao Ma", "Jiaxu Wang", "Hao Cheng", "Zhichao Liu", "Yang Wang", "Zheng Zhu", "Guan Huang", "Jian Tang", "Qiang Zhang"], "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Tech Report", "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios."}
{"id": "2507.20221", "pdf": "https://arxiv.org/pdf/2507.20221", "abs": "https://arxiv.org/abs/2507.20221", "authors": ["Uzzal Saha", "Surya Prakash"], "title": "Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "26 pages, 14 figures", "summary": "In this work, we address the challenge of binary lung nodule classification\n(benign vs malignant) using CT images by proposing a multi-level attention\nstacked ensemble of deep neural networks. Three pretrained backbones --\nEfficientNet V2 S, MobileViT XXS, and DenseNet201 -- are each adapted with a\ncustom classification head tailored to 96 x 96 pixel inputs. A two-stage\nattention mechanism learns both model-wise and class-wise importance scores\nfrom concatenated logits, and a lightweight meta-learner refines the final\nprediction. To mitigate class imbalance and improve generalization, we employ\ndynamic focal loss with empirically calculated class weights, MixUp\naugmentation during training, and test-time augmentation at inference.\nExperiments on the LIDC-IDRI dataset demonstrate exceptional performance,\nachieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in\nerror rate compared to state-of-the-art methods. The model exhibits balanced\nperformance across sensitivity (98.73) and specificity (98.96), with\nparticularly strong results on challenging cases where radiologist disagreement\nwas high. Statistical significance testing confirms the robustness of these\nimprovements across multiple experimental runs. Our approach can serve as a\nrobust, automated aid for radiologists in lung cancer screening."}
{"id": "2507.20243", "pdf": "https://arxiv.org/pdf/2507.20243", "abs": "https://arxiv.org/abs/2507.20243", "authors": ["Lang Yu", "Zhangyang Gao", "Cheng Tan", "Qin Chen", "Jie Zhou", "Liang He"], "title": "Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "SE(3)-based generative models have shown great promise in protein geometry\nmodeling and effective structure design. However, the field currently lacks a\nmodularized benchmark to enable comprehensive investigation and fair comparison\nof different methods. In this paper, we propose Protein-SE(3), a new benchmark\nbased on a unified training framework, which comprises protein scaffolding\ntasks, integrated generative models, high-level mathematical abstraction, and\ndiverse evaluation metrics. Recent advanced generative models designed for\nprotein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),\nScore Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and\nFrameFlow) are integrated into our framework. All integrated methods are fairly\ninvestigated with the same training dataset and evaluation metrics.\nFurthermore, we provide a high-level abstraction of the mathematical\nfoundations behind the generative models, enabling fast prototyping of future\nalgorithms without reliance on explicit protein structures. Accordingly, we\nrelease the first comprehensive benchmark built upon unified training framework\nfor SE(3)-based protein structure design, which is publicly accessible at\nhttps://github.com/BruthYU/protein-se3."}
{"id": "2507.20252", "pdf": "https://arxiv.org/pdf/2507.20252", "abs": "https://arxiv.org/abs/2507.20252", "authors": ["Xiang Fei", "Siqi Wang", "Shu Wei", "Yuxiang Nie", "Wei Shi", "Hao Feng", "Can Huang"], "title": "Post-Completion Learning for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>}) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency."}
{"id": "2507.20263", "pdf": "https://arxiv.org/pdf/2507.20263", "abs": "https://arxiv.org/abs/2507.20263", "authors": ["Junjie Zhao", "Chengxi Zhang", "Chenkai Wang", "Peng Yang"], "title": "Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining", "categories": ["cs.LG", "cs.AI", "q-fin.PM"], "comment": null, "summary": "Reinforcement learning (RL) has successfully automated the complex process of\nmining formulaic alpha factors, for creating interpretable and profitable\ninvestment strategies. However, existing methods are hampered by the sparse\nrewards given the underlying Markov Decision Process. This inefficiency limits\nthe exploration of the vast symbolic search space and destabilizes the training\nprocess. To address this, Trajectory-level Reward Shaping (TLRS), a novel\nreward shaping method, is proposed. TLRS provides dense, intermediate rewards\nby measuring the subsequence-level similarity between partially generated\nexpressions and a set of expert-designed formulas. Furthermore, a reward\ncentering mechanism is introduced to reduce training variance. Extensive\nexperiments on six major Chinese and U.S. stock indices show that TLRS\nsignificantly improves the predictive power of mined factors, boosting the Rank\nInformation Coefficient by 9.29% over existing potential-based shaping\nalgorithms. Notably, TLRS achieves a major leap in computational efficiency by\nreducing its time complexity with respect to the feature dimension from linear\nto constant, which is a significant improvement over distance-based baselines."}
{"id": "2507.20295", "pdf": "https://arxiv.org/pdf/2507.20295", "abs": "https://arxiv.org/abs/2507.20295", "authors": ["Tatsuro Hanyu", "Takahiro Katagiri", "Daichi Mukunoki", "Tetsuya Hoshino"], "title": "Towards Generalized Parameter Tuning in Coherent Ising Machines: A Portfolio-Based Approach", "categories": ["cs.PF", "cs.AI", "cs.LG"], "comment": null, "summary": "Coherent Ising Machines (CIMs) have recently gained attention as a promising\ncomputing model for solving combinatorial optimization problems. In particular,\nthe Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution\nquality, but its performance is highly sensitive to a large number of\nhyperparameters, making efficient tuning essential. In this study, we present\nan algorithm portfolio approach for hyperparameter tuning in CIMs employing\nChaotic Amplitude Control with momentum (CACm) algorithm. Our method\nincorporates multiple search strategies, enabling flexible and effective\nadaptation to the characteristics of the hyperparameter space. Specifically, we\npropose two representative tuning methods, Method A and Method B. Method A\noptimizes each hyperparameter sequentially with a fixed total number of trials,\nwhile Method B prioritizes hyperparameters based on initial evaluations before\napplying Method A in order. Performance evaluations were conducted on the\nSupercomputer \"Flow\" at Nagoya University, using planted Wishart instances and\nTime to Solution (TTS) as the evaluation metric. Compared to the baseline\nperformance with best-known hyperparameters, Method A achieved up to 1.47x\nimprovement, and Method B achieved up to 1.65x improvement. These results\ndemonstrate the effectiveness of the algorithm portfolio approach in enhancing\nthe tuning process for CIMs."}
{"id": "2507.20312", "pdf": "https://arxiv.org/pdf/2507.20312", "abs": "https://arxiv.org/abs/2507.20312", "authors": ["Jonas H. Müller Korndörfer", "Ali Mohammed", "Ahmed Eleliemy", "Quentin Guilloteau", "Reto Krummenacher", "Florina M. Ciorba"], "title": "A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": "To appear at IEEE ACCESS", "summary": "Scientific and data science applications are becoming increasingly complex,\nwith growing computational and memory demands. Modern high performance\ncomputing (HPC) systems provide high parallelism and heterogeneity across\nnodes, devices, and cores. To achieve good performance, effective scheduling\nand load balancing techniques are essential. Parallel programming frameworks\nsuch as OpenMP now offer a variety of advanced scheduling algorithms to support\ndiverse applications and platforms. This creates an instance of the scheduling\nalgorithm selection problem, which involves identifying the most suitable\nalgorithm for a given combination of workload and system characteristics.\n  In this work, we explore learning-based approaches for selecting scheduling\nalgorithms in OpenMP. We propose and evaluate expert-based and reinforcement\nlearning (RL)-based methods, and conduct a detailed performance analysis across\nsix applications and three systems. Our results show that RL methods are\ncapable of learning high-performing scheduling decisions, although they require\nsignificant exploration, with the choice of reward function playing a key role.\nExpert-based methods, in contrast, rely on prior knowledge and involve less\nexploration, though they may not always identify the optimal algorithm for a\nspecific application-system pair. By combining expert knowledge with RL-based\nlearning, we achieve improved performance and greater adaptability.\n  Overall, this work demonstrates that dynamic selection of scheduling\nalgorithms during execution is both viable and beneficial for OpenMP\napplications. The approach can also be extended to MPI-based programs, enabling\noptimization of scheduling decisions across multiple levels of parallelism."}
{"id": "2507.20326", "pdf": "https://arxiv.org/pdf/2507.20326", "abs": "https://arxiv.org/abs/2507.20326", "authors": ["Jiaxi Wang", "Yaosen Min", "Xun Zhu", "Miao Li", "Ji Wu"], "title": "MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 8 figures, accepted by ACM Multimedia 2025 (oral)", "summary": "Polymers, composed of repeating structural units called monomers, are\nfundamental materials in daily life and industry. Accurate property prediction\nfor polymers is essential for their design, development, and application.\nHowever, existing modeling approaches, which typically represent polymers by\nthe constituent monomers, struggle to capture the whole properties of polymer,\nsince the properties change during the polymerization process. In this study,\nwe propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training\nframework, which represents polymers as infinite sequences of monomers and\nintegrates both topological and spatial information for comprehensive modeling.\nFrom the topological perspective, we generalize message passing mechanism (MPM)\nand graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we\ndemonstrate that applying MPM to infinite polymer sequences is equivalent to\napplying MPM on the induced star-linking graph of monomers. For GAM, we propose\nto further replace global graph attention with localized graph attention (LGA).\nMoreover, we show the robustness of the \"star linking\" strategy through Repeat\nand Shift Invariance Test (RSIT). Despite its robustness, \"star linking\"\nstrategy exhibits limitations when monomer side chains contain ring structures,\na common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)\ntest. To overcome this issue, we propose backbone embedding to enhance the\ncapability of MPM and LGA on infinite polymer sequences. From the spatial\nperspective, we extract 3D descriptors of repeating monomers to capture spatial\ninformation. Finally, we design a cross-modal fusion mechanism to unify the\ntopological and spatial information. Experimental validation across eight\ndiverse polymer property prediction tasks reveals that MIPS achieves\nstate-of-the-art performance."}
{"id": "2507.20335", "pdf": "https://arxiv.org/pdf/2507.20335", "abs": "https://arxiv.org/abs/2507.20335", "authors": ["Siyu Song", "Wentao Liu", "Ye Lu", "Ruohua Zhang", "Tao Liu", "Jinze Lv", "Xinyun Wang", "Aimin Zhou", "Fei Tan", "Bo Jiang", "Hao Hao"], "title": "Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into education presents\nunprecedented opportunities for scalable personalized learning. However,\nstandard LLMs often function as generic information providers, lacking\nalignment with fundamental pedagogical principles such as helpfulness,\nstudent-centered personalization, and creativity cultivation. To bridge this\ngap, we propose EduAlign, a novel framework designed to guide LLMs toward\nbecoming more effective and responsible educational assistants. EduAlign\nconsists of two main stages. In the first stage, we curate a dataset of 8k\neducational interactions and annotate them-both manually and\nautomatically-along three key educational dimensions: Helpfulness,\nPersonalization, and Creativity (HPC). These annotations are used to train\nHPC-RM, a multi-dimensional reward model capable of accurately scoring LLM\noutputs according to these educational principles. We further evaluate the\nconsistency and reliability of this reward model. In the second stage, we\nleverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group\nRelative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then\nassess the pre- and post-finetuning models on both educational and\ngeneral-domain benchmarks across the three HPC dimensions. Experimental results\ndemonstrate that the fine-tuned model exhibits significantly improved alignment\nwith pedagogical helpfulness, personalization, and creativity stimulation. This\nstudy presents a scalable and effective approach to aligning LLMs with nuanced\nand desirable educational traits, paving the way for the development of more\nengaging, pedagogically aligned AI tutors."}
{"id": "2507.20353", "pdf": "https://arxiv.org/pdf/2507.20353", "abs": "https://arxiv.org/abs/2507.20353", "authors": ["Qian Qi"], "title": "A Theory of $θ$-Expectations", "categories": ["math.PR", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "The canonical theory of stochastic calculus under ambiguity, founded on\nsub-additivity, is insensitive to non-convex uncertainty structures, leading to\nan identifiability impasse. This paper develops a mathematical framework for an\nidentifiable calculus sensitive to non-convex geometry. We introduce the\n$\\theta$-BSDE, a class of backward stochastic differential equations where the\ndriver is determined by a pointwise maximization over a primitive, possibly\nnon-convex, uncertainty set. The system's tractability is predicated not on\nconvexity, but on a global analytic hypothesis: the existence of a unique and\nglobally Lipschitz maximizer map for the driver function. Under this\nhypothesis, which carves out a tractable class of models, we establish\nwell-posedness via a fixed-point argument. For a distinct, geometrically\nregular class of models, we prove a result of independent interest: under\nnon-degeneracy conditions from Malliavin calculus, the maximizer is unique\nalong any solution path, ensuring the model's internal consistency. We clarify\nthe fundamental logical gap between this pathwise property and the global\nregularity required by our existence proof. The resulting valuation operator\ndefines a dynamically consistent expectation, and we establish its connection\nto fully nonlinear PDEs via a Feynman-Kac formula."}
{"id": "2507.20369", "pdf": "https://arxiv.org/pdf/2507.20369", "abs": "https://arxiv.org/abs/2507.20369", "authors": ["Ahmed Shokry", "Ayman Khalafallah"], "title": "Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Clustering is a core task in machine learning with wide-ranging applications\nin data mining and pattern recognition. However, its unsupervised nature makes\nit inherently challenging. Many existing clustering algorithms suffer from\ncritical limitations: they often require careful parameter tuning, exhibit high\ncomputational complexity, lack interpretability, or yield suboptimal accuracy,\nespecially when applied to large-scale datasets. In this paper, we introduce a\nnovel clustering approach based on meta-learning. Our approach eliminates the\nneed for parameter optimization while achieving accuracy that outperforms\nstate-of-the-art clustering techniques. The proposed technique leverages a few\npre-clustered samples to guide the clustering process for the entire dataset in\na single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted\nTransformer Network (PFN) to perform clustering. The algorithm computes\nattention between the pre-clustered samples and the unclustered samples,\nallowing it to infer cluster assignments for the entire dataset based on the\nlearned relation. We theoretically and empirically demonstrate that, given just\na few pre-clustered examples, the model can generalize to accurately cluster\nthe rest of the dataset. Experiments on challenging benchmark datasets show\nthat our approach can successfully cluster well-separated data without any\npre-clustered samples, and significantly improves performance when a few\nclustered samples are provided. We show that our approach is superior to the\nstate-of-the-art techniques. These results highlight the effectiveness and\nscalability of our approach, positioning it as a promising alternative to\nexisting clustering techniques."}
{"id": "2507.20373", "pdf": "https://arxiv.org/pdf/2507.20373", "abs": "https://arxiv.org/abs/2507.20373", "authors": ["Kiymet Kaya", "Elif Ak", "Sule Gunduz Oguducu"], "title": "WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose the Wasserstein Black Hole Transformer (WBHT) framework for\ndetecting black hole (BH) anomalies in communication networks. These anomalies\ncause packet loss without failure notifications, disrupting connectivity and\nleading to financial losses. WBHT combines generative modeling, sequential\nlearning, and attention mechanisms to improve BH anomaly detection. It\nintegrates a Wasserstein generative adversarial network with attention\nmechanisms for stable training and accurate anomaly identification. The model\nuses long-short-term memory layers to capture long-term dependencies and\nconvolutional layers for local temporal patterns. A latent space encoding\nmechanism helps distinguish abnormal network behavior. Tested on real-world\nnetwork data, WBHT outperforms existing models, achieving significant\nimprovements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and\nability to detect previously undetected anomalies make it a valuable tool for\nproactive network monitoring and security, especially in mission-critical\nnetworks."}
{"id": "2507.20389", "pdf": "https://arxiv.org/pdf/2507.20389", "abs": "https://arxiv.org/abs/2507.20389", "authors": ["Naveen Mathews Renji", "Kruthika K", "Manasa Keshavamurthy", "Pooja Kumari", "S. Rajarajeswari"], "title": "Solving Scene Understanding for Autonomous Navigation in Unstructured Environments", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous vehicles are the next revolution in the automobile industry and\nthey are expected to revolutionize the future of transportation. Understanding\nthe scenario in which the autonomous vehicle will operate is critical for its\ncompetent functioning. Deep Learning has played a massive role in the progress\nthat has been made till date. Semantic Segmentation, the process of annotating\nevery pixel of an image with an object class, is one crucial part of this scene\ncomprehension using Deep Learning. It is especially useful in Autonomous\nDriving Research as it requires comprehension of drivable and non-drivable\nareas, roadside objects and the like. In this paper semantic segmentation has\nbeen performed on the Indian Driving Dataset which has been recently compiled\non the urban and rural roads of Bengaluru and Hyderabad. This dataset is more\nchallenging compared to other datasets like Cityscapes, since it is based on\nunstructured driving environments. It has a four level hierarchy and in this\npaper segmentation has been performed on the first level. Five different models\nhave been trained and their performance has been compared using the Mean\nIntersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and\nSegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the\ndataset, exploratory data analysis, preparation, implementation of the five\nmodels and studies the performance and compares the results achieved in the\nprocess."}
{"id": "2507.20408", "pdf": "https://arxiv.org/pdf/2507.20408", "abs": "https://arxiv.org/abs/2507.20408", "authors": ["Samiul Based Shuvo", "Taufiq Hasan"], "title": "A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Automated analysis of lung sound auscultation is essential for monitoring\nrespiratory health, especially in regions facing a shortage of skilled\nhealthcare workers. While respiratory sound classification has been widely\nstudied in adults, its ap plication in pediatric populations, particularly in\nchildren aged <6 years, remains an underexplored area. The developmental\nchanges in pediatric lungs considerably alter the acoustic proper ties of\nrespiratory sounds, necessitating specialized classification approaches\ntailored to this age group. To address this, we propose a multistage hybrid\nCNN-Transformer framework that combines CNN-extracted features with an\nattention-based architecture to classify pediatric respiratory diseases using\nscalogram images from both full recordings and individual breath events. Our\nmodel achieved an overall score of 0.9039 in binary event classifi cation and\n0.8448 in multiclass event classification by employing class-wise focal loss to\naddress data imbalance. At the recording level, the model attained scores of\n0.720 for ternary and 0.571 for multiclass classification. These scores\noutperform the previous best models by 3.81% and 5.94%, respectively. This\napproach offers a promising solution for scalable pediatric respiratory disease\ndiagnosis, especially in resource-limited settings."}
{"id": "2507.20409", "pdf": "https://arxiv.org/pdf/2507.20409", "abs": "https://arxiv.org/abs/2507.20409", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Gunhee Kim", "Motahhare Eslami", "Maarten Sap"], "title": "Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Under review; 17 pages", "summary": "Chain-of-Thought (CoT) prompting helps models think step by step. But what\nhappens when they must see, understand, and judge-all at once? In visual tasks\ngrounded in social context, where bridging perception with norm-grounded\njudgments is essential, flat CoT often breaks down. We introduce Cognitive\nChain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning\nthrough three cognitively inspired stages: perception, situation, and norm. Our\nexperiments show that, across multiple multimodal benchmarks (including intent\ndisambiguation, commonsense reasoning, and safety), CoCoT consistently\noutperforms CoT and direct prompting (+8\\% on average). Our findings\ndemonstrate that cognitively grounded reasoning stages enhance interpretability\nand social awareness in VLMs, paving the way for safer and more reliable\nmultimodal systems."}
{"id": "2507.20419", "pdf": "https://arxiv.org/pdf/2507.20419", "abs": "https://arxiv.org/abs/2507.20419", "authors": ["Khloud AL Jallad", "Nada Ghneim", "Ghaida Rebdawi"], "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Natural Language Understanding (NLU) is a basic task in Natural Language\nProcessing (NLP). The evaluation of NLU capabilities has become a trending\nresearch topic that attracts researchers in the last few years, resulting in\nthe development of numerous benchmarks. These benchmarks include various tasks\nand datasets in order to evaluate the results of pretrained models via public\nleaderboards. Notably, several benchmarks contain diagnostics datasets designed\nfor investigation and fine-grained error analysis across a wide range of\nlinguistic phenomena. This survey provides a comprehensive review of available\nEnglish, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on\ntheir diagnostics datasets and the linguistic phenomena they covered. We\npresent a detailed comparison and analysis of these benchmarks, highlighting\ntheir strengths and limitations in evaluating NLU tasks and providing in-depth\nerror analysis. When highlighting the gaps in the state-of-the-art, we noted\nthat there is no naming convention for macro and micro categories or even a\nstandard set of linguistic phenomena that should be covered. Consequently, we\nformulated a research question regarding the evaluation metrics of the\nevaluation diagnostics benchmarks: \"Why do not we have an evaluation standard\nfor the NLU evaluation diagnostics benchmarks?\" similar to ISO standard in\nindustry. We conducted a deep analysis and comparisons of the covered\nlinguistic phenomena in order to support experts in building a global hierarchy\nfor linguistic phenomena in future. We think that having evaluation metrics for\ndiagnostics evaluation could be valuable to gain more insights when comparing\nthe results of the studied models on different diagnostics benchmarks."}
{"id": "2507.20423", "pdf": "https://arxiv.org/pdf/2507.20423", "abs": "https://arxiv.org/abs/2507.20423", "authors": ["Sungwoo Han", "Hyeyeon Kim", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "title": "CodeNER: Code Prompting for Named Entity Recognition", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "18 pages, 6 figures", "summary": "Recent studies have explored various approaches for treating candidate named\nentity spans as both source and target sequences in named entity recognition\n(NER) by leveraging large language models (LLMs). Although previous approaches\nhave successfully generated candidate named entity spans with suitable labels,\nthey rely solely on input context information when using LLMs, particularly,\nChatGPT. However, NER inherently requires capturing detailed labeling\nrequirements with input context information. To address this issue, we propose\na novel method that leverages code-based prompting to improve the capabilities\nof LLMs in understanding and performing NER. By embedding code within prompts,\nwe provide detailed BIO schema instructions for labeling, thereby exploiting\nthe ability of LLMs to comprehend long-range scopes in programming languages.\nExperimental results demonstrate that the proposed code-based prompting method\noutperforms conventional text-based prompting on ten benchmarks across English,\nArabic, Finnish, Danish, and German datasets, indicating the effectiveness of\nexplicitly structuring NER instructions. We also verify that combining the\nproposed code-based prompting method with the chain-of-thought prompting\nfurther improves performance."}
{"id": "2507.20426", "pdf": "https://arxiv.org/pdf/2507.20426", "abs": "https://arxiv.org/abs/2507.20426", "authors": ["Samiul Based Shuvo", "Tasnia Binte Mamun", "U Rajendra Acharya"], "title": "ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.BM"], "comment": null, "summary": "DNA-binding proteins (DBPs) are integral to gene regulation and cellular\nprocesses, making their accurate identification essential for understanding\nbiological functions and disease mechanisms. Experimental methods for DBP\nidentification are time-consuming and costly, driving the need for efficient\ncomputational prediction techniques. In this study, we propose a novel deep\nlearning framework, ResCap-DBP, that combines a residual learning-based encoder\nwith a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly\nfrom raw protein sequences. Our architecture incorporates dilated convolutions\nwithin residual blocks to mitigate vanishing gradient issues and extract rich\nsequence features, while capsule layers with dynamic routing capture\nhierarchical and spatial relationships within the learned feature space. We\nconducted comprehensive ablation studies comparing global and local embeddings\nfrom ProteinBERT and conventional one-hot encoding. Results show that\nProteinBERT embeddings substantially outperform other representations on large\ndatasets. Although one-hot encoding showed marginal advantages on smaller\ndatasets, such as PDB186, it struggled to scale effectively. Extensive\nevaluations on four pairs of publicly available benchmark datasets demonstrate\nthat our model consistently outperforms current state-of-the-art methods. It\nachieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On\nindependent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%\nand 83.3%, while maintaining competitive performance on larger datasets such as\nPDB20000. Notably, the model maintains a well balanced sensitivity and\nspecificity across datasets. These results demonstrate the efficacy and\ngeneralizability of integrating global protein representations with advanced\ndeep learning architectures for reliable and scalable DBP prediction in diverse\ngenomic contexts."}
{"id": "2507.20433", "pdf": "https://arxiv.org/pdf/2507.20433", "abs": "https://arxiv.org/abs/2507.20433", "authors": ["Alessandro Capurso", "Elia Piccoli", "Davide Bacciu"], "title": "FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IEEE Conference on Games (CoG) 2025", "summary": "Transfer Learning (TL) offers the potential to accelerate learning by\ntransferring knowledge across tasks. However, it faces critical challenges such\nas negative transfer, domain adaptation and inefficiency in selecting solid\nsource policies. These issues often represent critical problems in evolving\ndomains, i.e. game development, where scenarios transform and agents must\nadapt. The continuous release of new agents is costly and inefficient. In this\nwork we challenge the key issues in TL to improve knowledge transfer, agents\nperformance across tasks and reduce computational costs. The proposed\nmethodology, called FAST - Framework for Adaptive Similarity-based Transfer,\nleverages visual frames and textual descriptions to create a latent\nrepresentation of tasks dynamics, that is exploited to estimate similarity\nbetween environments. The similarity scores guides our method in choosing\ncandidate policies from which transfer abilities to simplify learning of novel\ntasks. Experimental results, over multiple racing tracks, demonstrate that FAST\nachieves competitive final performance compared to learning-from-scratch\nmethods while requiring significantly less training steps. These findings\nhighlight the potential of embedding-driven task similarity estimations."}
{"id": "2507.20439", "pdf": "https://arxiv.org/pdf/2507.20439", "abs": "https://arxiv.org/abs/2507.20439", "authors": ["Maya Larbi", "Amal Akli", "Mike Papadakis", "Rihab Bouyousfi", "Maxime Cordy", "Federica Sarro", "Yves Le Traon"], "title": "When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance in code\ngeneration tasks under idealized conditions, where task descriptions are clear\nand precise. However, in practice, task descriptions frequently exhibit\nambiguity, incompleteness, or internal contradictions. In this paper, we\npresent the first empirical study examining the robustness of state-of-the-art\ncode generation models when faced with such unclear task descriptions. We\nextend the HumanEval and MBPP benchmarks by systematically introducing\nrealistic task descriptions flaws through guided mutation strategies, producing\na dataset that mirrors the messiness of informal developer instructions. We\nevaluate multiple LLMs of varying sizes and architectures, analyzing their\nfunctional correctness and failure modes across task descriptions categories.\nOur findings reveal that even minor imperfections in task description phrasing\ncan cause significant performance degradation, with contradictory task\ndescriptions resulting in numerous logical errors. Moreover, while larger\nmodels tend to be more resilient than smaller variants, they are not immune to\nthe challenges posed by unclear requirements. We further analyze semantic error\npatterns and identify correlations between description clarity, model behavior,\nand error types. Our results underscore the critical need for developing LLMs\nthat are not only powerful but also robust to the imperfections inherent in\nnatural user tasks, highlighting important considerations for improving model\ntraining strategies, designing more realistic evaluation benchmarks, and\nensuring reliable deployment in practical software development environments."}
{"id": "2507.20460", "pdf": "https://arxiv.org/pdf/2507.20460", "abs": "https://arxiv.org/abs/2507.20460", "authors": ["Selahattin Akkas", "Ariful Azad"], "title": "Shapley-Value-Based Graph Sparsification for GNN Inference", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages", "summary": "Graph sparsification is a key technique for improving inference efficiency in\nGraph Neural Networks by removing edges with minimal impact on predictions. GNN\nexplainability methods generate local importance scores, which can be\naggregated into global scores for graph sparsification. However, many\nexplainability methods produce only non-negative scores, limiting their\napplicability for sparsification. In contrast, Shapley value based methods\nassign both positive and negative contributions to node predictions, offering a\ntheoretically robust and fair allocation of importance by evaluating many\nsubsets of graphs. Unlike gradient-based or perturbation-based explainers,\nShapley values enable better pruning strategies that preserve influential edges\nwhile removing misleading or adversarial connections. Our approach shows that\nShapley value-based graph sparsification maintains predictive performance while\nsignificantly reducing graph complexity, enhancing both interpretability and\nefficiency in GNN inference."}
{"id": "2507.20491", "pdf": "https://arxiv.org/pdf/2507.20491", "abs": "https://arxiv.org/abs/2507.20491", "authors": ["Tuan Bui", "Trong Le", "Phat Thai", "Sang Nguyen", "Minh Hua", "Ngan Pham", "Thang Bui", "Tho Quan"], "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "8 pages, 3 figures. Accepted at the International Joint Conference on\n  Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in\n  Neuro-Symbolic AI. https://2025.ijcnn.org", "summary": "Recent advances in large language models (LLMs) have significantly enhanced\nquestion-answering (QA) capabilities, particularly in open-domain contexts.\nHowever, in closed-domain scenarios such as education, healthcare, and law,\nusers demand not only accurate answers but also transparent reasoning and\nexplainable decision-making processes. While neural-symbolic (NeSy) frameworks\nhave emerged as a promising solution, leveraging LLMs for natural language\nunderstanding and symbolic systems for formal reasoning, existing approaches\noften rely on large-scale models and exhibit inefficiencies in translating\nnatural language into formal logic representations.\n  To address these limitations, we introduce Text-JEPA (Text-based\nJoint-Embedding Predictive Architecture), a lightweight yet effective framework\nfor converting natural language into first-order logic (NL2FOL). Drawing\ninspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by\nefficiently generating logic representations, while the Z3 solver operates as\nSystem 2, enabling robust logical inference. To rigorously evaluate the\nNL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework\ncomprising three custom metrics: conversion score, reasoning score, and\nSpearman rho score, which collectively capture the quality of logical\ntranslation and its downstream impact on reasoning accuracy.\n  Empirical results on domain-specific datasets demonstrate that Text-JEPA\nachieves competitive performance with significantly lower computational\noverhead compared to larger LLM-based systems. Our findings highlight the\npotential of structured, interpretable reasoning frameworks for building\nefficient and explainable QA systems in specialized domains."}
{"id": "2507.20499", "pdf": "https://arxiv.org/pdf/2507.20499", "abs": "https://arxiv.org/abs/2507.20499", "authors": ["Linh Le Pham Van", "Minh Hoang Nguyen", "Duc Kieu", "Hung Le", "Hung The Tran", "Sunil Gupta"], "title": "DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "accepted at ECAI 2025", "summary": "Cross-domain offline reinforcement learning (RL) seeks to enhance sample\nefficiency in offline RL by utilizing additional offline source datasets. A key\nchallenge is to identify and utilize source samples that are most relevant to\nthe target domain. Existing approaches address this challenge by measuring\ndomain gaps through domain classifiers, target transition dynamics modeling, or\nmutual information estimation using contrastive loss. However, these methods\noften require large target datasets, which is impractical in many real-world\nscenarios. In this work, we address cross-domain offline RL under a limited\ntarget data setting, identifying two primary challenges: (1) Dataset imbalance,\nwhich is caused by large source and small target datasets and leads to\noverfitting in neural network-based domain gap estimators, resulting in\nuninformative measurements; and (2) Partial domain overlap, where only a subset\nof the source data is closely aligned with the target domain. To overcome these\nissues, we propose DmC, a novel framework for cross-domain offline RL with\nlimited target samples. Specifically, DmC utilizes $k$-nearest neighbor\n($k$-NN) based estimation to measure domain proximity without neural network\ntraining, effectively mitigating overfitting. Then, by utilizing this domain\nproximity, we introduce a nearest-neighbor-guided diffusion model to generate\nadditional source samples that are better aligned with the target domain, thus\nenhancing policy learning with more effective source samples. Through\ntheoretical analysis and extensive experiments in diverse MuJoCo environments,\nwe demonstrate that DmC significantly outperforms state-of-the-art cross-domain\noffline RL methods, achieving substantial performance gains."}
{"id": "2507.20509", "pdf": "https://arxiv.org/pdf/2507.20509", "abs": "https://arxiv.org/abs/2507.20509", "authors": ["Zhongchao Zhou", "Yuxi Lu", "Yaonan Zhu", "Yifan Zhao", "Bin He", "Liang He", "Wenwen Yu", "Yusuke Iwasawa"], "title": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "With rapid advances in code generation, reasoning, and problem-solving, Large\nLanguage Models (LLMs) are increasingly applied in robotics. Most existing work\nfocuses on high-level tasks such as task decomposition. A few studies have\nexplored the use of LLMs in feedback controller design; however, these efforts\nare restricted to overly simplified systems, fixed-structure gain tuning, and\nlack real-world validation. To further investigate LLMs in automatic control,\nthis work targets a key subfield: adaptive control. Inspired by the framework\nof model reference adaptive control (MRAC), we propose an LLM-guided adaptive\ncompensator framework that avoids designing controllers from scratch. Instead,\nthe LLMs are prompted using the discrepancies between an unknown system and a\nreference system to design a compensator that aligns the response of the\nunknown system with that of the reference, thereby achieving adaptivity.\nExperiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided\nadaptive controller, indirect adaptive control, learning-based adaptive\ncontrol, and MRAC, on soft and humanoid robots in both simulated and real-world\nenvironments. Results show that the LLM-guided adaptive compensator outperforms\ntraditional adaptive controllers and significantly reduces reasoning complexity\ncompared to the LLM-guided adaptive controller. The Lyapunov-based analysis and\nreasoning-path inspection demonstrate that the LLM-guided adaptive compensator\nenables a more structured design process by transforming mathematical\nderivation into a reasoning task, while exhibiting strong generalizability,\nadaptability, and robustness. This study opens a new direction for applying\nLLMs in the field of automatic control, offering greater deployability and\npracticality compared to vision-language models."}
{"id": "2507.20520", "pdf": "https://arxiv.org/pdf/2507.20520", "abs": "https://arxiv.org/abs/2507.20520", "authors": ["Praneeth Narisetty", "Uday Kumar Reddy Kattamanchi", "Lohit Akshant Nimma", "Sri Ram Kaushik Karnati", "Shiva Nagendra Babu Kore", "Mounika Golamari", "Tejashree Nageshreddy"], "title": "AQUA: A Large Language Model for Aquaculture & Fisheries", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": null, "summary": "Aquaculture plays a vital role in global food security and coastal economies\nby providing sustainable protein sources. As the industry expands to meet\nrising demand, it faces growing challenges such as disease outbreaks,\ninefficient feeding practices, rising labor costs, logistical inefficiencies,\nand critical hatchery issues, including high mortality rates and poor water\nquality control. Although artificial intelligence has made significant\nprogress, existing machine learning methods fall short of addressing the\ndomain-specific complexities of aquaculture. To bridge this gap, we introduce\nAQUA, the first large language model (LLM) tailored for aquaculture, designed\nto support farmers, researchers, and industry practitioners. Central to this\neffort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic\nFramework for generating and refining high-quality synthetic data using a\ncombination of expert knowledge, largescale language models, and automated\nevaluation techniques. Our work lays the foundation for LLM-driven innovations\nin aquaculture research, advisory systems, and decision-making tools."}
{"id": "2507.20525", "pdf": "https://arxiv.org/pdf/2507.20525", "abs": "https://arxiv.org/abs/2507.20525", "authors": ["Murray Shanahan", "Tara Das", "Robert Thurman"], "title": "The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated \"Sacred\" Text?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This paper presents a case study in the use of a large language model to\ngenerate a fictional Buddhist \"sutr\"', and offers a detailed analysis of the\nresulting text from a philosophical and literary point of view. The conceptual\nsubtlety, rich imagery, and density of allusion found in the text make it hard\nto causally dismiss on account of its mechanistic origin. This raises questions\nabout how we, as a society, should come to terms with the potentially\nunsettling possibility of a technology that encroaches on human meaning-making.\nWe suggest that Buddhist philosophy, by its very nature, is well placed to\nadapt."}
{"id": "2507.20529", "pdf": "https://arxiv.org/pdf/2507.20529", "abs": "https://arxiv.org/abs/2507.20529", "authors": ["Xun Liang", "Xin Guo", "Zhongming Jin", "Weihang Pan", "Penghui Shang", "Deng Cai", "Binbin Lin", "Jieping Ye"], "title": "Enhancing Spatial Reasoning through Visual and Textual Thinking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The spatial reasoning task aims to reason about the spatial relationships in\n2D and 3D space, which is a fundamental capability for Visual Question\nAnswering (VQA) and robotics. Although vision language models (VLMs) have\ndeveloped rapidly in recent years, they are still struggling with the spatial\nreasoning task. In this paper, we introduce a method that can enhance Spatial\nreasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In\nthe spatial visual thinking phase, our model is trained to generate\nlocation-related specific tokens of essential targets automatically. Not only\nare the objects mentioned in the problem addressed, but also the potential\nobjects related to the reasoning are considered. During the spatial textual\nthinking phase, Our model conducts long-term thinking based on visual cues and\ndialogues, gradually inferring the answers to spatial reasoning problems. To\neffectively support the model's training, we perform manual corrections to the\nexisting spatial reasoning dataset, eliminating numerous incorrect labels\nresulting from automatic annotation, restructuring the data input format to\nenhance generalization ability, and developing thinking processes with logical\nreasoning details. Without introducing additional information (such as masks or\ndepth), our model's overall average level in several spatial understanding\ntasks has significantly improved compared with other models."}
{"id": "2507.20534", "pdf": "https://arxiv.org/pdf/2507.20534", "abs": "https://arxiv.org/abs/2507.20534", "authors": ["Kimi Team", "Yifan Bai", "Yiping Bao", "Guanduo Chen", "Jiahao Chen", "Ningxin Chen", "Ruijue Chen", "Yanru Chen", "Yuankun Chen", "Yutian Chen", "Zhuofu Chen", "Jialei Cui", "Hao Ding", "Mengnan Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Yulun Du", "Yu Fan", "Yichen Feng", "Kelin Fu", "Bofei Gao", "Hongcheng Gao", "Peizhong Gao", "Tong Gao", "Xinran Gu", "Longyu Guan", "Haiqing Guo", "Jianhang Guo", "Hao Hu", "Xiaoru Hao", "Tianhong He", "Weiran He", "Wenyang He", "Chao Hong", "Yangyang Hu", "Zhenxing Hu", "Weixiao Huang", "Zhiqi Huang", "Zihao Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yongsheng Kang", "Guokun Lai", "Cheng Li", "Fang Li", "Haoyang Li", "Ming Li", "Wentao Li", "Yanhao Li", "Yiwei Li", "Zhaowei Li", "Zheming Li", "Hongzhan Lin", "Xiaohan Lin", "Zongyu Lin", "Chengyin Liu", "Chenyu Liu", "Hongzhang Liu", "Jingyuan Liu", "Junqi Liu", "Liang Liu", "Shaowei Liu", "T. Y. Liu", "Tianwei Liu", "Weizhou Liu", "Yangyang Liu", "Yibo Liu", "Yiping Liu", "Yue Liu", "Zhengying Liu", "Enzhe Lu", "Lijun Lu", "Shengling Ma", "Xinyu Ma", "Yingwei Ma", "Shaoguang Mao", "Jie Mei", "Xin Men", "Yibo Miao", "Siyuan Pan", "Yebo Peng", "Ruoyu Qin", "Bowen Qu", "Zeyu Shang", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Jianlin Su", "Zhengyuan Su", "Xinjie Sun", "Flood Sung", "Heyi Tang", "Jiawen Tao", "Qifeng Teng", "Chensi Wang", "Dinglu Wang", "Feng Wang", "Haiming Wang", "Jianzhou Wang", "Jiaxing Wang", "Jinhong Wang", "Shengjie Wang", "Shuyi Wang", "Yao Wang", "Yejie Wang", "Yiqin Wang", "Yuxin Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhengtao Wang", "Zhexu Wang", "Chu Wei", "Qianqian Wei", "Wenhao Wu", "Xingzhe Wu", "Yuxin Wu", "Chenjun Xiao", "Xiaotong Xie", "Weimin Xiong", "Boyu Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinran Xu", "Yangchuan Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Xiaofei Yang", "Ying Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Xingcheng Yao", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Longhui Yu", "Enming Yuan", "Hongbang Yuan", "Mengjie Yuan", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Wanlu Zhang", "Xiaobin Zhang", "Yangkun Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Haotian Zhao", "Yikai Zhao", "Huabin Zheng", "Shaojie Zheng", "Jianren Zhou", "Xinyu Zhou", "Zaida Zhou", "Zhen Zhu", "Weiyu Zhuang", "Xinxing Zu"], "title": "Kimi K2: Open Agentic Intelligence", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "tech report of Kimi K2", "summary": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32\nbillion activated parameters and 1 trillion total parameters. We propose the\nMuonClip optimizer, which improves upon Muon with a novel QK-clip technique to\naddress training instability while enjoying the advanced token efficiency of\nMuon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero\nloss spike. During post-training, K2 undergoes a multi-stage post-training\nprocess, highlighted by a large-scale agentic data synthesis pipeline and a\njoint reinforcement learning (RL) stage, where the model improves its\ncapabilities through interactions with real and synthetic environments.\n  Kimi K2 achieves state-of-the-art performance among open-source non-thinking\nmodels, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on\nTau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on\nSWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in\nnon-thinking settings. It also exhibits strong capabilities in coding,\nmathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,\n49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without\nextended thinking. These results position Kimi K2 as one of the most capable\nopen-source large language models to date, particularly in software engineering\nand agentic tasks. We release our base and post-trained model checkpoints to\nfacilitate future research and applications of agentic intelligence."}
{"id": "2507.20536", "pdf": "https://arxiv.org/pdf/2507.20536", "abs": "https://arxiv.org/abs/2507.20536", "authors": ["Chieh-Yun Chen", "Min Shi", "Gong Zhang", "Humphrey Shi"], "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "ICCV 2025", "summary": "Text-to-Image (T2I) generative models have revolutionized content creation\nbut remain highly sensitive to prompt phrasing, often requiring users to\nrepeatedly refine prompts multiple times without clear feedback. While\ntechniques such as automatic prompt engineering, controlled text embeddings,\ndenoising, and multi-turn generation mitigate these issues, they offer limited\ncontrollability, or often necessitate additional training, restricting the\ngeneralization abilities. Thus, we introduce T2I-Copilot, a training-free\nmulti-agent system that leverages collaboration between (Multimodal) Large\nLanguage Models to automate prompt phrasing, model selection, and iterative\nrefinement. This approach significantly simplifies prompt engineering while\nenhancing generation quality and text-image alignment compared to direct\ngeneration. Specifically, T2I-Copilot consists of three agents: (1) Input\nInterpreter, which parses the input prompt, resolves ambiguities, and generates\na standardized report; (2) Generation Engine, which selects the appropriate\nmodel from different types of T2I models and organizes visual and textual\nprompts to initiate generation; and (3) Quality Evaluator, which assesses\naesthetic quality and text-image alignment, providing scores and feedback for\npotential regeneration. T2I-Copilot can operate fully autonomously while also\nsupporting human-in-the-loop intervention for fine-grained control. On\nGenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA\nscore comparable to commercial models RecraftV3 and Imagen 3, surpasses\nFLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and\nSD 3.5 Large by 9.11% and 6.36%. Code will be released at:\nhttps://github.com/SHI-Labs/T2I-Copilot."}
{"id": "2507.20546", "pdf": "https://arxiv.org/pdf/2507.20546", "abs": "https://arxiv.org/abs/2507.20546", "authors": ["Joosung Lee", "Cheonbok Park", "Hwiyeol Jo", "Jeonghoon Kim", "Joonsuk Park", "Kang Min Yoo"], "title": "Enhancing Hallucination Detection via Future Context", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely used to generate plausible text on\nonline platforms, without revealing the generation process. As users\nincreasingly encounter such black-box outputs, detecting hallucinations has\nbecome a critical challenge. To address this challenge, we focus on developing\na hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample\nfuture contexts. The sampled future contexts provide valuable clues for\nhallucination detection and can be effectively integrated with various\nsampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach."}
{"id": "2507.20562", "pdf": "https://arxiv.org/pdf/2507.20562", "abs": "https://arxiv.org/abs/2507.20562", "authors": ["Hyung Kyu Kim", "Sangmin Lee", "Hak Gu Kim"], "title": "MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for ICCV 2025 Project Page:\n  https://cau-irislab.github.io/ICCV25-MemoryTalker/", "summary": "Speech-driven 3D facial animation aims to synthesize realistic facial motion\nsequences from given audio, matching the speaker's speaking style. However,\nprevious works often require priors such as class labels of a speaker or\nadditional 3D facial meshes at inference, which makes them fail to reflect the\nspeaking style and limits their practical use. To address these issues, we\npropose MemoryTalker which enables realistic and accurate 3D facial motion\nsynthesis by reflecting speaking style only with audio input to maximize\nusability in applications. Our framework consists of two training stages:\n1-stage is storing and retrieving general motion (i.e., Memorizing), and\n2-stage is to perform the personalized facial motion synthesis (i.e.,\nAnimating) with the motion memory stylized by the audio-driven speaking style\nfeature. In this second stage, our model learns about which facial motion types\nshould be emphasized for a particular piece of audio. As a result, our\nMemoryTalker can generate a reliable personalized facial animation without\nadditional prior information. With quantitative and qualitative evaluations, as\nwell as user study, we show the effectiveness of our model and its performance\nenhancement for personalized facial animation over state-of-the-art methods."}
{"id": "2507.20568", "pdf": "https://arxiv.org/pdf/2507.20568", "abs": "https://arxiv.org/abs/2507.20568", "authors": ["Hyung Kyu Kim", "Hak Gu Kim"], "title": "Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for Interspeech 2025 Project Page:\n  https://cau-irislab.github.io/interspeech25/", "summary": "Speech-driven 3D facial animation aims to generate realistic facial movements\nsynchronized with audio. Traditional methods primarily minimize reconstruction\nloss by aligning each frame with ground-truth. However, this frame-wise\napproach often fails to capture the continuity of facial motion, leading to\njittery and unnatural outputs due to coarticulation. To address this, we\npropose a novel phonetic context-aware loss, which explicitly models the\ninfluence of phonetic context on viseme transitions. By incorporating a viseme\ncoarticulation weight, we assign adaptive importance to facial movements based\non their dynamic changes over time, ensuring smoother and perceptually\nconsistent animations. Extensive experiments demonstrate that replacing the\nconventional reconstruction loss with ours improves both quantitative metrics\nand visual quality. It highlights the importance of explicitly modeling\nphonetic context-dependent visemes in synthesizing natural speech-driven 3D\nfacial animation. Project page: https://cau-irislab.github.io/interspeech25/"}
{"id": "2507.20571", "pdf": "https://arxiv.org/pdf/2507.20571", "abs": "https://arxiv.org/abs/2507.20571", "authors": ["Shuaipeng Zhang", "Lanju Kong", "Yixin Zhang", "Wei He", "Yongqing Zheng", "Han Yu", "Lizhen Cui"], "title": "DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, IEEE International Conference on Multimedia & Expo 2025\n  conference paper", "summary": "Due to the distributed nature of federated learning (FL), the vulnerability\nof the global model and the need for coordination among many client devices\npose significant challenges. As a promising decentralized, scalable and secure\nsolution, blockchain-based FL methods have attracted widespread attention in\nrecent years. However, traditional consensus mechanisms designed for Proof of\nWork (PoW) similar to blockchain incur substantial resource consumption and\ncompromise the efficiency of FL, particularly when participating devices are\nwireless and resource-limited. To address asynchronous client participation and\ndata heterogeneity in FL, while limiting the additional resource overhead\nintroduced by blockchain, we propose the Directed Acyclic Graph-based\nAsynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection\nalgorithm that considers temporal freshness, node reachability and model\naccuracy, with a DAG-based trusted verification strategy. Extensive experiments\non 3 benchmarking datasets against eight state-of-the-art approaches\ndemonstrate that DAG-AFL significantly improves training efficiency and model\naccuracy by 22.7% and 6.5% on average, respectively."}
{"id": "2507.20575", "pdf": "https://arxiv.org/pdf/2507.20575", "abs": "https://arxiv.org/abs/2507.20575", "authors": ["I Gede Eka Sulistyawan", "Takuro Ishii", "Riku Suzuki", "Yoshifumi Saijo"], "title": "Implicit Spatiotemporal Bandwidth Enhancement Filter by Sine-activated Deep Learning Model for Fast 3D Photoacoustic Tomography", "categories": ["eess.IV", "cs.AI"], "comment": "14 pages, 13 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "3D photoacoustic tomography (3D-PAT) using high-frequency hemispherical\ntransducers offers near-omnidirectional reception and enhanced sensitivity to\nthe finer structural details encoded in the high-frequency components of the\nbroadband photoacoustic (PA) signal. However, practical constraints such as\nlimited number of channels with bandlimited sampling rate often result in\nsparse and bandlimited sensors that degrade image quality. To address this, we\nrevisit the 2D deep learning (DL) approach applied directly to sensor-wise PA\nradio-frequency (PARF) data. Specifically, we introduce sine activation into\nthe DL model to restore the broadband nature of PARF signals given the observed\nband-limited and high-frequency PARF data. Given the scarcity of 3D training\ndata, we employ simplified training strategies by simulating random spherical\nabsorbers. This combination of sine-activated model and randomized training is\ndesigned to emphasize bandwidth learning over dataset memorization. Our model\nwas evaluated on a leaf skeleton phantom, a micro-CT-verified 3D spiral phantom\nand in-vivo human palm vasculature. The results showed that the proposed\ntraining mechanism on sine-activated model was well-generalized across the\ndifferent tests by effectively increasing the sensor density and recovering the\nspatiotemporal bandwidth. Qualitatively, the sine-activated model uniquely\nenhanced high-frequency content that produces clearer vascular structure with\nfewer artefacts. Quantitatively, the sine-activated model exhibits full\nbandwidth at -12 dB spectrum and significantly higher contrast-to-noise ratio\nwith minimal loss of structural similarity index. Lastly, we optimized our\napproach to enable fast enhanced 3D-PAT at 2 volumes-per-second for better\npractical imaging of a free-moving targets."}
{"id": "2507.20578", "pdf": "https://arxiv.org/pdf/2507.20578", "abs": "https://arxiv.org/abs/2507.20578", "authors": ["Zhaoyan Wang", "Hyunjun Ahn", "In-Young Ko"], "title": "Beyond Interactions: Node-Level Graph Generation for Knowledge-Free Augmentation in Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recent advances in recommender systems rely on external resources such as\nknowledge graphs or large language models to enhance recommendations, which\nlimit applicability in real-world settings due to data dependency and\ncomputational overhead. Although knowledge-free models are able to bolster\nrecommendations by direct edge operations as well, the absence of augmentation\nprimitives drives them to fall short in bridging semantic and structural gaps\nas high-quality paradigm substitutes. Unlike existing diffusion-based works\nthat remodel user-item interactions, this work proposes NodeDiffRec, a\npioneering knowledge-free augmentation framework that enables fine-grained\nnode-level graph generation for recommendations and expands the scope of\nrestricted augmentation primitives via diffusion. By synthesizing pseudo-items\nand corresponding interactions that align with the underlying distribution for\ninjection, and further refining user preferences through a denoising preference\nmodeling process, NodeDiffRec dramatically enhances both semantic diversity and\nstructural connectivity without external knowledge. Extensive experiments\nacross diverse datasets and recommendation algorithms demonstrate the\nsuperiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with\nmaximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5\nover selected baselines."}
{"id": "2507.20623", "pdf": "https://arxiv.org/pdf/2507.20623", "abs": "https://arxiv.org/abs/2507.20623", "authors": ["Yang Zhao", "Shusheng Li", "Xueshang Feng"], "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures, to be published in ACM Multimedia 2025", "summary": "As the development of lightweight deep learning algorithms, various deep\nneural network (DNN) models have been proposed for the remote sensing scene\nclassification (RSSC) application. However, it is still challenging for these\nRSSC models to achieve optimal performance among model accuracy, inference\nlatency, and energy consumption on resource-constrained edge devices. In this\npaper, we propose a lightweight RSSC framework, which includes a distilled\nglobal filter network (GFNet) model and an early-exit mechanism designed for\nedge devices to achieve state-of-the-art performance. Specifically, we first\napply frequency domain distillation on the GFNet model to reduce model size.\nThen we design a dynamic early-exit model tailored for DNN models on edge\ndevices to further improve model inference efficiency. We evaluate our E3C\nmodel on three edge devices across four datasets. Extensive experimental\nresults show that it achieves an average of 1.3x speedup on model inference and\nover 40% improvement on energy efficiency, while maintaining high\nclassification accuracy."}
{"id": "2507.20627", "pdf": "https://arxiv.org/pdf/2507.20627", "abs": "https://arxiv.org/abs/2507.20627", "authors": ["Junxian Wu", "Weitao You", "Heda Zuo", "Dengming Zhang", "Pei Chen", "Lingyun Sun"], "title": "Controllable Video-to-Music Generation with Multiple Time-Varying Conditions", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by the 33rd ACM International Conference on Multimedia\n  (ACMMM 2025). The project page is available at\n  https://kita-wjx.github.io/MCV2M/", "summary": "Music enhances video narratives and emotions, driving demand for automatic\nvideo-to-music (V2M) generation. However, existing V2M methods relying solely\non visual features or supplementary textual inputs generate music in a\nblack-box manner, often failing to meet user expectations. To address this\nchallenge, we propose a novel multi-condition guided V2M generation framework\nthat incorporates multiple time-varying conditions for enhanced control over\nmusic generation. Our method uses a two-stage training strategy that enables\nlearning of V2M fundamentals and audiovisual temporal synchronization while\nmeeting users' needs for multi-condition control. In the first stage, we\nintroduce a fine-grained feature selection module and a progressive temporal\nalignment attention mechanism to ensure flexible feature alignment. For the\nsecond stage, we develop a dynamic conditional fusion module and a\ncontrol-guided decoder module to integrate multiple conditions and accurately\nguide the music composition process. Extensive experiments demonstrate that our\nmethod outperforms existing V2M pipelines in both subjective and objective\nevaluations, significantly enhancing control and alignment with user\nexpectations."}
{"id": "2507.20630", "pdf": "https://arxiv.org/pdf/2507.20630", "abs": "https://arxiv.org/abs/2507.20630", "authors": ["Ao Li", "Yuxiang Duan", "Jinghui Zhang", "Congbo Ma", "Yutong Xie", "Gustavo Carneiro", "Mohammad Yaqub", "Hu Wang"], "title": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but\nface high computational costs due to the large number of visual tokens,\nmotivating token pruning to improve inference efficiency. The key challenge\nlies in identifying which tokens are truly important. Most existing approaches\nrely on attention-based criteria to estimate token importance. However, they\ninherently suffer from certain limitations, such as positional bias. In this\nwork, we explore a new perspective on token importance based on token\ntransitions in LVLMs. We observe that the transition of token representations\nprovides a meaningful signal of semantic information. Based on this insight, we\npropose TransPrune, a training-free and efficient token pruning method.\nSpecifically, TransPrune progressively prunes tokens by assessing their\nimportance through a combination of Token Transition Variation (TTV)-which\nmeasures changes in both the magnitude and direction of token\nrepresentations-and Instruction-Guided Attention (IGA), which measures how\nstrongly the instruction attends to image tokens via attention. Extensive\nexperiments demonstrate that TransPrune achieves comparable multimodal\nperformance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight\nbenchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV\nalone can serve as an effective criterion without relying on attention,\nachieving performance comparable to attention-based methods. The code will be\nmade publicly available upon acceptance of the paper at\nhttps://github.com/liaolea/TransPrune."}
{"id": "2507.20643", "pdf": "https://arxiv.org/pdf/2507.20643", "abs": "https://arxiv.org/abs/2507.20643", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Zhao Li", "Zirui Chen"], "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance."}
{"id": "2507.20650", "pdf": "https://arxiv.org/pdf/2507.20650", "abs": "https://arxiv.org/abs/2507.20650", "authors": ["Zhicheng Zhang", "Peizhuo Lv", "Mengke Wan", "Jiang Fang", "Diandian Guo", "Yezeng Chen", "Yinlong Liu", "Wei Ma", "Jiyan Sun", "Liru Geng"], "title": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recently, Deep Learning (DL) models have been increasingly deployed on\nend-user devices as On-Device AI, offering improved efficiency and privacy.\nHowever, this deployment trend poses more serious Intellectual Property (IP)\nrisks, as models are distributed on numerous local devices, making them\nvulnerable to theft and redistribution. Most existing ownership protection\nsolutions (e.g., backdoor-based watermarking) are designed for cloud-based\nAI-as-a-Service (AIaaS) and are not directly applicable to large-scale\ndistribution scenarios, where each user-specific model instance must carry a\nunique watermark. These methods typically embed a fixed watermark, and\nmodifying the embedded watermark requires retraining the model. To address\nthese challenges, we propose Hot-Swap MarkBoard, an efficient watermarking\nmethod. It encodes user-specific $n$-bit binary signatures by independently\nembedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)\nmodule, enabling efficient watermark customization without retraining through\nbranch swapping. A parameter obfuscation mechanism further entangles the\nwatermark weights with those of the base model, preventing removal without\ndegrading model performance. The method supports black-box verification and is\ncompatible with various model architectures and DL tasks, including\nclassification, image generation, and text generation. Extensive experiments\nacross three types of tasks and six backbone models demonstrate our method's\nsuperior efficiency and adaptability compared to existing approaches, achieving\n100\\% verification accuracy."}
{"id": "2507.20666", "pdf": "https://arxiv.org/pdf/2507.20666", "abs": "https://arxiv.org/abs/2507.20666", "authors": ["Harsh Purohit", "Tomoya Nishida", "Kota Dohi", "Takashi Endo", "Yohei Kawaguchi"], "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": null, "summary": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems."}
{"id": "2507.20670", "pdf": "https://arxiv.org/pdf/2507.20670", "abs": "https://arxiv.org/abs/2507.20670", "authors": ["Jonas Peche", "Aliaksei Tsishurou", "Alexander Zap", "Guenter Wallner"], "title": "A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting player movement in multiplayer games is crucial\nfor achieving use cases such as player-mimicking bot navigation, preemptive bot\ncontrol, strategy recommendation, and real-time player behavior analytics.\nHowever, the complex environments allow for a high degree of navigational\nfreedom, and the interactions and team-play between players require models that\nmake effective use of the available heterogeneous input data. This paper\npresents a multimodal architecture for predicting future player locations on a\ndynamic time horizon, using a U-Net-based approach for calculating endpoint\nlocation probability heatmaps, conditioned using a multimodal feature encoder.\nThe application of a multi-head attention mechanism for different groups of\nfeatures allows for communication between agents. In doing so, the architecture\nmakes efficient use of the multimodal game state including image inputs,\nnumerical and categorical features, as well as dynamic game data. Consequently,\nthe presented technique lays the foundation for various downstream tasks that\nrely on future player positions such as the creation of player-predictive bot\nbehavior or player anomaly detection."}
{"id": "2507.20704", "pdf": "https://arxiv.org/pdf/2507.20704", "abs": "https://arxiv.org/abs/2507.20704", "authors": ["Gabriel Downer", "Sean Craven", "Damian Ruck", "Jake Thomas"], "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript", "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications."}
{"id": "2507.20714", "pdf": "https://arxiv.org/pdf/2507.20714", "abs": "https://arxiv.org/abs/2507.20714", "authors": ["Asma Sadia Khan", "Fariba Tasnia Khan", "Tanjim Mahmud", "Salman Karim Khan", "Rishita Chakma", "Nahed Sharmen", "Mohammad Shahadat Hossain", "Karl Andersson"], "title": "Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "stat.AP"], "comment": null, "summary": "Prostate cancer, the second most prevalent male malignancy, requires advanced\ndiagnostic tools. We propose an explainable AI system combining BERT (for\ntextual clinical notes) and Random Forest (for numerical lab data) through a\nnovel multimodal fusion strategy, achieving superior classification performance\non PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is\nestablished, our work demonstrates that a simple yet interpretable BERT+RF\npipeline delivers clinically significant improvements - particularly for\nintermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824\nnumerical/0.725 textual). SHAP analysis provides transparent feature importance\nrankings, while ablation studies prove textual features' complementary value.\nThis accessible approach offers hospitals a balance of high performance\n(F1=89%), computational efficiency, and clinical interpretability - addressing\ncritical needs in prostate cancer diagnostics."}
{"id": "2507.20737", "pdf": "https://arxiv.org/pdf/2507.20737", "abs": "https://arxiv.org/abs/2507.20737", "authors": ["Geng-Xin Xu", "Xiang Zuo", "Ye Li"], "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "MICCAI2025", "summary": "Emotion recognition from physiological data is crucial for mental health\nassessment, yet it faces two significant challenges: incomplete multi-modal\nsignals and interference from body movements and artifacts. This paper presents\na novel Multi-Masked Querying Network (MMQ-Net) to address these issues by\nintegrating multiple querying mechanisms into a unified framework.\nSpecifically, it uses modality queries to reconstruct missing data from\nincomplete signals, category queries to focus on emotional state features, and\ninterference queries to separate relevant information from noise. Extensive\nexperiment results demonstrate the superior emotion recognition performance of\nMMQ-Net compared to existing approaches, particularly under high levels of data\nincompleteness."}
{"id": "2507.20745", "pdf": "https://arxiv.org/pdf/2507.20745", "abs": "https://arxiv.org/abs/2507.20745", "authors": ["Yue Zhu", "Haiwen Diao", "Shang Gao", "Jiazuo Yu", "Jiawen Zhu", "Yunzhi Zhuge", "Shuai Hao", "Xu Jia", "Lu Zhang", "Ying Zhang", "Huchuan Lu"], "title": "Regularizing Subspace Redundancy of Low-Rank Adaptation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 4 figures, Accepted by ACMMM2025", "summary": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA."}
{"id": "2507.20746", "pdf": "https://arxiv.org/pdf/2507.20746", "abs": "https://arxiv.org/abs/2507.20746", "authors": ["Zeyu Huang", "Wei Meng", "Quan Liu", "Kun Chen", "Li Ma"], "title": "AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": null, "summary": "Spiking neural networks possess the advantage of low energy consumption due\nto their event-driven nature. Compared with binary spike outputs, their\ninherent floating-point dynamics are more worthy of attention. The threshold\nlevel and reset mode of neurons play a crucial role in determining the number\nand timing of spikes. The existing hard reset method causes information loss,\nwhile the improved soft reset method adopts a uniform treatment for neurons. In\nresponse to this, this paper designs an adaptive reset neuron, establishing the\ncorrelation between input, output and reset, and integrating a simple yet\neffective threshold adjustment strategy. It achieves excellent performance on\nvarious datasets while maintaining the advantage of low energy consumption."}
{"id": "2507.20753", "pdf": "https://arxiv.org/pdf/2507.20753", "abs": "https://arxiv.org/abs/2507.20753", "authors": ["Yunus Lutz", "Timo Wilm", "Philipp Duwe"], "title": "Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library", "summary": "In e-commerce recommender and search systems, tree-based models, such as\nLambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.\nDespite their effectiveness and widespread adoption in industry, the debate\ncontinues whether deep neural networks (DNNs) can outperform traditional\ntree-based models in this domain. To contribute to this discussion, we\nsystematically benchmark DNNs against our production-grade LambdaMART model. We\nevaluate multiple DNN architectures and loss functions on a proprietary dataset\nfrom OTTO and validate our findings through an 8-week online A/B test. The\nresults show that a simple DNN architecture outperforms a strong tree-based\nbaseline in terms of total clicks and revenue, while achieving parity in total\nunits sold."}
{"id": "2507.20757", "pdf": "https://arxiv.org/pdf/2507.20757", "abs": "https://arxiv.org/abs/2507.20757", "authors": ["Matan Kichler", "Shai Bagon", "Mark Sheinin"], "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers."}
{"id": "2507.20782", "pdf": "https://arxiv.org/pdf/2507.20782", "abs": "https://arxiv.org/abs/2507.20782", "authors": ["Pavel Korshunov", "Ketan Kotwal", "Christophe Ecabert", "Vidit Vidit", "Amir Mohammadi", "Sebastien Marcel"], "title": "Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in IEEE International Joint Conference on\n  Biometrics (IJCB), 2025", "summary": "Synthetic data has emerged as a promising alternative for training face\nrecognition (FR) models, offering advantages in scalability, privacy\ncompliance, and potential for bias mitigation. However, critical questions\nremain on whether both high accuracy and fairness can be achieved with\nsynthetic data. In this work, we evaluate the impact of synthetic data on bias\nand performance of FR systems. We generate balanced face dataset, FairFaceGen,\nusing two state of the art text-to-image generators, Flux.1-dev and Stable\nDiffusion v3.5 (SD35), and combine them with several identity augmentation\nmethods, including Arc2Face and four IP-Adapters. By maintaining equal identity\ncount across synthetic and real datasets, we ensure fair comparisons when\nevaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging\nIJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our\nresults demonstrate that although synthetic data still lags behind the real\ndatasets in the generalization on IJB-B/C, demographically balanced synthetic\ndatasets, especially those generated with SD35, show potential for bias\nmitigation. We also observe that the number and quality of intra-class\naugmentations significantly affect FR accuracy and fairness. These findings\nprovide practical guidelines for constructing fairer FR systems using synthetic\ndata."}
{"id": "2507.20796", "pdf": "https://arxiv.org/pdf/2507.20796", "abs": "https://arxiv.org/abs/2507.20796", "authors": ["Wei Lu", "Daniel L. Chen", "Christian B. Hansen"], "title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach", "categories": ["econ.GN", "cs.AI", "cs.LG", "q-fin.EC"], "comment": null, "summary": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles."}
{"id": "2507.20800", "pdf": "https://arxiv.org/pdf/2507.20800", "abs": "https://arxiv.org/abs/2507.20800", "authors": ["Vinil Polepalli"], "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes."}
{"id": "2507.20810", "pdf": "https://arxiv.org/pdf/2507.20810", "abs": "https://arxiv.org/abs/2507.20810", "authors": ["Kaichen Ouyang"], "title": "Why Flow Matching is Particle Swarm Optimization?", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "7 pages, 0 figures", "summary": "This paper preliminarily investigates the duality between flow matching in\ngenerative models and particle swarm optimization (PSO) in evolutionary\ncomputation. Through theoretical analysis, we reveal the intrinsic connections\nbetween these two approaches in terms of their mathematical formulations and\noptimization mechanisms: the vector field learning in flow matching shares\nsimilar mathematical expressions with the velocity update rules in PSO; both\nmethods follow the fundamental framework of progressive evolution from initial\nto target distributions; and both can be formulated as dynamical systems\ngoverned by ordinary differential equations. Our study demonstrates that flow\nmatching can be viewed as a continuous generalization of PSO, while PSO\nprovides a discrete implementation of swarm intelligence principles. This\nduality understanding establishes a theoretical foundation for developing novel\nhybrid algorithms and creates a unified framework for analyzing both methods.\nAlthough this paper only presents preliminary discussions, the revealed\ncorrespondences suggest several promising research directions, including\nimproving swarm intelligence algorithms based on flow matching principles and\nenhancing generative models using swarm intelligence concepts."}
{"id": "2507.20836", "pdf": "https://arxiv.org/pdf/2507.20836", "abs": "https://arxiv.org/abs/2507.20836", "authors": ["Jakob Snel", "Seong Joon Oh"], "title": "First Hallucination Tokens Are Different from Conditional Ones", "categories": ["cs.LG", "cs.AI"], "comment": "4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination,\n  Trustworthiness", "summary": "Hallucination, the generation of untruthful content, is one of the major\nconcerns regarding foundational models. Detecting hallucinations at the token\nlevel is vital for real-time filtering and targeted correction, yet the\nvariation of hallucination signals within token sequences is not fully\nunderstood. Leveraging the RAGTruth corpus with token-level annotations and\nreproduced logits, we analyse how these signals depend on a token's position\nwithin hallucinated spans, contributing to an improved understanding of\ntoken-level hallucination. Our results show that the first hallucinated token\ncarries a stronger signal and is more detectable than conditional tokens. We\nrelease our analysis framework, along with code for logit reproduction and\nmetric computation at https://github.com/jakobsnl/RAGTruth_Xtended."}
{"id": "2507.20850", "pdf": "https://arxiv.org/pdf/2507.20850", "abs": "https://arxiv.org/abs/2507.20850", "authors": ["Meiting Dang", "Yanping Wu", "Yafei Wang", "Dezong Zhao", "David Flynn", "Chongfeng Wei"], "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments", "categories": ["cs.RO", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method."}
{"id": "2507.20853", "pdf": "https://arxiv.org/pdf/2507.20853", "abs": "https://arxiv.org/abs/2507.20853", "authors": ["Saket Tiwari", "Omer Gottesman", "George Konidaris"], "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the Thirteenth International Conference on Learning\n  Representations (ICLR 2025). arXiv admin note: text overlap with\n  arXiv:2301.00009", "summary": "Advances in reinforcement learning (RL) have led to its successful\napplication in complex tasks with continuous state and action spaces. Despite\nthese advances in practice, most theoretical work pertains to finite state and\naction spaces. We propose building a theoretical understanding of continuous\nstate and action spaces by employing a geometric lens to understand the locally\nattained set of states. The set of all parametrised policies learnt through a\nsemi-gradient based approach induces a set of attainable states in RL. We show\nthat the training dynamics of a two-layer neural policy induce a low\ndimensional manifold of attainable states embedded in the high-dimensional\nnominal state space trained using an actor-critic algorithm. We prove that,\nunder certain conditions, the dimensionality of this manifold is of the order\nof the dimensionality of the action space. This is the first result of its\nkind, linking the geometry of the state space to the dimensionality of the\naction space. We empirically corroborate this upper bound for four MuJoCo\nenvironments and also demonstrate the results in a toy environment with varying\ndimensionality. We also show the applicability of this theoretical result by\nintroducing a local manifold learning layer to the policy and value function\nnetworks to improve the performance in control environments with very high\ndegrees of freedom by changing one layer of the neural network to learn sparse\nrepresentations."}
{"id": "2507.20872", "pdf": "https://arxiv.org/pdf/2507.20872", "abs": "https://arxiv.org/abs/2507.20872", "authors": ["Ahmed Sharshar", "Yasser Ashraf", "Tameem Bakr", "Salma Hassan", "Hosam Elgendy", "Mohammad Yaqub", "Mohsen Guizani"], "title": "Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published in Third Workshop on Computer Vision for Automated Medical\n  Diagnosis CVAMD 2025 in ICCV 2025", "summary": "Alzheimer's disease affects over 55 million people worldwide and is projected\nto more than double by 2050, necessitating rapid, accurate, and scalable\ndiagnostics. However, existing approaches are limited because they cannot\nachieve clinically acceptable accuracy, generalization across datasets,\nrobustness to missing modalities, and explainability all at the same time. This\ninability to satisfy all these requirements simultaneously undermines their\nreliability in clinical settings. We propose OmniBrain, a multimodal framework\nthat integrates brain MRI, radiomics, gene expression, and clinical data using\na unified model with cross-attention and modality dropout. OmniBrain achieves\n$92.2 \\pm 2.4\\%$accuracy on the ANMerge dataset and generalizes to the MRI-only\nADNI dataset with $70.4 \\pm 2.7\\%$ accuracy, outperforming unimodal and prior\nmultimodal approaches. Explainability analyses highlight neuropathologically\nrelevant brain regions and genes, enhancing clinical trust. OmniBrain offers a\nrobust, interpretable, and practical solution for real-world Alzheimer's\ndiagnosis."}
{"id": "2507.20880", "pdf": "https://arxiv.org/pdf/2507.20880", "abs": "https://arxiv.org/abs/2507.20880", "authors": ["Renhang Liu", "Chia-Yu Hung", "Navonil Majumder", "Taylor Gautreaux", "Amir Ali Bagherzadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment", "categories": ["cs.SD", "cs.AI"], "comment": "https://github.com/declare-lab/jamify", "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes."}
{"id": "2507.20900", "pdf": "https://arxiv.org/pdf/2507.20900", "abs": "https://arxiv.org/abs/2507.20900", "authors": ["Yonghyun Kim", "Wayne Chi", "Anastasios N. Angelopoulos", "Wei-Lin Chiang", "Koichi Saito", "Shinji Watanabe", "Yuki Mitsufuji", "Chris Donahue"], "title": "Music Arena: Live Evaluation for Text-to-Music", "categories": ["cs.SD", "cs.AI", "cs.MM"], "comment": null, "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org"}
{"id": "2507.20907", "pdf": "https://arxiv.org/pdf/2507.20907", "abs": "https://arxiv.org/abs/2507.20907", "authors": ["Jeongun Ryu", "Heon Song", "Seungeun Lee", "Soo Ick Cho", "Jiwon Shin", "Kyunghyun Paeng", "Sérgio Pereira"], "title": "SCORPION: Addressing Scanner-Induced Variability in Histopathology", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in UNSURE 2025 workshop in MICCAI", "summary": "Ensuring reliable model performance across diverse domains is a critical\nchallenge in computational pathology. A particular source of variability in\nWhole-Slide Images is introduced by differences in digital scanners, thus\ncalling for better scanner generalization. This is critical for the real-world\nadoption of computational pathology, where the scanning devices may differ per\ninstitution or hospital, and the model should not be dependent on\nscanner-induced details, which can ultimately affect the patient's diagnosis\nand treatment planning. However, past efforts have primarily focused on\nstandard domain generalization settings, evaluating on unseen scanners during\ntraining, without directly evaluating consistency across scanners for the same\ntissue. To overcome this limitation, we introduce SCORPION, a new dataset\nexplicitly designed to evaluate model reliability under scanner variability.\nSCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding\n2,400 spatially aligned patches. This scanner-paired design allows for the\nisolation of scanner-induced variability, enabling a rigorous evaluation of\nmodel consistency while controlling for differences in tissue composition.\nFurthermore, we propose SimCons, a flexible framework that combines\naugmentation-based domain generalization techniques with a consistency loss to\nexplicitly address scanner generalization. We empirically show that SimCons\nimproves model consistency on varying scanners without compromising\ntask-specific performance. By releasing the SCORPION dataset and proposing\nSimCons, we provide the research community with a crucial resource for\nevaluating and improving model consistency across diverse scanners, setting a\nnew standard for reliability testing."}
{"id": "2507.20913", "pdf": "https://arxiv.org/pdf/2507.20913", "abs": "https://arxiv.org/abs/2507.20913", "authors": ["Jialei Cui", "Jianwei Du", "Yanzhe Li", "Lei Gao", "Hui Jiang", "Chenfu Bao"], "title": "HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid evolution of face manipulation techniques poses a critical\nchallenge for face forgery detection: cross-domain generalization. Conventional\nmethods, which rely on simple classification objectives, often fail to learn\ndomain-invariant representations. We propose HAMLET-FFD, a cognitively inspired\nHierarchical Adaptive Multi-modal Learning framework that tackles this\nchallenge via bidirectional cross-modal reasoning. Building on contrastive\nvision-language models such as CLIP, HAMLET-FFD introduces a knowledge\nrefinement loop that iteratively assesses authenticity by integrating visual\nevidence with conceptual cues, emulating expert forensic analysis. A key\ninnovation is a bidirectional fusion mechanism in which textual authenticity\nembeddings guide the aggregation of hierarchical visual features, while\nmodulated visual features refine text embeddings to generate image-adaptive\nprompts. This closed-loop process progressively aligns visual observations with\nsemantic priors to enhance authenticity assessment. By design, HAMLET-FFD\nfreezes all pretrained parameters, serving as an external plugin that preserves\nCLIP's original capabilities. Extensive experiments demonstrate its superior\ngeneralization to unseen manipulations across multiple benchmarks, and visual\nanalyses reveal a division of labor among embeddings, with distinct\nrepresentations specializing in fine-grained artifact recognition."}
{"id": "2507.20917", "pdf": "https://arxiv.org/pdf/2507.20917", "abs": "https://arxiv.org/abs/2507.20917", "authors": ["Adrien Bazoge"], "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces MediQAl, a French medical question answering dataset\ndesigned to evaluate the capabilities of language models in factual medical\nrecall and reasoning over real-world clinical scenarios. MediQAl contains\n32,603 questions sourced from French medical examinations across 41 medical\nsubjects. The dataset includes three tasks: (i) Multiple-Choice Question with\nUnique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)\nOpen-Ended Question with Short-Answer. Each question is labeled as\nUnderstanding or Reasoning, enabling a detailed analysis of models' cognitive\ncapabilities. We validate the MediQAl dataset through extensive evaluation with\n14 large language models, including recent reasoning-augmented models, and\nobserve a significant performance gap between factual recall and reasoning\ntasks. Our evaluation provides a comprehensive benchmark for assessing language\nmodels' performance on French medical question answering, addressing a crucial\ngap in multilingual resources for the medical domain."}
{"id": "2507.20919", "pdf": "https://arxiv.org/pdf/2507.20919", "abs": "https://arxiv.org/abs/2507.20919", "authors": ["Aman Shukla", "Daniel Patrick Scantlebury", "Rishabh Kumar"], "title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "Best Paper, NewInML @ ICML 2025", "summary": "Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications."}
{"id": "2507.20923", "pdf": "https://arxiv.org/pdf/2507.20923", "abs": "https://arxiv.org/abs/2507.20923", "authors": ["Minh Hieu Ha", "Hung Phan", "Tung Duy Doan", "Tung Dao", "Dao Tran", "Huynh Thi Thanh Binh"], "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization", "categories": ["cs.NE", "cs.AI"], "comment": "36 pages, 20 figures", "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE."}
{"id": "2507.20924", "pdf": "https://arxiv.org/pdf/2507.20924", "abs": "https://arxiv.org/abs/2507.20924", "authors": ["Roberto Labadie-Tamayo", "Adrian Jaques Böck", "Djordje Slijepčević", "Xihui Chen", "Andreas Babic", "Matthias Zeppelzauer"], "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "I.2"], "comment": "12 pages", "summary": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish."}
{"id": "2507.20930", "pdf": "https://arxiv.org/pdf/2507.20930", "abs": "https://arxiv.org/abs/2507.20930", "authors": ["Likun Tan", "Kuan-Wei Huang", "Kevin Wu"], "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/fine-grained-editting."}
{"id": "2507.20936", "pdf": "https://arxiv.org/pdf/2507.20936", "abs": "https://arxiv.org/abs/2507.20936", "authors": ["Ansh Poonia", "Maeghal Jain"], "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages", "summary": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities."}
{"id": "2507.20941", "pdf": "https://arxiv.org/pdf/2507.20941", "abs": "https://arxiv.org/abs/2507.20941", "authors": ["Sacha Braun", "Eugène Berta", "Michael I. Jordan", "Francis Bach"], "title": "Multivariate Conformal Prediction via Conformalized Gaussian Scoring", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "stat.OT"], "comment": null, "summary": "While achieving exact conditional coverage in conformal prediction is\nunattainable without making strong, untestable regularity assumptions, the\npromise of conformal prediction hinges on finding approximations to conditional\nguarantees that are realizable in practice. A promising direction for obtaining\nconditional dependence for conformal sets--in particular capturing\nheteroskedasticity--is through estimating the conditional density\n$\\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this\nvein has focused on nonconformity scores based on the empirical cumulative\ndistribution function (CDF). Such scores are, however, computationally costly,\ntypically requiring expensive sampling methods. To avoid the need for sampling,\nwe observe that the CDF-based score reduces to a Mahalanobis distance in the\ncase of Gaussian scores, yielding a closed-form expression that can be directly\nconformalized. Moreover, the use of a Gaussian-based score opens the door to a\nnumber of extensions of the basic conformal method; in particular, we show how\nto construct conformal sets with missing output values, refine conformal sets\nas partial information about $Y$ becomes available, and construct conformal\nsets on transformations of the output space. Finally, empirical results\nindicate that our approach produces conformal sets that more closely\napproximate conditional coverage in multivariate settings compared to\nalternative methods."}
{"id": "2507.20956", "pdf": "https://arxiv.org/pdf/2507.20956", "abs": "https://arxiv.org/abs/2507.20956", "authors": ["Max Peeperkorn", "Tom Kouwenhoven", "Dan Brown", "Anna Jordanous"], "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality."}
{"id": "2507.20957", "pdf": "https://arxiv.org/pdf/2507.20957", "abs": "https://arxiv.org/abs/2507.20957", "authors": ["Hoyoung Lee", "Junhyuk Seo", "Suhwan Park", "Junhyeong Lee", "Wonbin Ahn", "Chanyeol Choi", "Alejandro Lopez-Lira", "Yongjae Lee"], "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "categories": ["q-fin.PM", "cs.AI", "cs.CL"], "comment": null, "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence."}
{"id": "2507.20966", "pdf": "https://arxiv.org/pdf/2507.20966", "abs": "https://arxiv.org/abs/2507.20966", "authors": ["Hussein A. Ammar", "Raviraj Adve", "Shahram Shahbazpanahi", "Gary Boudreau", "Israfil Bahceci"], "title": "Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL", "categories": ["cs.IT", "cs.AI", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "comment": "Published in IEEE Transactions on Communications (IEEE TCOM)", "summary": "In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user\nmobility necessitates updating the set of serving access points to maintain the\nuser-centric clustering. Such updates are typically performed through handoff\n(HO) operations; however, frequent HOs lead to overheads associated with the\nallocation and release of resources. This paper presents a deep reinforcement\nlearning (DRL)-based solution to predict and manage these connections for\nmobile users. Our solution employs the Soft Actor-Critic algorithm, with\ncontinuous action space representation, to train a deep neural network to serve\nas the HO policy. We present a novel proposition for a reward function that\nintegrates a HO penalty in order to balance the attainable rate and the\nassociated overhead related to HOs. We develop two variants of our system; the\nfirst one uses mobility direction-assisted (DA) observations that are based on\nthe user movement pattern, while the second one uses history-assisted (HA)\nobservations that are based on the history of the large-scale fading (LSF).\nSimulation results show that our DRL-based continuous action space approach is\nmore scalable than discrete space counterpart, and that our derived HO policy\nautomatically learns to gather HOs in specific time slots to minimize the\noverhead of initiating HOs. Our solution can also operate in real time with a\nresponse time less than 0.4 ms."}
{"id": "2507.20968", "pdf": "https://arxiv.org/pdf/2507.20968", "abs": "https://arxiv.org/abs/2507.20968", "authors": ["Rongyao Cai", "Ming Jin", "Qingsong Wen", "Kexin Zhang"], "title": "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Domain shift poses a fundamental challenge in time series analysis, where\nmodels trained on source domain often fail dramatically when applied in target\ndomain with different yet similar distributions. While current unsupervised\ndomain adaptation (UDA) methods attempt to align cross-domain feature\ndistributions, they typically treat features as indivisible entities, ignoring\ntheir intrinsic compositions that governs domain adaptation. We introduce\nDARSD, a novel UDA framework with theoretical explainability that explicitly\nrealizes UDA tasks from the perspective of representation space decomposition.\nOur core insight is that effective domain adaptation requires not just\nalignment, but principled disentanglement of transferable knowledge from mixed\nrepresentations. DARSD consists three synergistic components: (I) An\nadversarial learnable common invariant basis that projects original features\ninto a domain-invariant subspace while preserving semantic content; (II) A\nprototypical pseudo-labeling mechanism that dynamically separates target\nfeatures based on confidence, hindering error accumulation; (III) A hybrid\ncontrastive optimization strategy that simultaneously enforces feature\nclustering and consistency while mitigating emerging distribution gaps.\nComprehensive experiments conducted on four benchmark datasets (WISDM, HAR,\nHHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,\nachieving optimal performance in 35 out of 53 cross-domain scenarios."}
{"id": "2507.20984", "pdf": "https://arxiv.org/pdf/2507.20984", "abs": "https://arxiv.org/abs/2507.20984", "authors": ["Yixin Song", "Zhenliang Xue", "Dongliang Wei", "Feiyang Chen", "Jianxiang Gao", "Junchen Liu", "Hangyu Liang", "Guangshuo Qin", "Chengrong Tian", "Bo Wen", "Longyu Zhao", "Xinrui Zheng", "Zeyu Mi", "Haibo Chen"], "title": "SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."}
{"id": "2507.20987", "pdf": "https://arxiv.org/pdf/2507.20987", "abs": "https://arxiv.org/abs/2507.20987", "authors": ["Xinhan Di", "Kristin Qi", "Pengqian Yu"], "title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1", "categories": ["cs.CV", "cs.AI"], "comment": "WiCV @ ICCV 2025", "summary": "Recent advances in diffusion-based video generation have enabled\nphoto-realistic short clips, but current methods still struggle to achieve\nmulti-modal consistency when jointly generating whole-body motion and natural\nspeech. Current approaches lack comprehensive evaluation frameworks that assess\nboth visual and audio quality, and there are insufficient benchmarks for\nregion-specific performance analysis. To address these gaps, we introduce the\nJoint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),\ncomprising a large-scale multi-modal dataset with 10,000 unique identities\nacross 2 million video samples, and an evaluation protocol for assessing joint\naudio-video generation of whole-body animatable avatars. Our evaluation of SOTA\nmodels reveals consistent performance disparities between face/hand-centric and\nwhole-body performance, which incidates essential areas for future research.\nThe dataset and evaluation tools are publicly available at\nhttps://github.com/deepreasonings/WholeBodyBenchmark."}
{"id": "2507.20993", "pdf": "https://arxiv.org/pdf/2507.20993", "abs": "https://arxiv.org/abs/2507.20993", "authors": ["Henri Arno", "Thomas Demeester"], "title": "Personalized Treatment Effect Estimation from Unstructured Data", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity."}
{"id": "2507.20994", "pdf": "https://arxiv.org/pdf/2507.20994", "abs": "https://arxiv.org/abs/2507.20994", "authors": ["Shen Li", "Liuyi Yao", "Wujia Niu", "Lan Zhang", "Yaliang Li"], "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM", "categories": ["cs.CV", "cs.AI"], "comment": "Codes and data are available at\n  https://github.com/listen0425/Security-Tensors", "summary": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality."}
{"id": "2507.20997", "pdf": "https://arxiv.org/pdf/2507.20997", "abs": "https://arxiv.org/abs/2507.20997", "authors": ["Haris Khan", "Shumaila Asif", "Sadia Asif"], "title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 6 figures, 3 tables. Will be Submitted to ICLR 2025 for\n  review", "summary": "In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design."}
{"id": "2507.21004", "pdf": "https://arxiv.org/pdf/2507.21004", "abs": "https://arxiv.org/abs/2507.21004", "authors": ["Fang Li"], "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount."}
{"id": "2507.21009", "pdf": "https://arxiv.org/pdf/2507.21009", "abs": "https://arxiv.org/abs/2507.21009", "authors": ["Danil Savine", "Muni Sreenivas Pydi", "Jamal Atif", "Olivier Cappé"], "title": "Memorization in Fine-Tuned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."}
