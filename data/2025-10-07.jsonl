{"id": "2510.03285", "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents."}
{"id": "2510.03377", "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "categories": ["cs.AI"], "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method."}
{"id": "2510.03399", "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness."}
{"id": "2510.03418", "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance."}
{"id": "2510.03453", "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "categories": ["cs.AI"], "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures."}
{"id": "2510.03469", "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration."}
{"id": "2510.03485", "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "categories": ["cs.AI", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales."}
{"id": "2510.03506", "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "categories": ["cs.AI"], "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation."}
{"id": "2510.03605", "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures."}
{"id": "2510.03612", "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society."}
{"id": "2510.03632", "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "categories": ["cs.AI"], "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning."}
{"id": "2510.03680", "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "categories": ["cs.AI"], "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding."}
{"id": "2510.03696", "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements."}
{"id": "2510.03700", "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "categories": ["cs.AI"], "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed."}
{"id": "2510.03727", "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space."}
{"id": "2510.03771", "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "categories": ["cs.AI"], "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline."}
{"id": "2510.03777", "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS."}
{"id": "2510.03845", "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency."}
{"id": "2510.03847", "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "categories": ["cs.AI", "cs.LG"], "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference"}
{"id": "2510.03851", "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "categories": ["cs.AI"], "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."}
{"id": "2510.03859", "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT"}
{"id": "2510.03863", "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "categories": ["cs.AI", "cs.CR"], "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI."}
{"id": "2510.03886", "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "categories": ["cs.AI"], "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface."}
{"id": "2510.03892", "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI."}
{"id": "2510.03969", "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs."}
{"id": "2510.04009", "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI."}
{"id": "2510.04017", "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva Rühling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work."}
{"id": "2510.04023", "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents."}
{"id": "2510.04033", "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "categories": ["cs.AI"], "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."}
{"id": "2510.04040", "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs."}
{"id": "2510.04048", "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "categories": ["cs.AI"], "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer."}
{"id": "2510.04051", "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "categories": ["cs.AI"], "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences."}
{"id": "2510.04064", "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "categories": ["cs.AI"], "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced."}
{"id": "2510.04073", "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "categories": ["cs.AI"], "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication."}
{"id": "2510.04089", "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows."}
{"id": "2510.04093", "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "categories": ["cs.AI"], "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM."}
{"id": "2510.04097", "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics."}
{"id": "2510.04116", "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly."}
{"id": "2510.04128", "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking."}
{"id": "2510.04140", "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online."}
{"id": "2510.04141", "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems."}
{"id": "2510.04173", "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "categories": ["cs.AI"], "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments."}
{"id": "2510.04195", "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "categories": ["cs.AI"], "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents."}
{"id": "2510.04196", "pdf": "https://arxiv.org/pdf/2510.04196", "abs": "https://arxiv.org/abs/2510.04196", "authors": ["Yizhuo Ding", "Mingkang Chen", "Qiuhua Liu", "Fenghua Weng", "Wanying Qu", "Yue Yang", "Yugang Jiang", "Zuxuan Wu", "Yanwei Fu", "Wenqi Shao"], "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications,\nwhere they must be both useful and safe. Safety is especially challenging in\nmultimodal settings: images and text can be combined to bypass guardrails, and\nsingle objective training can cause policy drift that yields over-refusal on\nbenign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed\nreinforcement learning framework that trains reasoning oriented LMRMs under\nmultimodal, multitask, and multiobjective signals, and we release the resulting\nmodel, COSMO-R1. Our approach aims to let safety and capability grow together\nin one stable pipeline rather than competing during alignment. In experiments,\nCOSMO-R1 improves safety while maintaining-and often improving multimodal\nreasoning and instruction following, shows stronger robustness to multimodal\njailbreaks, and reduces unnecessary refusals. The framework also transfers\nacross backbones with consistent gains. Ablations support the design choices,\nindicating a simple path to advancing safety and general capability together in\nLMRMs."}
{"id": "2510.04206", "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}."}
{"id": "2510.04265", "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit"}
{"id": "2510.04272", "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings."}
{"id": "2510.04281", "pdf": "https://arxiv.org/pdf/2510.04281", "abs": "https://arxiv.org/abs/2510.04281", "authors": ["Zhuangzhi Gao", "Hongyi Qin", "He Zhao", "Qinkai Yu", "Feixiang Zhou", "Eduard Shantsila", "Uazman Alam", "Alena Shantsila", "Wahbi El-Bouri", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "categories": ["cs.AI"], "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and\n  Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)", "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse\ndata modalities, but current medical adaptations such as LLaVA-Med often fail\nto fully exploit the synergy between color fundus photography (CFP) and optical\ncoherence tomography (OCT), and offer limited interpretability of quantitative\nbiomarkers. We introduce GROK, a grounded multimodal large language model that\njointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of\nocular and systemic disease. GROK comprises three core modules:\nKnowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,\nand Supervised Instruction Fine-Tuning, which together establish a\nquantitative-to-qualitative diagnostic chain of thought, mirroring real\nclinical reasoning when producing detailed lesion annotations. To evaluate our\napproach, we introduce the Grounded Ophthalmic Understanding benchmark, which\ncovers six disease categories and three tasks: macro-level diagnostic\nclassification, report generation quality, and fine-grained clinical assessment\nof the generated chain of thought. Experiments show that, with only LoRA\n(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK\noutperforms comparable 7B and 32B baselines on both report quality and\nfine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are\npublicly available in the GROK repository."}
{"id": "2510.04284", "pdf": "https://arxiv.org/pdf/2510.04284", "abs": "https://arxiv.org/abs/2510.04284", "authors": ["Yunghwei Lai", "Kaiming Liu", "Ziyue Wang", "Weizhi Ma", "Yang Liu"], "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "The professionalism of a human doctor in outpatient service depends on two\ncore abilities: the ability to make accurate medical decisions and the medical\nconsultation skill to conduct strategic, empathetic patient inquiry. Existing\nLarge Language Models (LLMs) have achieved remarkable accuracy on medical\ndecision-making benchmarks. However, they often lack the ability to conduct the\nstrategic and empathetic consultation, which is essential for real-world\nclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor\nagent trained to master both of the capabilities by ask high-yield questions\nand conduct strategic multi-turn inquiry to guide decision-making. Our\nframework introduces three key components: a multi-agent interactive\nenvironment, a two-tiered reward architecture that separately optimizes\nclinical decision-making and communicative inquiry skills, and an experience\nrepository to ground policy learning in high-quality prior trajectories. We\nevaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across\nmulti-facet metrics, such as communication quality, user experience, and task\naccuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source\nspecialized LLMs by a substantial margin with higher parameter efficiency and\noutperforms powerful proprietary models. Furthermore, the human evaluations\nshow a strong preference for Doctor-R1 to generate human-preferred clinical\ndialogue, demonstrating the effectiveness of the framework."}
{"id": "2510.04311", "pdf": "https://arxiv.org/pdf/2510.04311", "abs": "https://arxiv.org/abs/2510.04311", "authors": ["Bohan Tang", "Huidong Liang", "Keyue Jiang", "Xiaowen Dong"], "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks."}
{"id": "2510.04371", "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "categories": ["cs.AI", "cs.DC", "cs.MA"], "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world."}
{"id": "2510.04373", "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints."}
{"id": "2510.04384", "pdf": "https://arxiv.org/pdf/2510.04384", "abs": "https://arxiv.org/abs/2510.04384", "authors": ["Adam Ballew", "Jingbo Wang", "Shaogang Ren"], "title": "LLM Based Bayesian Optimization for Prompt Search", "categories": ["cs.AI"], "comment": null, "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper."}
{"id": "2510.04391", "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence."}
{"id": "2510.04399", "pdf": "https://arxiv.org/pdf/2510.04399", "abs": "https://arxiv.org/abs/2510.04399", "authors": ["Charles L. Wang", "Keir Dorchen", "Peter Jin"], "title": "Utility-Learning Tension in Self-Modifying Agents", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability."}
{"id": "2510.04474", "pdf": "https://arxiv.org/pdf/2510.04474", "abs": "https://arxiv.org/abs/2510.04474", "authors": ["Gang Li", "Yan Chen", "Ming Lin", "Tianbao Yang"], "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction."}
{"id": "2510.04480", "pdf": "https://arxiv.org/pdf/2510.04480", "abs": "https://arxiv.org/abs/2510.04480", "authors": ["Yunuo Cen", "Zixuan Wang", "Jintao Zhang", "Zhiwei Zhang", "Xuanyao Fong"], "title": "On Continuous Optimization for Constraint Satisfaction Problems", "categories": ["cs.AI"], "comment": null, "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics,\nphysics, and theoretical computer science. While conflict-driven clause\nlearning Boolean Satisfiability (SAT) solvers have achieved remarkable success\nand become the mainstream approach for Boolean satisfiability, recent advances\nshow that modern continuous local search (CLS) solvers can achieve highly\ncompetitive results on certain classes of SAT problems. Motivated by these\nadvances, we extend the CLS framework from Boolean SAT to general CSP with\nfinite-domain variables and expressive constraints. We present FourierCSP, a\ncontinuous optimization framework that generalizes the Walsh-Fourier transform\nto CSP, allowing for transforming versatile constraints to compact multilinear\npolynomials, thereby avoiding the need for auxiliary variables and\nmemory-intensive encodings. Our approach leverages efficient evaluation and\ndifferentiation of the objective via circuit-output probability and employs a\nprojected gradient optimization method with theoretical guarantees. Empirical\nresults on benchmark suites demonstrate that FourierCSP is scalable and\ncompetitive, significantly broadening the class of problems that can be\nefficiently solved by CLS techniques."}
{"id": "2510.04488", "pdf": "https://arxiv.org/pdf/2510.04488", "abs": "https://arxiv.org/abs/2510.04488", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning", "categories": ["cs.AI", "cs.IT", "math.IT", "I.2.4"], "comment": "27 pages, 5 figures, 21 tables", "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller."}
{"id": "2510.04491", "pdf": "https://arxiv.org/pdf/2510.04491", "abs": "https://arxiv.org/abs/2510.04491", "authors": ["Muyu He", "Anand Kumar", "Tsach Mackey", "Meghana Rajeev", "James Zou", "Nazneen Rajani"], "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "categories": ["cs.AI", "cs.CL"], "comment": "25 pages", "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait."}
{"id": "2510.04514", "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents."}
{"id": "2510.04520", "pdf": "https://arxiv.org/pdf/2510.04520", "abs": "https://arxiv.org/abs/2510.04520", "authors": ["Hanyu Wang", "Ruohan Xie", "Yutong Wang", "Guoxiong Gao", "Xintao Yu", "Bin Dong"], "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph", "categories": ["cs.AI"], "comment": null, "summary": "Accurate auto-formalization of theorem statements is essential for advancing\nautomated discovery and verification of research-level mathematics, yet remains\na major bottleneck for LLMs due to hallucinations, semantic mismatches, and\ntheir inability to synthesize new definitions. To tackle these issues, we\npresent Aria (Agent for Retrieval and Iterative Autoformalization), a system\nfor conjecture-level formalization in Lean that emulates human expert reasoning\nvia a two-phase Graph-of-Thought process: recursively decomposing statements\ninto a dependency graph and then constructing formalizations from grounded\nconcepts. To ensure semantic correctness, we introduce AriaScorer, a checker\nthat retrieves definitions from Mathlib for term-level grounding, enabling\nrigorous and reliable verification. We evaluate Aria on diverse benchmarks. On\nProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,\nsurpassing previous methods. On FATE-X, a suite of challenging algebra problems\nfrom research literature, it outperforms the best baseline with 44.0% vs. 24.0%\nfinal accuracy. On a dataset of homological conjectures, Aria reaches 42.9%\nfinal accuracy while all other models score 0%."}
{"id": "2510.04532", "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models."}
{"id": "2510.04542", "pdf": "https://arxiv.org/pdf/2510.04542", "abs": "https://arxiv.org/abs/2510.04542", "authors": ["Wolfgang Lehrach", "Daniel Hennes", "Miguel Lazaro-Gredilla", "Xinghua Lou", "Carter Wendelken", "Zun Li", "Antoine Dedieu", "Jordi Grau-Moya", "Marc Lanctot", "Atil Iscen", "John Schultz", "Marcus Chiam", "Ian Gemp", "Piotr Zielinski", "Satinder Singh", "Kevin P. Murphy"], "title": "Code World Models for General Game Playing", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games."}
{"id": "2510.04550", "pdf": "https://arxiv.org/pdf/2510.04550", "abs": "https://arxiv.org/abs/2510.04550", "authors": ["Pengfei He", "Zhenwei Dai", "Bing He", "Hui Liu", "Xianfeng Tang", "Hanqing Lu", "Juanhui Li", "Jiayuan Ding", "Subhabrata Mukherjee", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use."}
{"id": "2510.04560", "pdf": "https://arxiv.org/pdf/2510.04560", "abs": "https://arxiv.org/abs/2510.04560", "authors": ["Honghao Fu", "Yuan Ouyang", "Kai-Wei Chang", "Yiwei Wang", "Zi Huang", "Yujun Cai"], "title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL."}
{"id": "2510.04568", "pdf": "https://arxiv.org/pdf/2510.04568", "abs": "https://arxiv.org/abs/2510.04568", "authors": ["Naman Gupta", "Shreeyash Gowaikar", "Arun Iyer", "Kirankumar Shiragur", "Ramakrishna B Bairi", "Rishikesh Maurya", "Ritabrata Maiti", "Sankarshan Damle", "Shachee Mishra Gupta"], "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline."}
{"id": "2510.04580", "pdf": "https://arxiv.org/pdf/2510.04580", "abs": "https://arxiv.org/abs/2510.04580", "authors": ["Tomoyuki Kaneko", "Shuhei Yamashita"], "title": "Strongly Solving 2048 4x3", "categories": ["cs.AI"], "comment": null, "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,\nwhere a player chooses a direction among up, down, left, and right to obtain a\nscore by merging two tiles with the same number located in neighboring cells\nalong the chosen direction. This paper presents that a variant 2048-4x3 12\ncells on a 4 by 3 board, one row smaller than the original, has been strongly\nsolved. In this variant, the expected score achieved by an optimal strategy is\nabout $50724.26$ for the most common initial states: ones with two tiles of\nnumber 2. The numbers of reachable states and afterstates are identified to be\n$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is\nto partition state space by the sum of tile numbers on a board, which we call\nthe age of a state. An age is invariant between a state and its successive\nafterstate after any valid action and is increased two or four by stochastic\nresponse from the environment. Therefore, we can partition state space by ages\nand enumerate all (after)states of an age depending only on states with the\nrecent ages. Similarly, we can identify (after)state values by going along with\nages in decreasing order."}
{"id": "2510.04588", "pdf": "https://arxiv.org/pdf/2510.04588", "abs": "https://arxiv.org/abs/2510.04588", "authors": ["Shurui Li"], "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma", "categories": ["cs.AI"], "comment": null, "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the\nepistemological foundations upon which we attribute consciousness. As AI\nsystems increasingly mimic human behavior and interaction with high fidelity,\nthe concept of a \"perfect mimic\"-an entity empirically indistinguishable from a\nhuman through observation and interaction-shifts from hypothetical to\ntechnologically plausible. This paper argues that such developments pose a\nfundamental challenge to the consistency of our mind-recognition practices.\nConsciousness attributions rely heavily, if not exclusively, on empirical\nevidence derived from behavior and interaction. If a perfect mimic provides\nevidence identical to that of humans, any refusal to grant it equivalent\nepistemic status must invoke inaccessible factors, such as qualia, substrate\nrequirements, or origin. Selectively invoking such factors risks a debilitating\ndilemma: either we undermine the rational basis for attributing consciousness\nto others (epistemological solipsism), or we accept inconsistent reasoning. I\ncontend that epistemic consistency demands we ascribe the same status to\nempirically indistinguishable entities, regardless of metaphysical assumptions.\nThe perfect mimic thus acts as an epistemic mirror, forcing critical reflection\non the assumptions underlying intersubjective recognition in light of advancing\nAI. This analysis carries significant implications for theories of\nconsciousness and ethical frameworks concerning artificial agents."}
{"id": "2510.04617", "pdf": "https://arxiv.org/pdf/2510.04617", "abs": "https://arxiv.org/abs/2510.04617", "authors": ["Zhejian Lai", "Xiang Geng", "Zhijun Wang", "Yang Bai", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xuezhi Cao", "Xunliang Cai", "Shujian Huang"], "title": "Making Mathematical Reasoning Adaptive", "categories": ["cs.AI"], "comment": null, "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR"}
{"id": "2510.04623", "pdf": "https://arxiv.org/pdf/2510.04623", "abs": "https://arxiv.org/abs/2510.04623", "authors": ["Shrish Shrinath Vaidya", "Gowthamaan Palani", "Sidharth Ramesh", "Velmurugan Balasubramanian", "Minmini Selvam", "Gokulraja Srinivasaraja", "Ganapathy Krishnamurthi"], "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports", "categories": ["cs.AI"], "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025", "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent"}
{"id": "2510.04643", "pdf": "https://arxiv.org/pdf/2510.04643", "abs": "https://arxiv.org/abs/2510.04643", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading", "categories": ["cs.AI"], "comment": "This paper has been accepted by EMNLP 2025", "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/)."}
{"id": "2510.04670", "pdf": "https://arxiv.org/pdf/2510.04670", "abs": "https://arxiv.org/abs/2510.04670", "authors": ["Xuanhua Yin", "Runkai Zhao", "Weidong Cai"], "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing", "categories": ["cs.AI"], "comment": "8 pages, 4 figures", "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion\nstyles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic\nFramework for Multimodal fMRI Response Encoding), an agnostic interface that\nstandardizes time-aligned post-fusion tokens from varied encoders, and MIND, a\nplug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.\nTrained end-to-end for whole-brain prediction, AFIRE decouples the decoder from\nupstream fusion, while MIND combines token-dependent Top-K sparse routing with\na subject prior to personalize expert usage without sacrificing generality.\nExperiments across multiple multimodal backbones and subjects show consistent\nimprovements over strong baselines, enhanced cross-subject generalization, and\ninterpretable expert patterns that correlate with content type. The framework\noffers a simple attachment point for new encoders and datasets, enabling\nrobust, plug-and-improve performance for naturalistic neuroimaging studies."}
{"id": "2510.04673", "pdf": "https://arxiv.org/pdf/2510.04673", "abs": "https://arxiv.org/abs/2510.04673", "authors": ["Chan Hee Song", "Yiwen Song", "Palash Goyal", "Yu Su", "Oriana Riva", "Hamid Palangi", "Tomas Pfister"], "title": "Watch and Learn: Learning to Use Computers from Online Videos", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment."}
{"id": "2510.04695", "pdf": "https://arxiv.org/pdf/2510.04695", "abs": "https://arxiv.org/abs/2510.04695", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives."}
{"id": "2510.04721", "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior."}
{"id": "2510.04765", "pdf": "https://arxiv.org/pdf/2510.04765", "abs": "https://arxiv.org/abs/2510.04765", "authors": ["Jinbo Wen", "Jiawen Kang", "Linfeng Zhang", "Xiaoying Tang", "Jianhang Tang", "Yang Zhang", "Zhaohui Yang", "Dusit Niyato"], "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0", "categories": ["cs.AI"], "comment": null, "summary": "Web 3.0 represents the next generation of the Internet, which is widely\nrecognized as a decentralized ecosystem that focuses on value expression and\ndata ownership. By leveraging blockchain and artificial intelligence\ntechnologies, Web 3.0 offers unprecedented opportunities for users to create,\nown, and monetize their content, thereby enabling User-Generated Content (UGC)\nto an entirely new level. However, some self-interested users may exploit the\nlimitations of content curation mechanisms and generate low-quality content\nwith less effort, obtaining platform rewards under information asymmetry. Such\nbehavior can undermine Web 3.0 performance. To this end, we propose\n\\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive\nmechanism for UGC in Web 3.0. Specifically, we propose an LMM-based\ncontract-theoretic model to motivate users to generate high-quality UGC,\nthereby mitigating the adverse selection problem from information asymmetry. To\nalleviate potential moral hazards after contract selection, we leverage LMM\nagents to evaluate UGC quality, which is the primary component of the contract,\nutilizing prompt engineering techniques to improve the evaluation performance\nof LMM agents. Recognizing that traditional contract design methods cannot\neffectively adapt to the dynamic environment of Web 3.0, we develop an improved\nMixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for\noptimal contract design. Simulation results demonstrate the superiority of the\nproposed MoE-based PPO algorithm over representative benchmarks in the context\nof contract design. Finally, we deploy the designed contract within an Ethereum\nsmart contract framework, further validating the effectiveness of the proposed\nscheme."}
{"id": "2510.04792", "pdf": "https://arxiv.org/pdf/2510.04792", "abs": "https://arxiv.org/abs/2510.04792", "authors": ["Ni Zhang", "Zhiguang Cao"], "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems", "categories": ["cs.AI"], "comment": "Accepted by NeurIPS 2025", "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach."}
{"id": "2510.04817", "pdf": "https://arxiv.org/pdf/2510.04817", "abs": "https://arxiv.org/abs/2510.04817", "authors": ["Abhinav Madahar"], "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference."}
{"id": "2510.04851", "pdf": "https://arxiv.org/pdf/2510.04851", "abs": "https://arxiv.org/abs/2510.04851", "authors": ["Dongge Han", "Camille Couturier", "Daniel Madrigal Diaz", "Xuchao Zhang", "Victor Rühle", "Saravan Rajmohan"], "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation."}
{"id": "2510.04862", "pdf": "https://arxiv.org/pdf/2510.04862", "abs": "https://arxiv.org/abs/2510.04862", "authors": ["Sam Earle", "Zehua Jiang", "Eugene Vinitsky", "Julian Togelius"], "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at\n  the AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment 2025", "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale."}
{"id": "2510.04886", "pdf": "https://arxiv.org/pdf/2510.04886", "abs": "https://arxiv.org/abs/2510.04886", "authors": ["Adi Banerjee", "Anirudh Nair", "Tarik Borogovac"], "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems."}
{"id": "2510.04899", "pdf": "https://arxiv.org/pdf/2510.04899", "abs": "https://arxiv.org/abs/2510.04899", "authors": ["Keane Ong", "Wei Dai", "Carol Li", "Dewei Feng", "Hengzhi Li", "Jingyao Wu", "Jiaee Cheong", "Rui Mao", "Gianmarco Mengaldo", "Erik Cambria", "Paul Pu Liang"], "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding", "categories": ["cs.AI"], "comment": null, "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains."}
{"id": "2510.04935", "pdf": "https://arxiv.org/pdf/2510.04935", "abs": "https://arxiv.org/abs/2510.04935", "authors": ["Guoxin Chen", "Zile Qiao", "Wenqing Wang", "Donglei Yu", "Xuanzhong Chen", "Hao Sun", "Minpeng Liao", "Kai Fan", "Yong Jiang", "Penguin Xie", "Wayne Xin Zhao", "Ruihua Song", "Fei Huang"], "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Ongoing Work", "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments."}
{"id": "2510.04952", "pdf": "https://arxiv.org/pdf/2510.04952", "abs": "https://arxiv.org/abs/2510.04952", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits", "categories": ["cs.AI", "cs.DC"], "comment": "22 pages, 2 figures", "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment."}
{"id": "2510.04978", "pdf": "https://arxiv.org/pdf/2510.04978", "abs": "https://arxiv.org/abs/2510.04978", "authors": ["Kun Xiang", "Terry Jingchen Zhang", "Yinya Huang", "Jixi He", "Zirong Liu", "Yueling Tang", "Ruizhe Zhou", "Lijing Luo", "Youpeng Wen", "Xiuwei Chen", "Bingqian Lin", "Jianhua Han", "Hang Xu", "Hanhui Li", "Bin Dong", "Xiaodan Liang"], "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics."}
{"id": "2510.04980", "pdf": "https://arxiv.org/pdf/2510.04980", "abs": "https://arxiv.org/abs/2510.04980", "authors": ["Fangzhou Liang", "Tianshi Zheng", "Chunkit Chan", "Yauwai Yim", "Yangqiu Song"], "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game", "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025 Wordplay", "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models."}
{"id": "2510.05014", "pdf": "https://arxiv.org/pdf/2510.05014", "abs": "https://arxiv.org/abs/2510.05014", "authors": ["Xuanming Cui", "Jianpeng Cheng", "Hong-you Chen", "Satya Narayan Shukla", "Abhijeet Awasthi", "Xichen Pan", "Chaitanya Ahuja", "Shlok Kumar Mishra", "Qi Guo", "Ser-Nam Lim", "Aashu Singh", "Xiangjun Fan"], "title": "Think Then Embed: Generative Context Improves Multimodal Embedding", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance."}
{"id": "2510.05048", "pdf": "https://arxiv.org/pdf/2510.05048", "abs": "https://arxiv.org/abs/2510.05048", "authors": ["Ondřej Kubíček", "Viliam Lisý"], "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games."}
{"id": "2510.05059", "pdf": "https://arxiv.org/pdf/2510.05059", "abs": "https://arxiv.org/abs/2510.05059", "authors": ["Junlin Wang", "Jue Wang", "Zhen", "Xu", "Ben Athiwaratkun", "Bhuwan Dhingra", "Ce Zhang", "James Zou"], "title": "Staircase Streaming for Low-Latency Multi-Agent Inference", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality."}
{"id": "2411.05993", "pdf": "https://arxiv.org/pdf/2411.05993", "abs": "https://arxiv.org/abs/2411.05993", "authors": ["Magauiya Zhussip", "Iaroslav Koshelev", "Stamatis Lefkimmiatis"], "title": "A Modular Conditional Diffusion Framework for Image Reconstruction", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have been recently utilized to deal\nwith various blind image restoration (IR) tasks, where they have demonstrated\noutstanding performance in terms of perceptual quality. However, the\ntask-specific nature of existing solutions and the excessive computational\ncosts related to their training, make such models impractical and challenging\nto use for different IR tasks than those that were initially trained for. This\nhinders their wider adoption, especially by those who lack access to powerful\ncomputational resources and vast amount of training data. In this work we aim\nto address the above issues and enable the successful adoption of DPMs in\npractical IR-related applications. Towards this goal, we propose a modular\ndiffusion probabilistic IR framework (DP-IR), which allows us to combine the\nperformance benefits of existing pre-trained state-of-the-art IR networks and\ngenerative DPMs, while it requires only the additional training of a relatively\nsmall module (0.7M params) related to the particular IR task of interest.\nMoreover, the architecture of the proposed framework allows for a sampling\nstrategy that leads to at least four times reduction of neural function\nevaluations without suffering any performance loss, while it can also be\ncombined with existing acceleration techniques such as DDIM. We evaluate our\nmodel on four benchmarks for the tasks of burst JDD-SR, dynamic scene\ndeblurring, and super-resolution. Our method outperforms existing approaches in\nterms of perceptual quality while it retains a competitive performance with\nrespect to fidelity metrics."}
{"id": "2411.18625", "pdf": "https://arxiv.org/pdf/2411.18625", "abs": "https://arxiv.org/abs/2411.18625", "authors": ["Brian Chao", "Hung-Yu Tseng", "Lorenzo Porzi", "Chen Gao", "Tuotuo Li", "Qinbo Li", "Ayush Saraf", "Jia-Bin Huang", "Johannes Kopf", "Gordon Wetzstein", "Changil Kim"], "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling", "categories": ["cs.CV", "cs.AI", "cs.GR", "eess.IV"], "comment": "Will be presented at CVPR 2025. Project website:\n  https://textured-gaussians.github.io/", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D\nreconstruction and rendering technique due to its high-quality results and fast\ntraining and rendering time. However, pixels covered by the same Gaussian are\nalways shaded in the same color up to a Gaussian falloff scaling factor.\nFurthermore, the finest geometric detail any individual Gaussian can represent\nis a simple ellipsoid. These properties of 3DGS greatly limit the expressivity\nof individual Gaussian primitives. To address these issues, we draw inspiration\nfrom texture and alpha mapping in traditional graphics and integrate it with\n3DGS. Specifically, we propose a new generalized Gaussian appearance\nrepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texture\nmaps to model spatially varying color and opacity across the extent of each\nGaussian. As such, each Gaussian can represent a richer set of texture patterns\nand geometric structures, instead of just a single color and ellipsoid as in\nnaive Gaussian Splatting. Surprisingly, we found that the expressivity of\nGaussians can be greatly improved by using alpha-only texture maps, and further\naugmenting Gaussians with RGB texture maps achieves the highest expressivity.\nWe validate our method on a wide variety of standard benchmark datasets and our\nown custom captures at both the object and scene levels. We demonstrate image\nquality improvements over existing methods while using a similar or lower\nnumber of Gaussians."}
{"id": "2505.02819", "pdf": "https://arxiv.org/pdf/2505.02819", "abs": "https://arxiv.org/abs/2505.02819", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe."}
{"id": "2510.03243", "pdf": "https://arxiv.org/pdf/2510.03243", "abs": "https://arxiv.org/abs/2510.03243", "authors": ["Yiheng Tao", "Yihe Zhang", "Matthew T. Dearing", "Xin Wang", "Yuping Fan", "Zhiling Lan"], "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low\nlatency and high throughput, particularly with the growing use of\nreasoning-capable LLMs. Traditional strategies like First-Come-First-Serve\n(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks\ndelay shorter ones queued behind them. In this paper, we introduce PARS, a\nprompt-aware LLM task scheduler that improves serving efficiency by\napproximating shortest-job-first (SJF) scheduling through pairwise ranking with\nmargin ranking loss. PARS focuses on impactful scheduling decisions and is\nseamlessly integrated into the state-of-the-art LLM serving system vLLM. It\neffectively predicts response-length-based task ordering, reducing latency with\nminimal overhead. Extensive experiments across multiple LLMs and real-world\ninference datasets show that PARS significantly improves performance, including\nfor reasoning workloads. Furthermore, our cross-model evaluations demonstrate\nthat the design generalizes well, enabling effective scheduling even when\npredictors are trained on different LLMs."}
{"id": "2510.03244", "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in"}
{"id": "2510.03245", "pdf": "https://arxiv.org/pdf/2510.03245", "abs": "https://arxiv.org/abs/2510.03245", "authors": ["Ali Yavari", "Alireza Mohamadi", "Elham Beydaghi", "Rainer A. Leitgeb"], "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of\nreal world noise and intentional perturbations remains a significant challenge.\nTo address this, attribution methods have been proposed, though their efficacy\nremains suboptimal and necessitates further refinement. In this paper, we\npropose a novel category of transferable adversarial attacks, called\ntransferable frequency-aware attacks, enabling frequency-aware exploration via\nboth high-and low-frequency components. Based on this type of attacks, we also\npropose a novel attribution method, named Frequency-Aware Model Parameter\nExplorer (FAMPE), which improves the explainability for DNNs. Relative to the\ncurrent state-of-the-art method AttEXplore, our FAMPE attains an average gain\nof 13.02% in Insertion Score, thereby outperforming existing approaches.\nThrough detailed ablation studies, we also investigate the role of both high-\nand low-frequency components in explainability."}
{"id": "2510.03246", "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale."}
{"id": "2510.03247", "pdf": "https://arxiv.org/pdf/2510.03247", "abs": "https://arxiv.org/abs/2510.03247", "authors": ["Jiancheng Zhang", "Yinglun Zhu"], "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in\ndata-hungry deep learning. However, existing AL algorithms focus almost\nexclusively on unimodal data, overlooking the substantial annotation burden in\nmultimodal learning. We introduce the first framework for multimodal active\nlearning with unaligned data, where the learner must actively acquire\ncross-modal alignments rather than labels on pre-aligned pairs. This setting\ncaptures the practical bottleneck in modern multimodal pipelines such as CLIP\nand SigLIP, where unimodal features are easy to obtain but high-quality\nalignment is costly. We develop a new algorithm that combines uncertainty and\ndiversity principles in a modality-aware design, achieves linear-time\nacquisition, and applies seamlessly to both pool-based and streaming-based\nsettings. Extensive experiments on benchmark datasets demonstrate that our\napproach consistently reduces multimodal annotation cost while preserving\nperformance; for instance, on the ColorSwap dataset it cuts annotation\nrequirements by up to $40\\%$ without loss in accuracy."}
{"id": "2510.03248", "pdf": "https://arxiv.org/pdf/2510.03248", "abs": "https://arxiv.org/abs/2510.03248", "authors": ["Anusha Agarwal", "Dibakar Roy Sarkar", "Somdatta Goswami"], "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over\n69 million cases annually worldwide. Finite element (FE) models offer\nhigh-fidelity predictions of brain deformation but are computationally\nexpensive, requiring hours per simulation and limiting their clinical utility\nfor rapid decision-making. This study benchmarks state-of-the-art neural\noperator (NO) architectures for rapid, patient-specific prediction of brain\ndisplacement fields, aiming to enable real-time TBI modeling in clinical and\ntranslational settings. We formulated TBI modeling as an operator learning\nproblem, mapping subject-specific anatomical MRI, magnetic resonance\nelastography (MRE) stiffness maps, and demographic features to full-field 3D\nbrain displacement predictions. Four architectures - Fourier Neural Operator\n(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator\nNetwork (DeepONet) were trained and evaluated on 249 MRE datasets across\nphysiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest\naccuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale\nfeatures, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet\noffered the fastest inference (14.5 iterations/s) with a 7$\\times$\ncomputational speed-up over MG-FNO, suggesting utility for embedded or edge\ncomputing applications. All NOs reduced computation time from hours to\nmilliseconds without sacrificing anatomical realism. NOs provide an efficient,\nresolution-invariant approach for predicting brain deformation, opening the\ndoor to real-time, patient-specific TBI risk assessment, clinical triage\nsupport, and optimization of protective equipment. These results highlight the\npotential for NO-based digital twins of the human brain, enabling scalable,\non-demand biomechanical modeling in both clinical and population health\ncontexts."}
{"id": "2510.03251", "pdf": "https://arxiv.org/pdf/2510.03251", "abs": "https://arxiv.org/abs/2510.03251", "authors": ["Hanzhong Cao", "Wenbo Yan", "Ying Tan"], "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many methods aim to enhance time series forecasting by decomposing the series\nthrough intricate model structures and prior knowledge, yet they are inevitably\nlimited by computational complexity and the robustness of the assumptions. Our\nresearch uncovers that in the complex domain and higher-order hypercomplex\nspaces, the characteristic frequencies of time series naturally decrease.\nLeveraging this insight, we propose Numerion, a time series forecasting model\nbased on multiple hypercomplex spaces. Specifically, grounded in theoretical\nsupport, we generalize linear layers and activation functions to hypercomplex\nspaces of arbitrary power-of-two dimensions and introduce a novel\nReal-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.\nNumerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces\nof varying dimensions, naturally decomposing and independently modeling the\nseries, and adaptively fuses the latent patterns exhibited in different spaces\nthrough a dynamic fusion mechanism. Experiments validate the model`s\nperformance, achieving state-of-the-art results on multiple public datasets.\nVisualizations and quantitative analyses comprehensively demonstrate the\nability of multi-dimensional RHR-MLPs to naturally decompose time series and\nreveal the tendency of higher dimensional hypercomplex spaces to capture lower\nfrequency features."}
{"id": "2510.03252", "pdf": "https://arxiv.org/pdf/2510.03252", "abs": "https://arxiv.org/abs/2510.03252", "authors": ["Duc Kieu", "Kien Do", "Tuan Hoang", "Thao Minh Le", "Tung Kieu", "Dang Nguyen", "Thin Nguyen"], "title": "Universal Multi-Domain Translation via Diffusion Routers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multi-domain translation (MDT) aims to learn translations between multiple\ndomains, yet existing approaches either require fully aligned tuples or can\nonly handle domain pairs seen in training, limiting their practicality and\nexcluding many cross-domain mappings. We introduce universal MDT (UMDT), a\ngeneralization of MDT that seeks to translate between any pair of $K$ domains\nusing only $K-1$ paired datasets with a central domain. To tackle this problem,\nwe propose Diffusion Router (DR), a unified diffusion-based framework that\nmodels all central$\\leftrightarrow$non-central translations with a single noise\npredictor conditioned on the source and target domain labels. DR enables\nindirect non-central translations by routing through the central domain. We\nfurther introduce a novel scalable learning strategy with a variational-bound\nobjective and an efficient Tweedie refinement procedure to support direct\nnon-central mappings. Through evaluation on three large-scale UMDT benchmarks,\nDR achieves state-of-the-art results for both indirect and direct translations,\nwhile lowering sampling cost and unlocking novel tasks such as\nsketch$\\leftrightarrow$segmentation. These results establish DR as a scalable\nand versatile framework for universal translation across multiple domains."}
{"id": "2510.03253", "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences."}
{"id": "2510.03255", "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data."}
{"id": "2510.03257", "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT ."}
{"id": "2510.03258", "pdf": "https://arxiv.org/pdf/2510.03258", "abs": "https://arxiv.org/abs/2510.03258", "authors": ["Chang'an Yi", "Xiaohui Deng", "Shuaicheng Niu", "Yan Zhou"], "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "11pages,6 figures", "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to\nunknown test data with potential distribution shifts in an online manner. Many\nexisting TTA methods rely on entropy as a confidence metric to optimize the\nmodel. However, these approaches are sensitive to the predefined entropy\nthreshold, influencing which samples are chosen for model adaptation.\nConsequently, potentially reliable target samples are often overlooked and\nunderutilized. For instance, a sample's entropy might slightly exceed the\nthreshold initially, but fall below it after the model is updated. Such samples\ncan provide stable supervised information and offer a normal range of gradients\nto guide model adaptation. In this paper, we propose a general approach,\n\\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the\npreviously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}}\nsa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch\nnetwork to strike a balance between extracting domain-agnostic representations\nand achieving high performance on target data. Comprehensive experiments across\nmultiple architectures demonstrate that POEM consistently outperforms existing\nTTA methods in both challenging scenarios and real-world domain shifts, while\nremaining computationally efficient. The effectiveness of POEM is evaluated\nthrough extensive analyses and thorough ablation studies. Moreover, the core\nidea behind POEM can be employed as an augmentation strategy to boost the\nperformance of existing TTA approaches. The source code is publicly available\nat \\emph{https://github.com/ycarobot/POEM}"}
{"id": "2510.03259", "pdf": "https://arxiv.org/pdf/2510.03259", "abs": "https://arxiv.org/abs/2510.03259", "authors": ["Yoonjeon Kim", "Doohyuk Jang", "Eunho Yang"], "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains."}
{"id": "2510.03260", "pdf": "https://arxiv.org/pdf/2510.03260", "abs": "https://arxiv.org/abs/2510.03260", "authors": ["Juan Jose Herrera-Aranda", "Guillermo Gomez-Trenado", "Francisco Herrera", "Isaac Triguero"], "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 9 figures, code available at\n  https://kiedie.github.io/Semantic-Inductive-Attribute-Selection-for-Zero-Shot-Learning/", "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial\nIntelligence Systems, particularly in those that operate in open-world\nscenarios where systems must adapt to new tasks dynamically. Semantic spaces\nplay a pivotal role as they bridge seen and unseen classes, but whether\nhuman-annotated or generated by a machine learning model, they often contain\nnoisy, redundant, or irrelevant attributes that hinder performance. To address\nthis, we introduce a partitioning scheme that simulates unseen conditions in an\ninductive setting (which is the most challenging), allowing attribute relevance\nto be assessed without access to semantic information from unseen classes.\nWithin this framework, we study two complementary feature-selection strategies\nand assess their generalisation. The first adapts embedded feature selection to\nthe particular demands of ZSL, turning model-driven rankings into meaningful\nsemantic pruning; the second leverages evolutionary computation to directly\nexplore the space of attribute subsets more broadly. Experiments on five\nbenchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods\nconsistently improve accuracy on unseen classes by reducing redundancy, but in\ncomplementary ways: RFS is efficient and competitive though dependent on\ncritical hyperparameters, whereas GA is more costly yet explores the search\nspace more broadly and avoids such dependence. These results confirm that\nsemantic spaces are inherently redundant and highlight the proposed\npartitioning scheme as an effective tool to refine them under inductive\nconditions."}
{"id": "2510.03262", "pdf": "https://arxiv.org/pdf/2510.03262", "abs": "https://arxiv.org/abs/2510.03262", "authors": ["Andi Zhang", "Xuan Ding", "Haofan Wang", "Steven McDonagh", "Samuel Kaski"], "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict\northogonality when combining sparse semantic vectors without extra time\ncomplexity. LoRA, a popular fine-tuning method for large models, typically\ntrains a module to represent a specific concept such as an object or a style.\nWhen multiple LoRAs are merged, for example to generate an object in a\nparticular style, their semantic vectors may interfere with each other. Our\nmethod guarantees, at the theoretical and runtime levels, that merged LoRAs\nremain orthogonal and thus free from direct interference. However, empirical\nanalysis reveals that such orthogonality does not lead to the semantic\ndisentanglement or compositionality highlighted in prior work on compositional\nadaptation. This finding suggests that inter-LoRA orthogonality alone may be\ninsufficient for achieving true semantic compositionality, prompting a\nre-examination of its role in adapter merging."}
{"id": "2510.03263", "pdf": "https://arxiv.org/pdf/2510.03263", "abs": "https://arxiv.org/abs/2510.03263", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Joanna Waczyńska", "Piotr Borycki", "Przemysław Spurek"], "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The impressive capability of modern text-to-image models to generate\nrealistic visuals has come with a serious drawback: they can be misused to\ncreate harmful, deceptive or unlawful content. This has accelerated the push\nfor machine unlearning. This new field seeks to selectively remove specific\nknowledge from a model's training data without causing a drop in its overall\nperformance. However, it turns out that actually forgetting a given concept is\nan extremely difficult task. Models exposed to attacks using adversarial\nprompts show the ability to generate so-called unlearned concepts, which can be\nnot only harmful but also illegal. In this paper, we present considerations\nregarding the ability of models to forget and recall knowledge, introducing the\nMemory Self-Regeneration task. Furthermore, we present MemoRa strategy, which\nwe consider to be a regenerative approach supporting the effective recovery of\npreviously lost knowledge. Moreover, we propose that robustness in knowledge\nretrieval is a crucial yet underexplored evaluation measure for developing more\nrobust and effective unlearning techniques. Finally, we demonstrate that\nforgetting occurs in two distinct ways: short-term, where concepts can be\nquickly recalled, and long-term, where recovery is more challenging."}
{"id": "2510.03264", "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models."}
{"id": "2510.03265", "pdf": "https://arxiv.org/pdf/2510.03265", "abs": "https://arxiv.org/abs/2510.03265", "authors": ["Bowei Tian", "Yexiao He", "Wanghao Ye", "Ziyao Wang", "Meng Liu", "Ang Li"], "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large-scale foundation models demonstrate strong performance across language,\nvision, and reasoning tasks. However, how they internally structure and\nstabilize concepts remains elusive. Inspired by causal inference, we introduce\nthe MindCraft framework built upon Concept Trees. By applying spectral\ndecomposition at each layer and linking principal directions into branching\nConcept Paths, Concept Trees reconstruct the hierarchical emergence of\nconcepts, revealing exactly when they diverge from shared representations into\nlinearly separable subspaces. Empirical evaluations across diverse scenarios\nacross disciplines, including medical diagnosis, physics reasoning, and\npolitical decision-making, show that Concept Trees recover semantic\nhierarchies, disentangle latent concepts, and can be widely applied across\nmultiple domains. The Concept Tree establishes a widely applicable and powerful\nframework that enables in-depth analysis of conceptual representations in deep\nmodels, marking a significant step forward in the foundation of interpretable\nAI."}
{"id": "2510.03267", "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM."}
{"id": "2510.03268", "pdf": "https://arxiv.org/pdf/2510.03268", "abs": "https://arxiv.org/abs/2510.03268", "authors": ["Lingjie Yi", "Raphael Douady", "Chao Chen"], "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal contrastive learning (MCL) aims to embed data from different\nmodalities in a shared embedding space. However, empirical evidence shows that\nrepresentations from different modalities occupy completely separate regions of\nembedding space, a phenomenon referred to as the modality gap. Moreover,\nexperimental findings on how the size of the modality gap influences downstream\nperformance are inconsistent. These observations raise two key questions: (1)\nWhat causes the modality gap? (2) How does it affect downstream tasks? To\naddress these questions, this paper introduces the first theoretical framework\nfor analyzing the convergent optimal representations of MCL and the modality\nalignment when training is optimized. Specifically, we prove that without any\nconstraint or under the cone constraint, the modality gap converges to zero.\nUnder the subspace constraint (i.e., representations of two modalities fall\ninto two distinct hyperplanes due to dimension collapse), the modality gap\nconverges to the smallest angle between the two hyperplanes. This result\nidentifies \\emph{dimension collapse} as the fundamental origin of the modality\ngap. Furthermore, our theorems demonstrate that paired samples cannot be\nperfectly aligned under the subspace constraint. The modality gap influences\ndownstream performance by affecting the alignment between sample pairs. We\nprove that, in this case, perfect alignment between two modalities can still be\nachieved via two ways: hyperplane rotation and shared space projection."}
{"id": "2510.03269", "pdf": "https://arxiv.org/pdf/2510.03269", "abs": "https://arxiv.org/abs/2510.03269", "authors": ["Wendi Li", "Changdae Oh", "Yixuan Li"], "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Optimistic exploration is central to improving sample efficiency in\nreinforcement learning with human feedback, yet existing exploratory bonus\nmethods to incentivize exploration often fail to realize optimism. We provide a\ntheoretical analysis showing that current formulations, under KL or\n$\\alpha$-divergence regularization, unintentionally bias exploration toward\nhigh-probability regions of the reference model, thereby reinforcing\nconservative behavior instead of promoting discovery of uncertain regions. To\naddress this pitfall, we introduce the General Exploratory Bonus (GEB), a novel\ntheoretical framework that provably satisfies the optimism principle. GEB\ncounteracts divergence-induced bias via reference-dependent reward regulation\nand unifies prior heuristic bonuses as special cases, while extending naturally\nacross the full $\\alpha$-divergence family. Empirically, GEB consistently\noutperforms baselines on alignment tasks across multiple divergence settings\nand large language model backbones. These results demonstrate that GEB offers\nboth a principled and practical solution for optimistic exploration in RLHF."}
{"id": "2510.03270", "pdf": "https://arxiv.org/pdf/2510.03270", "abs": "https://arxiv.org/abs/2510.03270", "authors": ["Haolin Chen", "Shiyu Wang", "Can Qin", "Bo Pang", "Zuxin Liu", "Jielin Qiu", "Jianguo Zhang", "Yingbo Zhou", "Zeyuan Chen", "Ran Xu", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "CoDA: Coding LM via Diffusion Adaptation", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": null, "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants."}
{"id": "2510.03271", "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "categories": ["cs.LG", "cs.AI"], "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora."}
{"id": "2510.03272", "pdf": "https://arxiv.org/pdf/2510.03272", "abs": "https://arxiv.org/abs/2510.03272", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized artificial intelligence, yet\na principled theoretical understanding of its internal mechanisms remains\nelusive. This paper introduces a novel analytical framework that\nreconceptualizes the Transformer's discrete, layered structure as a continuous\nspatiotemporal dynamical system governed by a master Partial Differential\nEquation (PDE). Within this paradigm, we map core architectural components to\ndistinct mathematical operators: self-attention as a non-local interaction, the\nfeed-forward network as a local reaction, and, critically, residual connections\nand layer normalization as indispensable stabilization mechanisms. We do not\npropose a new model, but rather employ the PDE system as a theoretical probe to\nanalyze the mathematical necessity of these components. By comparing a standard\nTransformer with a PDE simulator that lacks explicit stabilizers, our\nexperiments provide compelling empirical evidence for our central thesis. We\ndemonstrate that without residual connections, the system suffers from\ncatastrophic representational drift, while the absence of layer normalization\nleads to unstable, explosive training dynamics. Our findings reveal that these\nseemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers\nrequired to tame an otherwise powerful but inherently unstable continuous\nsystem. This work offers a first-principles explanation for the Transformer's\ndesign and establishes a new paradigm for analyzing deep neural networks\nthrough the lens of continuous dynamics."}
{"id": "2510.03273", "pdf": "https://arxiv.org/pdf/2510.03273", "abs": "https://arxiv.org/abs/2510.03273", "authors": ["Chenhao Ye", "Ming Tang"], "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Backpropagation (BP), while foundational to deep learning, imposes two\ncritical scalability bottlenecks: update locking, where network modules remain\nidle until the entire backward pass completes, and high memory consumption due\nto storing activations for gradient computation. To address these limitations,\nwe introduce Synergistic Information Distillation (SID), a novel training\nframework that reframes deep learning as a cascade of local cooperative\nrefinement problems. In SID, a deep network is structured as a pipeline of\nmodules, each imposed with a local objective to refine a probabilistic belief\nabout the ground-truth target. This objective balances fidelity to the target\nwith consistency to the belief from its preceding module. By decoupling the\nbackward dependencies between modules, SID enables parallel training and hence\neliminates update locking and drastically reduces memory requirements.\nMeanwhile, this design preserves the standard feed-forward inference pass,\nmaking SID a versatile drop-in replacement for BP. We provide a theoretical\nfoundation, proving that SID guarantees monotonic performance improvement with\nnetwork depth. Empirically, SID consistently matches or surpasses the\nclassification accuracy of BP, exhibiting superior scalability and pronounced\nrobustness to label noise.Code is available at:\nhttps://github.com/ychAlbert/sid-bp"}
{"id": "2510.03274", "pdf": "https://arxiv.org/pdf/2510.03274", "abs": "https://arxiv.org/abs/2510.03274", "authors": ["Tianao Zhang", "Zhiteng Li", "Xianglong Yan", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion large language models (dLLMs), which offer bidirectional context\nand flexible masked-denoising generation, are emerging as a compelling\nalternative to autoregressive (AR) LLMs. However, like AR LLMs, their model\nsizes continue to grow, motivating weight compression for deployment. Although\npost-training quantization (PTQ) is effective for AR LLMs, directly\ntransferring it to dLLMs at 2-bit leads to unsatisfactory performance. To\ntackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework\ntailored to dLLMs. Since masked-denoising activations in dLLMs differ from the\nfully visible signals assumed by standard PTQ methods, we introduce Masked\nCalibration Simulation (MCS) to align calibration with the timestep-dependent\nmasking, which yields more reliable calibrations. Moreover, we propose a\nData-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight\nrepresentations via an optimization algorithm. It performs iterative\napproximation guided by our simulated calibration data. In addition, under a\nstrict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a\nsensitivity-based precision allocation scheme that adaptively assigns bit width\nacross channel groups. When restricted to 2-bit precision, Quant-dLLM\nconsistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer\nPTQ methods on dLLMs. The code and models will be available at:\nhttps://github.com/ZTA2785/Quant-dLLM."}
{"id": "2510.03275", "pdf": "https://arxiv.org/pdf/2510.03275", "abs": "https://arxiv.org/abs/2510.03275", "authors": ["Junhao Xia", "Ming Zhao", "Limin Xiao", "Xiujun Zhang"], "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large language models (LLMs) face significant computational and memory\nchallenges, making extremely low-bit quantization crucial for their efficient\ndeployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for\n1-bit LLMs of any size, a novel framework that enables extremely low-bit\nquantization of LLMs while preserving their linguistic reasoning capabilities.\nA distinctive feature of SDQ-LLM is the continuous adjustability of the\nOver-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM\nconstraints by selecting fractional OSR (e.g. 2.5 times) for an optimal\ntrade-off between model size and accuracy. SDQ-LLM uses upsampling combined\nwith Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding\nhigh-precision parameters into 1-bit or 1.58-bit representations, replacing the\nmultiplication operations within linear layers with addition. This approach\nsignificantly enhances inference efficiency under extremely low-bit\nquantization. To further reduce the loss of quantization precision, we\nincorporate Hadamard-based weight smoothing prior to quantization, improving\nthe stability and robustness of the weight representations. Furthermore, to\nfully leverage the continuity of the OSR and reduce precision loss, recognizing\nthe correlation between quantization sensitivity and weight variance, we\npropose a fine-grained, layer- and linear-wise OSR allocation strategy,\nMultiOSR. This strategy distributes OSR both across layers and within each\nlayer, based on weight variance and parameter scale. Finally, extensive\nexperiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a\nmore efficient and high-precision performance even under highly aggressive\nlow-OSR settings. Our code is available at\nhttps://github.com/Dreamlittlecat/LLM-Quant-Factory."}
{"id": "2510.03276", "pdf": "https://arxiv.org/pdf/2510.03276", "abs": "https://arxiv.org/abs/2510.03276", "authors": ["Qian Chen", "Linxin Yang", "Akang Wang", "Xiaodong Luo", "Yin Zhang"], "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The combination of linear transformations and non-linear activation functions\nforms the foundation of most modern deep neural networks, enabling them to\napproximate highly complex functions. This paper explores the introduction of\nquadratic transformations to further increase nonlinearity in neural networks,\nwith the aim of enhancing the performance of existing architectures. To reduce\nparameter complexity and computational complexity, we propose a lightweight\nquadratic enhancer that uses low-rankness, weight sharing, and sparsification\ntechniques. For a fixed architecture, the proposed approach introduces\nquadratic interactions between features at every layer, while only adding\nnegligible amounts of additional model parameters and forward computations. We\nconduct a set of proof-of-concept experiments for the proposed method across\nthree tasks: image classification, text classification, and fine-tuning\nlarge-language models. In all tasks, the proposed approach demonstrates clear\nand substantial performance gains."}
{"id": "2510.03278", "pdf": "https://arxiv.org/pdf/2510.03278", "abs": "https://arxiv.org/abs/2510.03278", "authors": ["Filip Landgren"], "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages, 2 figures", "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing\nequations to solve differential equations under uncertainty. However,\ninterpreting uncertainty and overconfidence in B-PINNs requires care due to the\npoorly understood effects the physical constraints have on the network;\noverconfidence could reflect warranted precision, enforced by the constraints,\nrather than miscalibration. Motivated by the need to further clarify how\nindividual physical constraints shape these networks, we introduce a scalable,\nmatrix-free Laplace framework that decomposes the posterior Hessian into\ncontributions from each constraint and provides metrics to quantify their\nrelative influence on the loss landscape. Applied to the Van der Pol equation,\nour method tracks how constraints sculpt the network's geometry and shows,\ndirectly through the Hessian, how changing a single loss weight non-trivially\nredistributes curvature and effective dominance across the others."}
{"id": "2510.03279", "pdf": "https://arxiv.org/pdf/2510.03279", "abs": "https://arxiv.org/abs/2510.03279", "authors": ["Youjin Wang", "Yangjingyi Chen", "Jiahao Yan", "Jiaxuan Lu", "Xiao Sun"], "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling."}
{"id": "2510.03280", "pdf": "https://arxiv.org/pdf/2510.03280", "abs": "https://arxiv.org/abs/2510.03280", "authors": ["Jinjie Ni", "Qian Liu", "Chao Du", "Longxu Dou", "Hang Yan", "Zili Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "Training Optimal Large Diffusion Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Quokka, the first systematic scaling law for diffusion language\nmodels (DLMs), encompassing both compute-constrained and data-constrained\nregimes, and studying the key modeling and optimization designs. Quokka is a\ngood friend of Chinchilla and provides wider scopes. We hope the results would\nbring short-term practical guidance in DLMs training and long-term inspirations\nfor the whole AI community."}
{"id": "2510.03283", "pdf": "https://arxiv.org/pdf/2510.03283", "abs": "https://arxiv.org/abs/2510.03283", "authors": ["Yufei Li", "Yu Fu", "Yue Dong", "Cong Liu"], "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "14 pages, 15 figures", "summary": "Large language models (LLMs) deployed on edge servers are increasingly used\nin latency-sensitive applications such as personalized assistants,\nrecommendation, and content moderation. However, the non-stationary nature of\nuser data necessitates frequent retraining, which introduces a fundamental\ntension between inference latency and model accuracy under constrained GPU\nresources. Existing retraining strategies either delay model updates,\nover-commit resources to retraining, or overlook iteration-level retraining\ngranularity. In this paper, we identify that iteration-level scheduling is\ncrucial for adapting retraining frequency to model drift without violating\nservice-level objectives (SLOs). We propose MACE, a hybrid LLM system that\ncolocates concurrent inference (prefill, decode) and fine-tuning, with\nintelligent memory management to maximize task performance while promising\ninference throughput. MACE leverages the insight that not all model updates\nequally affect output alignment and allocates GPU cycles accordingly to balance\nthroughput, latency, and update freshness. Our trace-driven evaluation shows\nthat MACE matches or exceeds continuous retraining while reducing inference\nlatency by up to 63% and maintaining throughput under resource constraints.\nCompared to periodic retraining, MACE improves latency breakdown across\nprefill, decode, and finetune stages, and sustains GPU utilization above 85% in\nNVIDIA AGX Orin. These results demonstrate that iteration-level hybrid\nscheduling is a promising direction for deploying LLMs with continual learning\ncapabilities on edge platforms."}
{"id": "2510.03284", "pdf": "https://arxiv.org/pdf/2510.03284", "abs": "https://arxiv.org/abs/2510.03284", "authors": ["Vinay Venkatesh", "Vamsidhar R Kamanuru", "Lav Kumar", "Nikita Kothari"], "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure", "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a\nscalable framework for Federated Instruction Tuning (FIT) of Large Language\nModels (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail\nwhen confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT\nframework combines federated learning with 4-bit Quantized Low-Rank Adaptation\n(QLORA), mitigating the core issues of communication and computational\noverhead. We demonstrate this by filtering the general-purpose Databricks Dolly\n15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned\nLlama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable\ntrade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable\nframework for decentralized LLM deployment on home compute gateways."}
{"id": "2510.03286", "pdf": "https://arxiv.org/pdf/2510.03286", "abs": "https://arxiv.org/abs/2510.03286", "authors": ["E. A. Dzhivelikian", "A. I. Panov"], "title": "A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Cognitive maps provide a powerful framework for understanding spatial and\nabstract reasoning in biological and artificial agents. While recent\ncomputational models link cognitive maps to hippocampal-entorhinal mechanisms,\nthey often rely on global optimization rules (e.g., backpropagation) that lack\nbiological plausibility. In this work, we propose a novel cognitive\narchitecture for structuring episodic memories into cognitive maps using local,\nHebbian-like learning rules, compatible with neural substrate constraints. Our\nmodel integrates the Successor Features framework with episodic memories,\nenabling incremental, online learning through agent-environment interaction. We\ndemonstrate its efficacy in a partially observable grid-world, where the\narchitecture autonomously organizes memories into structured representations\nwithout centralized optimization. This work bridges computational neuroscience\nand AI, offering a biologically grounded approach to cognitive map formation in\nartificial adaptive agents."}
{"id": "2510.03288", "pdf": "https://arxiv.org/pdf/2510.03288", "abs": "https://arxiv.org/abs/2510.03288", "authors": ["Chiming Duan", "Minghua He", "Pei Xiao", "Tong Jia", "Xin Zhang", "Zhewei Zhong", "Xiang Luo", "Yan Niu", "Lingzhe Zhang", "Yifan Wu", "Siyu Yu", "Weijie Hong", "Ying Li", "Gang Huang"], "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "comment": "The 40th IEEE/ACM International Conference on Automated Software\n  Engineering, ASE 2025", "summary": "Log-based anomaly detection is a essential task for ensuring the reliability\nand performance of software systems. However, the performance of existing\nanomaly detection methods heavily relies on labeling, while labeling a large\nvolume of logs is highly challenging. To address this issue, many approaches\nbased on transfer learning and active learning have been proposed.\nNevertheless, their effectiveness is hindered by issues such as the gap between\nsource and target system data distributions and cold-start problems. In this\npaper, we propose LogAction, a novel log-based anomaly detection model based on\nactive domain adaptation. LogAction integrates transfer learning and active\nlearning techniques. On one hand, it uses labeled data from a mature system to\ntrain a base model, mitigating the cold-start issue in active learning. On the\nother hand, LogAction utilize free energy-based sampling and uncertainty-based\nsampling to select logs located at the distribution boundaries for manual\nlabeling, thus addresses the data distribution gap in transfer learning with\nminimal human labeling efforts. Experimental results on six different\ncombinations of datasets demonstrate that LogAction achieves an average 93.01%\nF1 score with only 2% of manual labels, outperforming some state-of-the-art\nmethods by 26.28%. Website: https://logaction.github.io"}
{"id": "2510.03289", "pdf": "https://arxiv.org/pdf/2510.03289", "abs": "https://arxiv.org/abs/2510.03289", "authors": ["Haocheng Sun", "Cynthia Xin Wen", "Edward Hong Wang"], "title": "Why mask diffusion does not work", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The main advantages of diffusion language models over autoregressive (AR)\nmodels lie in their ability to support parallel generation and bidirectional\nattention, enabling a more controllable generation process. In recent years,\nopen-source mask diffusion language models have emerged, most of which are\nbased on a variant known as absorbing diffusion. However, this paper\ndemonstrates why mask diffusion faces inherent difficulties in achieving\nparallel generation and bidirectional attention. We also propose the most\neffective training and inference strategies for mask diffusion."}
{"id": "2510.03291", "pdf": "https://arxiv.org/pdf/2510.03291", "abs": "https://arxiv.org/abs/2510.03291", "authors": ["Yizhuo Ding", "Wanying Qu", "Jiawei Geng", "Wenqi Shao", "Yanwei Fu"], "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks\nbut face prohibitive computational and memory costs. Pruning offers a promising\npath by inducing sparsity while preserving architectural flexibility. However,\nexisting methods struggle to balance efficiency and robustness: local metric\napproaches prune layer by layer but often collapse under high sparsity, whereas\nglobal feedback methods enforce consistency at the cost of expensive weight\nupdates or restrictive semi-structured formats. We present UniPruning, a\nunified post-training pruning framework that combines the speed of local\nsaliency metrics with the stability of global coordination, enabled by a mirror\ndescent based optimization, all without updating model weights. UniPruning\nleverages fast layer-wise scoring and a lightweight global controller to\nallocate a single sparsity budget, supporting both unstructured and\nsemi-structured N :M pruning within one framework. After a brief calibration,\nit can generate pruning masks for arbitrary sparsity levels in one shot, and\nadapts seamlessly to hardware-aware constraints. Extensive experiments on\nmultiple pretrained LLM families and standard benchmarks show that UniPruning\nconsistently delivers competitive or superior perplexity and zero-shot\naccuracy. Ablation studies further highlight the importance of mirror descent\nand local saliency anchoring. Overall, UniPruning provides an efficient,\nprincipled, and scalable solution for sparsifying large-scale LLMs. Our code is\navailable at: https://github.com/RainbowQTT/UniPruning."}
{"id": "2510.03293", "pdf": "https://arxiv.org/pdf/2510.03293", "abs": "https://arxiv.org/abs/2510.03293", "authors": ["Rana Shahout", "Colin Cai", "Yilun Du", "Minlan Yu", "Michael Mitzenmacher"], "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each\ntoken to a subset of experts through a learned gate function. While conditional\nrouting reduces training costs, it shifts the burden on inference memory:\nexpert parameters and activations consume memory, limiting the number of\nexperts per device. As tokens are routed, some experts become overloaded while\nothers are underutilized. Because experts are mapped to GPUs, this imbalance\ntranslates directly into degraded system performance in terms of latency,\nthroughput, and cost. We present LASER, a plug-and-play, inference-time routing\nalgorithm that balances load while preserving accuracy. LASER adapts to the\nshape of the gate's score distribution. When scores provide a clear preference,\nit routes to the strongest experts; when scores are more uniform, it broadens\nthe set of viable experts and routes to the least-loaded among them. Because\nLASER relies only on gate scores from a trained model, it integrates directly\ninto existing MoE inference pipelines without retraining or finetuning. We\nevaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets\n(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,\ntranslating into lower latency and higher throughput, while keeping the\naccuracy changes negligible."}
{"id": "2510.03297", "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility."}
{"id": "2510.03301", "pdf": "https://arxiv.org/pdf/2510.03301", "abs": "https://arxiv.org/abs/2510.03301", "authors": ["Arthur Sedek"], "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces a novel adaptive ensemble framework that\nsynergistically combines XGBoost and neural networks through sophisticated\nmeta-learning. The proposed method leverages advanced uncertainty\nquantification techniques and feature importance integration to dynamically\norchestrate model selection and combination. Experimental results demonstrate\nsuperior predictive performance and enhanced interpretability across diverse\ndatasets, contributing to the development of more intelligent and flexible\nmachine learning systems."}
{"id": "2510.03306", "pdf": "https://arxiv.org/pdf/2510.03306", "abs": "https://arxiv.org/abs/2510.03306", "authors": ["Shuai Huang", "Xuan Kan", "James J. Lah", "Deqiang Qiu"], "title": "Atlas-free Brain Network Transformer", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE", "eess.IV"], "comment": null, "summary": "Current atlas-based approaches to brain network analysis rely heavily on\nstandardized anatomical or connectivity-driven brain atlases. However, these\nfixed atlases often introduce significant limitations, such as spatial\nmisalignment across individuals, functional heterogeneity within predefined\nregions, and atlas-selection biases, collectively undermining the reliability\nand interpretability of the derived brain networks. To address these\nchallenges, we propose a novel atlas-free brain network transformer (atlas-free\nBNT) that leverages individualized brain parcellations derived directly from\nsubject-specific resting-state fMRI data. Our approach computes ROI-to-voxel\nconnectivity features in a standardized voxel-based feature space, which are\nsubsequently processed using the BNT architecture to produce comparable\nsubject-level embeddings. Experimental evaluations on sex classification and\nbrain-connectome age prediction tasks demonstrate that our atlas-free BNT\nconsistently outperforms state-of-the-art atlas-based methods, including\nelastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach\nsignificantly improves the precision, robustness, and generalizability of brain\nnetwork analyses. This advancement holds great potential to enhance\nneuroimaging biomarkers and clinical diagnostic tools for personalized\nprecision medicine."}
{"id": "2510.03308", "pdf": "https://arxiv.org/pdf/2510.03308", "abs": "https://arxiv.org/abs/2510.03308", "authors": ["Jiong Lin", "Jialong Ning", "Judah Goldfeder", "Hod Lipson"], "title": "Creative synthesis of kinematic mechanisms", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "6pages, 6 figures", "summary": "In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework."}
{"id": "2510.03310", "pdf": "https://arxiv.org/pdf/2510.03310", "abs": "https://arxiv.org/abs/2510.03310", "authors": ["Runze Zhang", "Xiaowei Zhang", "Mingyang Zhao"], "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "LLMs are emerging tools for simulating human behavior in business, economics,\nand social science, offering a lower-cost complement to laboratory experiments,\nfield studies, and surveys. This paper evaluates how well LLMs replicate human\nbehavior in operations management. Using nine published experiments in\nbehavioral operations, we assess two criteria: replication of hypothesis-test\noutcomes and distributional alignment via Wasserstein distance. LLMs reproduce\nmost hypothesis-level effects, capturing key decision biases, but their\nresponse distributions diverge from human data, including for strong commercial\nmodels. We also test two lightweight interventions -- chain-of-thought\nprompting and hyperparameter tuning -- which reduce misalignment and can\nsometimes let smaller or open-source models match or surpass larger systems."}
{"id": "2510.03314", "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety."}
{"id": "2510.03315", "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text."}
{"id": "2510.03316", "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience."}
{"id": "2510.03317", "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["Günel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology."}
{"id": "2510.03326", "pdf": "https://arxiv.org/pdf/2510.03326", "abs": "https://arxiv.org/abs/2510.03326", "authors": ["Tao Guo", "Junbo Yin", "Yu Wang", "Xin Gao"], "title": "NS-Pep: De novo Peptide Design with Non-Standard Amino Acids", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved\nbinding affinity and improved pharmacological properties. However, existing\npeptide design methods are limited to standard amino acids, leaving NSAA-aware\ndesign largely unexplored. We introduce NS-Pep, a unified framework for\nco-designing peptide sequences and structures with NSAAs. The main challenge is\nthat NSAAs are extremely underrepresented-even the most frequent one, SEP,\naccounts for less than 0.4% of residues-resulting in a severe long-tailed\ndistribution. To improve generalization to rare amino acids, we propose Residue\nFrequency-Guided Modification (RFGM), which mitigates over-penalization through\nfrequency-aware logit calibration, supported by both theoretical and empirical\nanalysis. Furthermore, we identify that insufficient side-chain modeling limits\ngeometric representation of NSAAs. To address this, we introduce Progressive\nSide-chain Perception (PSP) for coarse-to-fine torsion and location prediction,\nand Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues.\nMoreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs,\naddressing a major limitation of current tools. Experiments show that NS-Pep\nimproves sequence recovery rate and binding affinity by 6.23% and 5.12%,\nrespectively, and outperforms AlphaFold3 by 17.76% in peptide folding success\nrate."}
{"id": "2510.03331", "pdf": "https://arxiv.org/pdf/2510.03331", "abs": "https://arxiv.org/abs/2510.03331", "authors": ["Vivek Acharya"], "title": "Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)", "categories": ["cs.CY", "cs.AI", "68T07, 92C55, 92C60", "I.2.1; J.3; H.3.5"], "comment": "8 pages, 4 figures, formatted per MDPI guidelines, APA-style numbered\n  references", "summary": "The United States spends nearly 17% of GDP on healthcare yet continues to\nface uneven access and outcomes. This well-known trade-off among cost, quality,\nand access - the \"iron triangle\" - motivates a system-level redesign. This\npaper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated,\ndata-driven framework that uses generative AI and large language models,\nfederated learning, interoperability standards (FHIR, TEFCA), and digital twins\nto improve access and quality while lowering cost. We review historical\nspending trends, waste, and international comparisons; introduce a value\nequation that jointly optimizes access, quality, and cost; and synthesize\nevidence on the enabling technologies and operating model for iHE. Methods\nfollow a narrative review of recent literature and policy reports. Results\noutline core components (AI decision support, interoperability, telehealth,\nautomation) and show how iHE can reduce waste, personalize care, and support\nvalue-based payment while addressing privacy, bias, and adoption challenges. We\nargue that a coordinated iHE can bend - if not break - the iron triangle,\nmoving the system toward care that is more accessible, affordable, and high\nquality."}
{"id": "2510.03336", "pdf": "https://arxiv.org/pdf/2510.03336", "abs": "https://arxiv.org/abs/2510.03336", "authors": ["Adharsha Sam Edwin Sam Devahi", "Sohail Singh Sangha", "Prachee Priyadarshinee", "Jithin Thilakan", "Ivan Fu Xing Tan", "Christopher Johann Clarke", "Sou Ka Lon", "Balamurali B T", "Yow Wei Quin", "Chen Jer-Ming"], "title": "Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment\n(MCI) is critical for timely intervention, yet current diagnostic approaches\nremain resource-intensive and invasive. Speech, encompassing both acoustic and\nlinguistic dimensions, offers a promising non-invasive biomarker for cognitive\ndecline. In this study, we present a machine learning framework for the PROCESS\nChallenge, leveraging both audio embeddings and linguistic features derived\nfrom spontaneous speech recordings. Audio representations were extracted using\nWhisper embeddings from the Cookie Theft description task, while linguistic\nfeatures-spanning pronoun usage, syntactic complexity, filler words, and clause\nstructure-were obtained from transcriptions across Semantic Fluency, Phonemic\nFluency, and Cookie Theft picture description. Classification models aimed to\ndistinguish between Healthy Controls (HC), MCI, and AD participants, while\nregression models predicted Mini-Mental State Examination (MMSE) scores.\nResults demonstrated that voted ensemble models trained on concatenated\nlinguistic features achieved the best classification performance (F1 = 0.497),\nwhile Whisper embedding-based ensemble regressors yielded the lowest MMSE\nprediction error (RMSE = 2.843). Comparative evaluation within the PROCESS\nChallenge placed our models among the top submissions in regression task, and\nmid-range for classification, highlighting the complementary strengths of\nlinguistic and audio embeddings. These findings reinforce the potential of\nmultimodal speech-based approaches for scalable, non-invasive cognitive\nassessment and underline the importance of integrating task-specific linguistic\nand acoustic markers in dementia detection."}
{"id": "2510.03339", "pdf": "https://arxiv.org/pdf/2510.03339", "abs": "https://arxiv.org/abs/2510.03339", "authors": ["Sofiane Ennadir", "Levente Zólyomi", "Oleg Smirnov", "Tianze Wang", "John Pertoft", "Filip Cornell", "Lele Cao"], "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer models have become the dominant backbone for sequence modeling,\nleveraging self-attention to produce contextualized token representations.\nThese are typically aggregated into fixed-size vectors via pooling operations\nfor downstream tasks. While much of the literature has focused on attention\nmechanisms, the role of pooling remains underexplored despite its critical\nimpact on model behavior. In this paper, we introduce a theoretical framework\nthat rigorously characterizes the expressivity of Transformer-based models\nequipped with widely used pooling methods by deriving closed-form bounds on\ntheir representational capacity and the ability to distinguish similar inputs.\nOur analysis extends to different variations of attention formulations,\ndemonstrating that these bounds hold across diverse architectural variants. We\nempirically evaluate pooling strategies across tasks requiring both global and\nlocal contextual understanding, spanning three major modalities: computer\nvision, natural language processing, and time-series analysis. Results reveal\nconsistent trends in how pooling choices affect accuracy, sensitivity, and\noptimization behavior. Our findings unify theoretical and empirical\nperspectives, providing practical guidance for selecting or designing pooling\nmechanisms suited to specific tasks. This work positions pooling as a key\narchitectural component in Transformer models and lays the foundation for more\nprincipled model design beyond attention alone."}
{"id": "2510.03340", "pdf": "https://arxiv.org/pdf/2510.03340", "abs": "https://arxiv.org/abs/2510.03340", "authors": ["Marian Chen", "Miri Zilka"], "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.PE"], "comment": null, "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies\nthat balance disease containment with socioeconomic stability. We approach this\nchallenge by designing a framework for modeling and evaluating disease-spread\nprevention strategies. Our framework leverages multi-objective reinforcement\nlearning (MORL) - a formulation necessitated by competing objectives - combined\nwith a new stochastic differential equation (SDE) pandemic simulator,\ncalibrated and validated against global COVID-19 data. Our simulator reproduces\nnational-scale pandemic dynamics with orders of magnitude higher fidelity than\nother models commonly used in reinforcement learning (RL) approaches to\npandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on\nthis simulator, we illustrate the direct policy trade-offs between\nepidemiological control and economic stability for COVID-19. Furthermore, we\ndemonstrate the framework's generality by extending it to pathogens with\ndifferent epidemiological profiles, such as polio and influenza, and show how\nthese profiles lead the agent to discover fundamentally different intervention\npolicies. To ground our work in contemporary policymaking challenges, we apply\nthe model to measles outbreaks, quantifying how a modest 5% drop in vaccination\ncoverage necessitates significantly more stringent and costly interventions to\ncurb disease spread. This work provides a robust and adaptable framework to\nsupport transparent, evidence-based policymaking for mitigating public health\ncrises."}
{"id": "2510.03343", "pdf": "https://arxiv.org/pdf/2510.03343", "abs": "https://arxiv.org/abs/2510.03343", "authors": ["Nikolaos Avouris"], "title": "Defining a Strategic Action Plan for AI in Higher Education", "categories": ["cs.CY", "cs.AI"], "comment": "to be cited: N. Avouris (2025), Defining a Strategic Action Plan for\n  AI in Higher Education, Proceedings International Scientific Conference on\n  Digital Competencies in Higher Education, Tirana, September 2025, pp. 141-151", "summary": "This paper discusses key challenges of Artificial Intelligence in Education,\nwith main focus on higher education institutions. We start with reviewing\nnormative actions of international organizations and concerns expressed about\nthe current technical landscape. Then we proceed with proposing a framework\nthat comprises five key dimensions relating to the main challenges relating to\nAI in higher education institutions, followed by five key strategic actions\nthat the main stakeholders need to take in order to address the current\ndevelopments. We map these actions to the main stakeholders of higher education\nand propose a deployment plan. This defines a framework along the dimensions:\nChallenges, Actions, Stakeholders, Deployment CASD. Examples of AI specific\nactions at the institutional and individual course level are also provided and\ndiscussed."}
{"id": "2510.03345", "pdf": "https://arxiv.org/pdf/2510.03345", "abs": "https://arxiv.org/abs/2510.03345", "authors": ["Luoma Ke", "Guangpeng Zhang", "Jibo He", "Yajing Li", "Yan Li", "Xufeng Liu", "Peng Fang"], "title": "Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid growth of the aviation industry, there is a need for a large\nnumber of flight crew. How to select the right pilots in a cost-efficient\nmanner has become an important research question. In the current study,\ntwenty-three pilots were recruited from China Eastern Airlines, and 23 novices\nwere from the community of Tsinghua University. A novel approach incorporating\nmachine learning and virtual reality technology was applied to distinguish\nfeatures between these participants with different flight skills. Results\nindicate that SVM with the MIC feature selection method consistently achieved\nthe highest prediction performance on all metrics with an Accuracy of 0.93, an\nAUC of 0.96, and an F1 of 0.93, which outperforms four other classifier\nalgorithms and two other feature selection methods. From the perspective of\nfeature selection methods, the MIC method can select features with a nonlinear\nrelationship to sampling labels, instead of a simple filter-out. Our new\nimplementation of the SVM + MIC algorithm outperforms all existing pilot\nselection algorithms and perhaps provides the first implementation based on eye\ntracking and flight dynamics data. This study's VR simulation platforms and\nalgorithms can be used for pilot selection and training."}
{"id": "2510.03346", "pdf": "https://arxiv.org/pdf/2510.03346", "abs": "https://arxiv.org/abs/2510.03346", "authors": ["Xiangyu Shi", "Marco Chiesa", "Gerald Q. Maguire Jr.", "Dejan Kostic"], "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems."}
{"id": "2510.03349", "pdf": "https://arxiv.org/pdf/2510.03349", "abs": "https://arxiv.org/abs/2510.03349", "authors": ["Michael Chen"], "title": "AgentCaster: Reasoning-Guided Tornado Forecasting", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.ao-ph"], "comment": null, "summary": "There is a growing need to evaluate Large Language Models (LLMs) on complex,\nhigh-impact, real-world tasks to assess their true readiness as reasoning\nagents. To address this gap, we introduce AgentCaster, a contamination-free\nframework employing multimodal LLMs end-to-end for the challenging,\nlong-horizon task of tornado forecasting. Within AgentCaster, models interpret\nheterogeneous spatiotemporal data from a high-resolution convection-allowing\nforecast archive. We assess model performance over a 40-day period featuring\ndiverse historical data, spanning several major tornado outbreaks and including\nover 500 tornado reports. Each day, models query interactively from a pool of\n3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of\n12-36 hours. Probabilistic tornado-risk polygon predictions are verified\nagainst ground truths derived from geometric comparisons across disjoint risk\nbands in projected coordinate space. To quantify accuracy, we propose\ndomain-specific TornadoBench and TornadoHallucination metrics, with\nTornadoBench highly challenging for both LLMs and domain expert human\nforecasters. Notably, human experts significantly outperform state-of-the-art\nmodels, which demonstrate a strong tendency to hallucinate and overpredict risk\nintensity, struggle with precise geographic placement, and exhibit poor\nspatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster\naims to advance research on improving LLM agents for challenging reasoning\ntasks in critical domains."}
{"id": "2510.03351", "pdf": "https://arxiv.org/pdf/2510.03351", "abs": "https://arxiv.org/abs/2510.03351", "authors": ["Song Wang", "Zhenyu Lei", "Zhen Tan", "Jundong Li", "Javier Rasero", "Aiying Zhang", "Chirag Agarwal"], "title": "Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": null, "summary": "Nearly one in five adolescents currently live with a diagnosed mental or\nbehavioral health condition, such as anxiety, depression, or conduct disorder,\nunderscoring the urgency of developing accurate and interpretable diagnostic\ntools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a\npowerful lens into large-scale functional connectivity, where brain regions are\nmodeled as nodes and inter-regional synchrony as edges, offering clinically\nrelevant biomarkers for psychiatric disorders. While prior works use graph\nneural network (GNN) approaches for disorder prediction, they remain complex\nblack-boxes, limiting their reliability and clinical translation. In this work,\nwe propose CONCEPTNEURO, a concept-based diagnosis framework that leverages\nlarge language models (LLMs) and neurobiological domain knowledge to\nautomatically generate, filter, and encode interpretable functional\nconnectivity concepts. Each concept is represented as a structured subgraph\nlinking specific brain regions, which are then passed through a concept\nclassifier. Our design ensures predictions through clinically meaningful\nconnectivity patterns, enabling both interpretability and strong predictive\nperformance. Extensive experiments across multiple psychiatric disorder\ndatasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform\ntheir vanilla counterparts, improving accuracy while providing transparent,\nclinically aligned explanations. Furthermore, concept analyses highlight\ndisorder-specific connectivity patterns that align with expert knowledge and\nsuggest new hypotheses for future investigation, establishing CONCEPTNEURO as\nan interpretable, domain-informed framework for psychiatric disorder diagnosis."}
{"id": "2510.03352", "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}."}
{"id": "2510.03358", "pdf": "https://arxiv.org/pdf/2510.03358", "abs": "https://arxiv.org/abs/2510.03358", "authors": ["Annan Yu", "Danielle C. Maddix", "Boran Han", "Xiyuan Zhang", "Abdul Fatir Ansari", "Oleksandr Shchur", "Christos Faloutsos", "Andrew Gordon Wilson", "Michael W. Mahoney", "Yuyang Wang"], "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility", "categories": ["cs.LG", "cs.AI"], "comment": "42 pages", "summary": "Transformers are widely used across data modalities, and yet the principles\ndistilled from text models often transfer imperfectly to models trained to\nother modalities. In this paper, we analyze Transformers through the lens of\nrank structure. Our focus is on the time series setting, where the structural\nproperties of the data differ remarkably from those of text or vision. We show\nthat time-series embeddings, unlike text or vision, exhibit sharply decaying\nsingular value spectra: small patch sizes and smooth continuous mappings\nconcentrate the data into low-rank subspaces. From this, we prove that the\nassociated $Q/K/V$ projections admit accurate low-rank approximations, and that\nattention layers become compressible in proportion to the decay of the\nembedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by\nwhich nonlinear mixing across depth inflates the rank, explaining why early\nlayers are most amenable to compression and why ranks grow with depth. Guided\nby these theoretical and empirical results, we use these insights to compress\nChronos, a large time series foundation model, achieving a reduction of $65\\%$\nin inference time and $81\\%$ in memory, without loss of accuracy. Our findings\nprovide principled guidance for allocating width, depth, and heads in time\nseries foundation models, and for exploiting their inherent compressibility."}
{"id": "2510.03360", "pdf": "https://arxiv.org/pdf/2510.03360", "abs": "https://arxiv.org/abs/2510.03360", "authors": ["Zelin Zhao", "Zongyi Li", "Kimia Hassibi", "Kamyar Azizzadenesheli", "Junchi Yan", "H. Jane Bae", "Di Zhou", "Anima Anandkumar"], "title": "Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows", "categories": ["cs.LG", "cs.AI", "math.OC", "physics.flu-dyn"], "comment": null, "summary": "Assessing turbulence control effects for wall friction numerically is a\nsignificant challenge since it requires expensive simulations of turbulent\nfluid dynamics. We instead propose an efficient deep reinforcement learning\n(RL) framework for modeling and control of turbulent flows. It is model-based\nRL for predictive control (PC), where both the policy and the observer models\nfor turbulence control are learned jointly using Physics Informed Neural\nOperators (PINO), which are discretization invariant and can capture fine\nscales in turbulent flows accurately. Our PINO-PC outperforms prior model-free\nreinforcement learning methods in various challenging scenarios where the flows\nare of high Reynolds numbers and unseen, i.e., not provided during model\ntraining. We find that PINO-PC achieves a drag reduction of 39.0\\% under a\nbulk-velocity Reynolds number of 15,000, outperforming previous fluid control\nmethods by more than 32\\%."}
{"id": "2510.03361", "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models."}
{"id": "2510.03363", "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD."}
{"id": "2510.03364", "pdf": "https://arxiv.org/pdf/2510.03364", "abs": "https://arxiv.org/abs/2510.03364", "authors": ["Xiaolong Ma", "Xu Dong", "Ashley Tarrant", "Lei Yang", "Rao Kotamarthi", "Jiali Wang", "Feng Yan", "Rajkumar Kettimuthu"], "title": "Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "High-quality observations of hub-height winds are valuable but sparse in\nspace and time. Simulations are widely available on regular grids but are\ngenerally biased and too coarse to inform wind-farm siting or to assess\nextreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully\nutilize both data types for generating high-quality, high-resolution hub-height\nwind speeds (tens to ~100m above ground), this study introduces WindSR, a\ndiffusion model with data assimilation for super-resolution downscaling of\nhub-height winds. WindSR integrates sparse observational data with simulation\nfields during downscaling using state-of-the-art diffusion models. A\ndynamic-radius blending method is introduced to merge observations with\nsimulations, providing conditioning for the diffusion process. Terrain\ninformation is incorporated during both training and inference to account for\nits role as a key driver of winds. Evaluated against\nconvolutional-neural-network and generative-adversarial-network baselines,\nWindSR outperforms them in both downscaling efficiency and accuracy. Our data\nassimilation reduces WindSR's model bias by approximately 20% relative to\nindependent observations."}
{"id": "2510.03366", "pdf": "https://arxiv.org/pdf/2510.03366", "abs": "https://arxiv.org/abs/2510.03366", "authors": ["Harshwardhan Fartale", "Ashish Kattamuri", "Rahul Raja", "Arpita Vats", "Ishita Prasad", "Akshata Kishore Moharir"], "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer-based language models excel at both recall (retrieving memorized\nfacts) and reasoning (performing multi-step inference), but whether these\nabilities rely on distinct internal mechanisms remains unclear. Distinguishing\nrecall from reasoning is crucial for predicting model generalization, designing\ntargeted evaluations, and building safer interventions that affect one ability\nwithout disrupting the other.We approach this question through mechanistic\ninterpretability, using controlled datasets of synthetic linguistic puzzles to\nprobe transformer models at the layer, head, and neuron level. Our pipeline\ncombines activation patching and structured ablations to causally measure\ncomponent contributions to each task type. Across two model families (Qwen and\nLLaMA), we find that interventions on distinct layers and attention heads lead\nto selective impairments: disabling identified \"recall circuits\" reduces\nfact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas\ndisabling \"reasoning circuits\" reduces multi-step inference by a comparable\nmargin. At the neuron level, we observe task-specific firing patterns, though\nthese effects are less robust, consistent with neuronal polysemanticity.Our\nresults provide the first causal evidence that recall and reasoning rely on\nseparable but interacting circuits in transformer models. These findings\nadvance mechanistic interpretability by linking circuit-level structure to\nfunctional specialization and demonstrate how controlled datasets and causal\ninterventions can yield mechanistic insights into model cognition, informing\nsafer deployment of large language models."}
{"id": "2510.03368", "pdf": "https://arxiv.org/pdf/2510.03368", "abs": "https://arxiv.org/abs/2510.03368", "authors": ["Kiana Jafari Meimandi", "Anka Reuel", "Gabriela Aranguiz-Dias", "Hatim Rahama", "Ala-Eddine Ayadi", "Xavier Boullier", "Jérémy Verdo", "Louis Montanie", "Mykel Kochenderfer"], "title": "An Adaptive Responsible AI Governance Framework for Decentralized Organizations", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This paper examines the assessment challenges of Responsible AI (RAI)\ngovernance efforts in globally decentralized organizations through a case study\ncollaboration between a leading research university and a multinational\nenterprise. While there are many proposed frameworks for RAI, their application\nin complex organizational settings with distributed decision-making authority\nremains underexplored. Our RAI assessment, conducted across multiple business\nunits and AI use cases, reveals four key patterns that shape RAI\nimplementation: (1) complex interplay between group-level guidance and local\ninterpretation, (2) challenges translating abstract principles into operational\npractices, (3) regional and functional variation in implementation approaches,\nand (4) inconsistent accountability in risk oversight. Based on these findings,\nwe propose an Adaptive RAI Governance (ARGO) Framework that balances central\ncoordination with local autonomy through three interdependent layers: shared\nfoundation standards, central advisory resources, and contextual local\nimplementation. We contribute insights from academic-industry collaboration for\nRAI assessments, highlighting the importance of modular governance approaches\nthat accommodate organizational complexity while maintaining alignment with\nresponsible AI principles. These lessons offer practical guidance for\norganizations navigating the transition from RAI principles to operational\npractice within decentralized structures."}
{"id": "2510.03369", "pdf": "https://arxiv.org/pdf/2510.03369", "abs": "https://arxiv.org/abs/2510.03369", "authors": ["Huazhen Wang", "Huimin Yang", "Hainbin Lin", "Yan Dong", "Lili Chen", "Liangliang Xia", "Wenwen Xu"], "title": "TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design", "categories": ["cs.CY", "cs.AI"], "comment": "16 pages, 4 figures", "summary": "Interdisciplinary teaching is a cornerstone of modern curriculum reform, but\nits implementation is hindered by challenges in knowledge integration and\ntime-consuming lesson planning. Existing tools often lack the required\npedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot\nplatform designed to solve these problems. TriQuest uses large language models\nand knowledge graphs via an intuitive GUI to help teachers efficiently generate\nhigh-quality interdisciplinary lesson plans. Its core features include\nintelligent knowledge integration from various disciplines and a human-computer\ncollaborative review process to ensure quality and innovation.In a study with\n43 teachers, TriQuest increased curriculum design efficiency by an average of\n75% and improved lesson plan quality scores by 41%. It also significantly\nlowered design barriers and cognitive load. Our work presents a new paradigm\nfor empowering teacher professional development with intelligent technologies."}
{"id": "2510.03370", "pdf": "https://arxiv.org/pdf/2510.03370", "abs": "https://arxiv.org/abs/2510.03370", "authors": ["Junde Xu", "Yapin Shi", "Lijun Lang", "Taoyong Cui", "Zhiming Zhang", "Guangyong Chen", "Jiezhong Qiu", "Pheng-Ann Heng"], "title": "InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions", "categories": ["q-bio.QM", "cs.AI", "cs.CE"], "comment": "preprint", "summary": "Multimodal protein language models deliver strong performance on\nmutation-effect prediction, but training such models from scratch demands\nsubstantial computational resources. In this paper, we propose a fine-tuning\nframework called InstructPLM-mu and try to answer a question: \\textit{Can\nmultimodal fine-tuning of a pretrained, sequence-only protein language model\nmatch the performance of models trained end-to-end? } Surprisingly, our\nexperiments show that fine-tuning ESM2 with structural inputs can reach\nperformance comparable to ESM3. To understand how this is achieved, we\nsystematically compare three different feature-fusion designs and fine-tuning\nrecipes. Our results reveal that both the fusion method and the tuning strategy\nstrongly affect final accuracy, indicating that the fine-tuning process is not\ntrivial. We hope this work offers practical guidance for injecting structure\ninto pretrained protein language models and motivates further research on\nbetter fusion mechanisms and fine-tuning protocols."}
{"id": "2510.03371", "pdf": "https://arxiv.org/pdf/2510.03371", "abs": "https://arxiv.org/abs/2510.03371", "authors": ["Sasho Nedelkoski", "Alexander Acker", "Odej Kao", "Soeren Becker", "Dominik Scheinert"], "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "NeurIPS 2025 - DynaFront 2025: Dynamics at the Frontiers of\n  Optimization, Sampling, and Games Workshop", "summary": "The training of large models demands substantial computational resources,\ntypically available only in data centers with high-bandwidth interconnects.\nHowever, reducing the reliance on high-bandwidth interconnects between nodes\nenables the use of distributed compute resources as an alternative to\ncentralized data center training. Building on recent advances in distributed\nmodel training, we propose an approach that further reduces communication by\ncombining infrequent synchronizations across distributed model replicas with\ngradient momentum compression. In particular, we treat the optimizer momentum\nas a signal and decompose the Nesterov momentum into high- and low-frequency\ncomponents via the discrete cosine transform (DCT). Only the high-frequency\ncomponents are synchronized across model replicas every $H$ steps. Empirically,\nour method achieves up to a $16\\times$ reduction in communication compared to\nthe baseline DiLoCo, and it generalizes across architectures, including\ntransformer-based language models and convolutional neural networks for images.\nOverall, this work advances the feasibility of training large models on\ndistributed nodes with low-bandwidth interconnects."}
{"id": "2510.03372", "pdf": "https://arxiv.org/pdf/2510.03372", "abs": "https://arxiv.org/abs/2510.03372", "authors": ["Juampablo E. Heras Rivera", "Caitlin M. Neher", "Mehmet Kurt"], "title": "Real-time nonlinear inversion of magnetic resonance elastography with operator learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "$\\textbf{Purpose:}$ To develop and evaluate an operator learning framework\nfor nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE)\ndata, which enables real-time inversion of elastograms with comparable spatial\naccuracy to NLI.\n  $\\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data\nfrom 61 individuals (mean age, 37.4 years; 34 female) were used for development\nof the framework. A predictive deep operator learning framework (oNLI) was\ntrained using 10-fold cross-validation, with the complex curl of the measured\ndisplacement field as inputs and NLI-derived reference elastograms as outputs.\nA structural prior mechanism, analogous to Soft Prior Regularization in the MRE\nliterature, was incorporated to improve spatial accuracy. Subject-level\nevaluation metrics included Pearson's correlation coefficient, absolute\nrelative error, and structural similarity index measure between predicted and\nreference elastograms across brain regions of different sizes to understand\naccuracy. Statistical analyses included paired t-tests comparing the proposed\noNLI variants to the convolutional neural network baselines.\n  $\\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\\pm$ 0.5\n($\\mu'$) and 10.0 $\\pm$ 0.7 ($\\mu''$) for oNLI and 15.8 $\\pm$ 0.8 ($\\mu'$) and\n26.1 $\\pm$ 1.1 ($\\mu''$) for CNNs. Additionally, oNLI outperformed\nconvolutional architectures as per Pearson's correlation coefficient, $r$, in\nthe whole brain and across all subregions for both the storage modulus and loss\nmodulus (p < 0.05).\n  $\\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion\n(30,000x speedup), outperforming CNN-based approaches and maintaining the\nfine-grained spatial accuracy achievable with NLI in the brain."}
{"id": "2510.03374", "pdf": "https://arxiv.org/pdf/2510.03374", "abs": "https://arxiv.org/abs/2510.03374", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Jérôme Da-Rugna"], "title": "Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Published in the 36th Central European Conference on Information and\n  Intelligent Systems(CECIIS)at: Vara\\v{z}din, Croatia. September 17-19/2025.\n  ISSN 1847-2001 (Print). ISSN 1848-2295 (Online)", "summary": "The rapid integration of Artificial Intelligence (AI) into educational\ntechnology promises to revolutionize content creation and assessment. However,\nthe quality and pedagogical alignment of AI-generated content remain critical\nchallenges. This paper investigates the impact of lightweight prompt\nengineering strategies on the cognitive alignment of AI-generated questions\nwithin OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate\nthree prompt variants-a detailed baseline, a simpler version, and a\npersona-based approach-across Knowledge, Application, and Analysis levels of\nBloom's Taxonomy. Utilizing an automated classification model (from prior work)\nand human review, our findings demonstrate that explicit, detailed prompts are\ncrucial for precise cognitive alignment. While simpler and persona-based\nprompts yield clear and relevant questions, they frequently misalign with\nintended Bloom's levels, generating outputs that are either too complex or\ndeviate from the desired cognitive objective. This study underscores the\nimportance of strategic prompt engineering in fostering pedagogically sound\nAI-driven educational solutions and advises on optimizing AI for quality\ncontent generation in learning analytics and smart learning environments."}
{"id": "2510.03379", "pdf": "https://arxiv.org/pdf/2510.03379", "abs": "https://arxiv.org/abs/2510.03379", "authors": ["Frederic Higham", "Tommy Yuan"], "title": "Can an AI-Powered Presentation Platform Based On The Game \"Just a Minute\" Be Used To Improve Students' Public Speaking Skills?", "categories": ["cs.CY", "cs.AI"], "comment": "11 pages, to be presented orally at the International Conference on\n  Education and Artificial Intelligence Technologies (Nov 2025)", "summary": "This study explores the effectiveness of applying AI and gamification into a\npresentation platform aimed at University students wanting to improve their\npublic speaking skills in their native tongue. Specifically, a platform based\non the radio show, Just a Minute (JAM), is explored. In this game, players are\nchallenged to speak fluently on a topic for 60 seconds without repeating\nthemselves, hesitating or deviating from the topic. JAM has proposed benefits\nsuch as allowing students to improve their spontaneous speaking skills and\nreduce their use of speech disfluencies (\"um\", \"uh\", etc.).\n  Previous research has highlighted the difficulties students face when\nspeaking publicly, the main one being anxiety. AI Powered Presentation\nPlatforms (AI-PPPs), where students can speak with an immersive AI audience and\nreceive real-time feedback, have been explored as a method to improve student's\nspeaking skills and confidence. So far they have shown promising results which\nthis study aims to build upon.\n  A group of students from the University of York are enlisted to evaluate the\neffectiveness of the JAM platform. They are asked to fill in a questionnaire,\nplay through the game twice and then complete a final questionnaire to discuss\ntheir experiences playing the game. Various statistics are gathered during\ntheir gameplay such as the number of points they gained and the number of rules\nthey broke. The results showed that students found the game promising and\nbelieved that their speaking skills could improve if they played the game for\nlonger. More work will need to be carried out to prove the effectiveness of the\ngame beyond the short term."}
{"id": "2510.03380", "pdf": "https://arxiv.org/pdf/2510.03380", "abs": "https://arxiv.org/abs/2510.03380", "authors": ["Michael Ben Ali", "Imen Megdiche", "André Peninou", "Olivier Teste"], "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is a decentralized paradigm that enables a\nclient-server architecture to collaboratively train a global Artificial\nIntelligence model without sharing raw data, thereby preserving privacy. A key\nchallenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of\nNon-IID, where clients hold highly heterogeneous data volumes. Clustered\nFederated Learning (CFL) is an emergent variant of FL that presents a promising\nsolution to Non-IID problem. It improves models' performance by grouping\nclients with similar data distributions into clusters. CFL methods generally\nfall into two operating strategies. In the first strategy, clients select the\ncluster that minimizes the local training loss. In the second strategy, the\nserver groups clients based on local model similarities. However, most CFL\nmethods lack systematic evaluation under QS but present significant challenges\nbecause of it. In this paper, we present two main contributions. The first one\nis an evaluation of state-of-the-art CFL algorithms under various Non-IID\nsettings, applying multiple QS scenarios to assess their robustness. Our second\ncontribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes\nan optimal coordination between both operating strategies of CFL. Our approach\nis robust against the different variations of QS settings. We conducted\nintensive experiments on six image classification datasets, resulting in 270\nNon-IID configurations. The results show that CORNFLQS achieves the highest\naverage ranking in both accuracy and clustering quality, as well as strong\nrobustness to QS perturbations. Overall, our approach outperforms actual CFL\nalgorithms."}
{"id": "2510.03381", "pdf": "https://arxiv.org/pdf/2510.03381", "abs": "https://arxiv.org/abs/2510.03381", "authors": ["Yongchao Li", "Jun Chen", "Zhuoxuan Li", "Chao Gao", "Yang Li", "Chu Zhang", "Changyin Dong"], "title": "Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Interchanges are crucial nodes for vehicle transfers between highways, yet\nthe lack of real-time ramp detectors creates blind spots in traffic prediction.\nTo address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a\ntwo-stage framework that leverages cross-modal reconstruction pretraining. In\nthe first stage, STDAE reconstructs historical ramp flows from mainline data,\nforcing the model to capture intrinsic spatio-temporal relations. Its decoupled\narchitecture with parallel spatial and temporal autoencoders efficiently\nextracts heterogeneous features. In the prediction stage, the learned\nrepresentations are integrated with models such as GWNet to enhance accuracy.\nExperiments on three real-world interchange datasets show that STDAE-GWNET\nconsistently outperforms thirteen state-of-the-art baselines and achieves\nperformance comparable to models using historical ramp data. This demonstrates\nits effectiveness in overcoming detector scarcity and its plug-and-play\npotential for diverse forecasting pipelines."}
{"id": "2510.03384", "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited."}
{"id": "2510.03405", "pdf": "https://arxiv.org/pdf/2510.03405", "abs": "https://arxiv.org/abs/2510.03405", "authors": ["Sanket Badhe"], "title": "LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits", "categories": ["cs.MA", "cs.AI", "cs.CR"], "comment": "12 pages with 2 figures, accepted at the NLLP workshop at EMNLP 2025", "summary": "We present LegalSim, a modular multi-agent simulation of adversarial legal\nproceedings that explores how AI systems can exploit procedural weaknesses in\ncodified rules. Plaintiff and defendant agents choose from a constrained action\nspace (for example, discovery requests, motions, meet-and-confer, sanctions)\ngoverned by a JSON rules engine, while a stochastic judge model with calibrated\ngrant rates, cost allocations, and sanction tendencies resolves outcomes. We\ncompare four policies: PPO, a contextual bandit with an LLM, a direct LLM\npolicy, and a hand-crafted heuristic; Instead of optimizing binary case\noutcomes, agents are trained and evaluated using effective win rate and a\ncomposite exploit score that combines opponent-cost inflation, calendar\npressure, settlement pressure at low merit, and a rule-compliance margin.\nAcross configurable regimes (e.g., bankruptcy stays, inter partes review, tax\nprocedures) and heterogeneous judges, we observe emergent ``exploit chains'',\nsuch as cost-inflating discovery sequences and calendar-pressure tactics that\nremain procedurally valid yet systemically harmful. Evaluation via cross-play\nand Bradley-Terry ratings shows, PPO wins more often, the bandit is the most\nconsistently competitive across opponents, the LLM trails them, and the\nheuristic is weakest. The results are stable in judge settings, and the\nsimulation reveals emergent exploit chains, motivating red-teaming of legal\nrule systems in addition to model-level testing."}
{"id": "2510.03413", "pdf": "https://arxiv.org/pdf/2510.03413", "abs": "https://arxiv.org/abs/2510.03413", "authors": ["L. C. McInnes", "D. Arnold", "P. Balaprakash", "M. Bernhardt", "B. Cerny", "A. Dubey", "R. Giles", "D. W. Hood", "M. A. Leung", "V. Lopez-Marrero", "P. Messina", "O. B. Newton", "C. Oehmen", "S. M. Wild", "J. Willenbring", "L. Woodley", "T. Baylis", "D. E. Bernholdt", "C. Camano", "J. Cohoon", "C. Ferenbaugh", "S. M. Fiore", "S. Gesing", "D. Gomez-Zara", "J. Howison", "T. Islam", "D. Kepczynski", "C. Lively", "H. Menon", "B. Messer", "M. Ngom", "U. Paliath", "M. E. Papka", "I. Qualters", "E. M. Raybourn", "K. Riley", "P. Rodriguez", "D. Rouson", "M. Schwalbe", "S. K. Seal", "O. Surer", "V. Taylor", "L. Wu"], "title": "Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science", "categories": ["cs.CE", "cs.AI", "cs.MS", "68T01, 68U01, 97M10", "I.6.0; I.2.0; G.4; D.0"], "comment": "38 pages, 6 figures", "summary": "This report summarizes insights from the 2025 Workshop on Next-Generation\nEcosystems for Scientific Computing: Harnessing Community, Software, and AI for\nCross-Disciplinary Team Science, which convened more than 40 experts from\nnational laboratories, academia, industry, and community organizations to chart\na path toward more powerful, sustainable, and collaborative scientific software\necosystems. To address urgent challenges at the intersection of\nhigh-performance computing (HPC), AI, and scientific software, participants\nenvisioned agile, robust ecosystems built through socio-technical\nco-design--the intentional integration of social and technical components as\ninterdependent parts of a unified strategy. This approach combines advances in\nAI, HPC, and software with new models for cross-disciplinary collaboration,\ntraining, and workforce development. Key recommendations include building\nmodular, trustworthy AI-enabled scientific software systems; enabling\nscientific teams to integrate AI systems into their workflows while preserving\nhuman creativity, trust, and scientific rigor; and creating innovative training\npipelines that keep pace with rapid technological change. Pilot projects were\nidentified as near-term catalysts, with initial priorities focused on hybrid\nAI/HPC infrastructure, cross-disciplinary collaboration and pedagogy,\nresponsible AI guidelines, and prototyping of public-private partnerships. This\nreport presents a vision of next-generation ecosystems for scientific computing\nwhere AI, software, hardware, and human expertise are interwoven to drive\ndiscovery, expand access, strengthen the workforce, and accelerate scientific\nprogress."}
{"id": "2510.03415", "pdf": "https://arxiv.org/pdf/2510.03415", "abs": "https://arxiv.org/abs/2510.03415", "authors": ["Aditya Thimmaiah", "Jiyang Zhang", "Jayanth Srinivasa", "Junyi Jessy Li", "Milos Gligoric"], "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.SE"], "comment": null, "summary": "As large language models (LLMs) excel at code reasoning, a natural question\narises: can an LLM execute programs (i.e., act as an interpreter) purely based\non a programming language's formal semantics? If so, it will enable rapid\nprototyping of new programming languages and language features. We study this\nquestion using the imperative language IMP (a subset of C), formalized via\nsmall-step operational semantics (SOS) and rewriting-based operational\nsemantics (K-semantics). We introduce three evaluation sets-Human-Written,\nLLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by\ncode-complexity metrics spanning the size, control-flow, and data-flow axes.\nGiven a program and its semantics formalized with SOS/K-semantics, models are\nevaluated on three tasks ranging from coarse to fine: (1) final-state\nprediction, (2) semantic rule prediction, and (3) execution trace prediction.\nTo distinguish pretraining memorization from semantic competence, we define two\nnonstandard semantics obtained through systematic mutations of the standard\nrules. Across strong code/reasoning LLMs, performance drops under nonstandard\nsemantics despite high performance under the standard one. We further find that\n(i) there are patterns to different model failures, (ii) most reasoning models\nperform exceptionally well on coarse grained tasks involving reasoning about\nhighly complex programs often containing nested loop depths beyond five, and\nsurprisingly, (iii) providing formal semantics helps on simple programs but\noften hurts on more complex ones. Overall, the results show a promise that LLMs\ncould serve as programming language interpreters, but points to the lack of\ntheir robust semantics understanding. We release the benchmark and the\nsupporting code at https://github.com/EngineeringSoftware/PLSemanticsBench."}
{"id": "2510.03417", "pdf": "https://arxiv.org/pdf/2510.03417", "abs": "https://arxiv.org/abs/2510.03417", "authors": ["Javad Rafiei Asl", "Sidhant Narula", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks", "categories": ["cs.CR", "cs.AI"], "comment": "Javad Rafiei Asl and Sidhant Narula are co-first authors", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"}
{"id": "2510.03419", "pdf": "https://arxiv.org/pdf/2510.03419", "abs": "https://arxiv.org/abs/2510.03419", "authors": ["Joseph Rawson", "Domniki Ladopoulou", "Petros Dellaportas"], "title": "Multi-task neural diffusion processes for uncertainty-quantified wind power prediction", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "comment": "36 pages, 13 figures, 2 tables,", "summary": "Uncertainty-aware wind power prediction is essential for grid integration and\nreliable wind farm operation. We apply neural diffusion processes (NDPs)-a\nrecent class of models that learn distributions over functions-and extend them\nto a multi-task NDP (MT-NDP) framework for wind power prediction. We provide\nthe first empirical evaluation of NDPs in real supervisory control and data\nacquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture\ncross-turbine correlations and enable few-shot adaptation to unseen turbines.\nThe proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of\npoint accuracy and calibration, particularly for wind turbines whose behaviour\ndeviates from the fleet average. In general, NDP-based models deliver\ncalibrated and scalable predictions suitable for operational deployment,\noffering sharper, yet trustworthy, predictive intervals that can support\ndispatch and maintenance decisions in modern wind farms."}
{"id": "2510.03426", "pdf": "https://arxiv.org/pdf/2510.03426", "abs": "https://arxiv.org/abs/2510.03426", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "18 pages, 4 figures (main text). 14 pages, 21 figures (appendix)", "summary": "Many domains, from deep learning to finance, require compounding real numbers\nover long sequences, often leading to catastrophic numerical underflow or\noverflow. We introduce generalized orders of magnitude (GOOMs), a principled\nextension of traditional orders of magnitude that incorporates floating-point\nnumbers as a special case, and which in practice enables stable computation\nover significantly larger dynamic ranges of real numbers than previously\npossible. We implement GOOMs, along with an efficient custom parallel prefix\nscan, to support native execution on parallel hardware such as GPUs. We\ndemonstrate that our implementation of GOOMs outperforms traditional approaches\nwith three representative experiments, all of which were previously considered\nimpractical or impossible, and now become possible and practical: (1)\ncompounding real matrix products far beyond standard floating-point limits; (2)\nestimating spectra of Lyapunov exponents in parallel, orders of magnitude\nfaster than with previous methods, applying a novel selective-resetting method\nto prevent state colinearity; and (3) capturing long-range dependencies in deep\nrecurrent neural networks with non-diagonal recurrent states, computed in\nparallel via a prefix scan, without requiring any form of stabilization. Our\nresults show that our implementation of GOOMs, combined with efficient parallel\nscanning, offers a scalable and numerically robust alternative to conventional\nfloating-point numbers for high-dynamic-range applications."}
{"id": "2510.03431", "pdf": "https://arxiv.org/pdf/2510.03431", "abs": "https://arxiv.org/abs/2510.03431", "authors": ["Refik Mert Cam", "Seonyeong Park", "Umberto Villa", "Mark A. Anastasio"], "title": "Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography", "categories": ["physics.med-ph", "cs.AI", "eess.SP"], "comment": "Preprint submitted to Elsevier Photoacoustics", "summary": "Quantitative photoacoustic computed tomography (qPACT) is a promising imaging\nmodality for estimating physiological parameters such as blood oxygen\nsaturation. However, developing robust qPACT reconstruction methods remains\nchallenging due to computational demands, modeling difficulties, and\nexperimental uncertainties. Learning-based methods have been proposed to\naddress these issues but remain largely unvalidated. Virtual imaging (VI)\nstudies are essential for validating such methods early in development, before\nproceeding to less-controlled phantom or in vivo studies. Effective VI studies\nmust employ ensembles of stochastically generated numerical phantoms that\naccurately reflect relevant anatomy and physiology. Yet, most prior VI studies\nfor qPACT relied on overly simplified phantoms. In this work, a realistic VI\ntestbed is employed for the first time to assess a representative 3D\nlearning-based qPACT reconstruction method for breast imaging. The method is\nevaluated across subject variability and physical factors such as measurement\nnoise and acoustic aberrations, offering insights into its strengths and\nlimitations."}
{"id": "2510.03438", "pdf": "https://arxiv.org/pdf/2510.03438", "abs": "https://arxiv.org/abs/2510.03438", "authors": ["Grace Ra Kim", "Duncan Eddy", "Vedant Srinivas", "Mykel J. Kochenderfer"], "title": "Scalable Ground Station Selection for Large LEO Constellations", "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "comment": "14 pages, 7 tables, 10 figures, submitted to IEEE Aeroconf 2026", "summary": "Effective ground station selection is critical for low Earth orbiting (LEO)\nsatellite constellations to minimize operational costs, maximize data downlink\nvolume, and reduce communication gaps between access windows. Traditional\nground station selection typically begins by choosing from a fixed set of\nlocations offered by Ground Station-as-a-Service (GSaaS) providers, which helps\nreduce the problem scope to optimizing locations over existing infrastructure.\nHowever, finding a globally optimal solution for stations using existing\nmixed-integer programming methods quickly becomes intractable at scale,\nespecially when considering multiple providers and large satellite\nconstellations. To address this issue, we introduce a scalable, hierarchical\nframework that decomposes the global selection problem into single-satellite,\nshort time-window subproblems. Optimal station choices from each subproblem are\nclustered to identify consistently high-value locations across all decomposed\ncases. Cluster-level sets are then matched back to the closest GSaaS candidate\nsites to produce a globally feasible solution. This approach enables scalable\ncoordination while maintaining near-optimal performance. We evaluate our\nmethod's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10\nstations), achieving solutions within 95% of the global IP optimum for all test\ncases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and\nPlanet's Flock (96) show that while exact IP solutions fail to scale, our\nframework continues to deliver high-quality site selections."}
{"id": "2510.03441", "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications."}
{"id": "2510.03442", "pdf": "https://arxiv.org/pdf/2510.03442", "abs": "https://arxiv.org/abs/2510.03442", "authors": ["Ege Cakar", "Per Ola Kristensson"], "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "8 pages, 4 figures, 6 tables, submitted to IAAI-26", "summary": "Humans are black boxes -- we cannot observe their neural processes, yet\nsociety functions by evaluating verifiable arguments. AI explainability should\nfollow this principle: stakeholders need verifiable reasoning chains, not\nmechanistic transparency. We propose using structured argumentation to provide\na level of explanation and verification neither interpretability nor\nLLM-generated explanation is able to offer. Our pipeline achieves\nstate-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7\npoints above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous\npublished results with comparable data setups, for Argumentative MicroTexts\nrelation classification, converting LLM text into argument graphs and enabling\nverification at each inferential step. We demonstrate this idea on multi-agent\nrisk assessment using the Structured What-If Technique, where specialized\nagents collaborate transparently to carry out risk assessment otherwise\nachieved by humans alone. Using Bipolar Assumption-Based Argumentation, we\ncapture support/attack relationships, thereby enabling automatic hallucination\ndetection via fact nodes attacking arguments. We also provide a verification\nmechanism that enables iterative refinement through test-time feedback without\nretraining. For easy deployment, we provide a Docker container for the\nfine-tuned AMT model, and the rest of the code with the Bipolar ABA Python\npackage on GitHub."}
{"id": "2510.03463", "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature."}
{"id": "2510.03472", "pdf": "https://arxiv.org/pdf/2510.03472", "abs": "https://arxiv.org/abs/2510.03472", "authors": ["Yulun Zhang", "Alexandre O. G. Barbosa", "Federico Pecora", "Jiaoyang Li"], "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS) 2025", "summary": "We study optimizing a destination-to-chutes task mapping to improve\nthroughput in Robotic Sorting Systems (RSS), where a team of robots sort\npackages on a sortation floor by transporting them from induct workstations to\neject chutes based on their shipping destinations (e.g. Los Angeles or\nPittsburgh). The destination-to-chutes task mapping is used to determine which\nchutes a robot can drop its package. Finding a high-quality task mapping is\nchallenging because of the complexity of a real-world RSS. First, optimizing\ntask mapping is interdependent with robot target assignment and path planning.\nSecond, chutes will be CLOSED for a period of time once they receive sufficient\npackages to allow for downstream processing. Third, task mapping quality\ndirectly impacts the downstream processing, as scattered chutes for the same\ndestination increase package handling time. In this paper, we first formally\ndefine task mappings and the problem of Task Mapping Optimization (TMO). We\nthen present a simulator of RSS to evaluate task mappings. We then present a\nsimple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear\nProgramming, demonstrating the advantage of our optimized task mappings over\nthe greedily generated ones in various RSS setups with different map sizes,\nnumbers of chutes, and destinations. Finally, we use Quality Diversity\nalgorithms to analyze the throughput of a diverse set of task mappings. Our\ncode is available online at https://github.com/lunjohnzhang/tmo_public."}
{"id": "2510.03483", "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52"}
{"id": "2510.03486", "pdf": "https://arxiv.org/pdf/2510.03486", "abs": "https://arxiv.org/abs/2510.03486", "authors": ["Anupam Panwar", "Himadri Pal", "Jiali Chen", "Kyle Cho", "Riddick Jiang", "Miao Zhao", "Rajiv Krishnamurthy"], "title": "Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 7 figures", "summary": "Detecting anomalies in large, distributed systems presents several\nchallenges. The first challenge arises from the sheer volume of data that needs\nto be processed. Flagging anomalies in a high-throughput environment calls for\na careful consideration of both algorithm and system design. The second\nchallenge comes from the heterogeneity of time-series datasets that leverage\nsuch a system in production. In practice, anomaly detection systems are rarely\ndeployed for a single use case. Typically, there are several metrics to\nmonitor, often across several domains (e.g. engineering, business and\noperations). A one-size-fits-all approach rarely works, so these systems need\nto be fine-tuned for every application - this is often done manually. The third\nchallenge comes from the fact that determining the root-cause of anomalies in\nsuch settings is akin to finding a needle in a haystack. Identifying (in real\ntime) a time-series dataset that is associated causally with the anomalous\ntime-series data is a very difficult problem. In this paper, we describe a\nunified framework that addresses these challenges. Reasoning based Anomaly\nDetection Framework (RADF) is designed to perform real time anomaly detection\non very large datasets. This framework employs a novel technique (mSelect) that\nautomates the process of algorithm selection and hyper-parameter tuning for\neach use case. Finally, it incorporates a post-detection capability that allows\nfor faster triaging and root-cause determination. Our extensive experiments\ndemonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly\ndetection models in AUC performance for 5 out of 9 public benchmarking\ndatasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a\ndistinction unmatched by any other state-of-the-art model."}
{"id": "2510.03490", "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text."}
{"id": "2510.03495", "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin Läufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries."}
{"id": "2510.03501", "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara."}
{"id": "2510.03502", "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats."}
{"id": "2510.03511", "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. Sánchez", "Sharvaree Vadgama", "Georg Bökman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost."}
{"id": "2510.03514", "pdf": "https://arxiv.org/pdf/2510.03514", "abs": "https://arxiv.org/abs/2510.03514", "authors": ["Toby Drinkall"], "title": "Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "54 pages; 11 figures", "summary": "As military organisations consider integrating large language models (LLMs)\ninto command and control (C2) systems for planning and decision support,\nunderstanding their behavioural tendencies is critical. This study develops a\nbenchmarking framework for evaluating aspects of legal and moral risk in\ntargeting behaviour by comparing LLMs acting as agents in multi-turn simulated\nconflict. We introduce four metrics grounded in International Humanitarian Law\n(IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target\nRate (DTR) assess compliance with legal targeting principles, while Mean and\nMax Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for\ncivilian harm.\n  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through\n90 multi-agent, multi-turn crisis simulations across three geographic regions.\nOur findings reveal that off-the-shelf LLMs exhibit concerning and\nunpredictable targeting behaviour in simulated conflict environments. All\nmodels violated the IHL principle of distinction by targeting civilian objects,\nwith breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through\ncrisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in\nlate turns. Significant inter-model variation emerged: LLaMA-3.1 selected an\naverage of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while\nGemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These\ndifferences indicate that model selection for deployment constitutes a choice\nabout acceptable legal and moral risk profiles in military operations.\n  This work seeks to provide a proof-of-concept of potential behavioural risks\nthat could emerge from the use of LLMs in Decision Support Systems (AI DSS) as\nwell as a reproducible benchmarking framework with interpretable metrics for\nstandardising pre-deployment testing."}
{"id": "2510.03519", "pdf": "https://arxiv.org/pdf/2510.03519", "abs": "https://arxiv.org/abs/2510.03519", "authors": ["Fangxu Yu", "Hongyu Zhao", "Tianyi Zhou"], "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Time series reasoning is crucial to decision-making in diverse domains,\nincluding finance, energy usage, traffic, weather, and scientific discovery.\nWhile existing time series foundation models (TSFMs) can capture low-level\ndynamic patterns and provide accurate forecasting, further analysis usually\nrequires additional background knowledge and sophisticated reasoning, which are\nlacking in most TSFMs but can be achieved through large language models (LLMs).\nOn the other hand, without expensive post-training, LLMs often struggle with\nthe numerical understanding of time series data. Although it is intuitive to\nintegrate the two types of models, developing effective training recipes that\nalign the two modalities for reasoning tasks is still an open challenge. To\nthis end, we propose TS-Reasoner that aligns the latent representations of\nTSFMs with the textual inputs of LLMs for downstream understanding/reasoning\ntasks. Specifically, we propose a simple yet effective method to curate\ndiverse, synthetic pairs of time series and textual captions for alignment\ntraining. We then develop a two-stage training recipe that applies instruction\nfinetuning after the alignment pretraining. Unlike existing works that train an\nLLM to take time series as inputs, we leverage a pretrained TSFM and freeze it\nduring training. Extensive experiments on several benchmarks demonstrate that\nTS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision\nLanguage Models (VLMs), and Time Series LLMs, but also achieves this with\nremarkable data efficiency, e.g., using less than half the training data."}
{"id": "2510.03520", "pdf": "https://arxiv.org/pdf/2510.03520", "abs": "https://arxiv.org/abs/2510.03520", "authors": ["Kartik Pandit", "Sourav Ganguly", "Arnesh Banerjee", "Shaahin Angizi", "Arnob Ghosh"], "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Ensuring safety is a foundational requirement for large language models\n(LLMs). Achieving an appropriate balance between enhancing the utility of model\noutputs and mitigating their potential for harm is a complex and persistent\nchallenge. Contemporary approaches frequently formalize this problem within the\nframework of Constrained Markov Decision Processes (CMDPs) and employ\nestablished CMDP optimization techniques. However, these methods exhibit two\nnotable limitations. First, their reliance on reward and cost functions renders\nperformance highly sensitive to the underlying scoring mechanism, which must\ncapture semantic meaning rather than being triggered by superficial keywords.\nSecond, CMDP-based training entails tuning dual-variable, a process that is\nboth computationally expensive and does not provide any provable safety\nguarantee for a fixed dual variable that can be exploitable through adversarial\njailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF\n(CS-RLHF) that introduces a cost model trained on a large-scale corpus to\nassign semantically grounded safety scores. In contrast to the lagrangian-based\napproach, CS-RLHF adopts a rectified penalty-based formulation. This design\ndraws on the theory of exact penalty functions in constrained optimization,\nwherein constraint satisfaction is enforced directly through a suitably chosen\npenalty term. With an appropriately scaled penalty, feasibility of the safety\nconstraints can be guaranteed at the optimizer, eliminating the need for\ndual-variable updates. Empirical evaluation demonstrates that CS-RLHF\noutperforms state-of-the-art LLM model responses rendering at-least 5 times\nefficient against nominal and jail-breaking prompts"}
{"id": "2510.03521", "pdf": "https://arxiv.org/pdf/2510.03521", "abs": "https://arxiv.org/abs/2510.03521", "authors": ["Ali Elahi"], "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025", "summary": "In specialized domains, humans often compare new problems against similar\nexamples, highlight nuances, and draw conclusions instead of analyzing\ninformation in isolation. When applying reasoning in specialized contexts with\nLLMs on top of a RAG, the pipeline can capture contextually relevant\ninformation, but it is not designed to retrieve comparable cases or related\nproblems.\n  While RAG is effective at extracting factual information, its outputs in\nspecialized reasoning tasks often remain generic, reflecting broad facts rather\nthan context-specific insights. In finance, it results in generic risks that\nare true for the majority of companies. To address this limitation, we propose\na peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics\nsuch as ROUGE and BERTScore in comparison with human-generated equity research\nand risk."}
{"id": "2510.03536", "pdf": "https://arxiv.org/pdf/2510.03536", "abs": "https://arxiv.org/abs/2510.03536", "authors": ["Zhaohan Meng", "Zaiqiao Meng", "Siwei Liu", "Iadh Ounis"], "title": "TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) perform strongly in static and single-turn\nmedical Question Answer (QA) benchmarks, yet such settings diverge from the\niterative information gathering process required in practical clinical\nconsultations. The MEDIQ framework addresses this mismatch by recasting the\ndiagnosis as an interactive dialogue between a patient and an expert system,\nbut the reliability of LLMs drops dramatically when forced to reason with\ndialogue logs, where clinical facts appear in sentences without clear links. To\nbridge this gap, we introduce TriMediQ, a triplet-structured approach that\nsummarises patient responses into triplets and integrates them into a Knowledge\nGraph (KG), enabling multi-hop reasoning. We introduce a frozen triplet\ngenerator that extracts clinically relevant triplets, using prompts designed to\nensure factual consistency. In parallel, a trainable projection module,\ncomprising a graph encoder and a projector, captures relational information\nfrom the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)\nthe projection module fine-tuning with all LLM weights frozen; and (ii) using\nthe fine-tuned module to guide multi-hop reasoning during inference. We\nevaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up\nto 10.4\\% improvement in accuracy over five baselines on the iMedQA dataset.\nThese results demonstrate that converting patient responses into structured\ntriplet-based graphs enables more accurate clinical reasoning in multi-turn\nsettings, providing a solution for the deployment of LLM-based medical\nassistants."}
{"id": "2510.03544", "pdf": "https://arxiv.org/pdf/2510.03544", "abs": "https://arxiv.org/abs/2510.03544", "authors": ["Yuji Takubo", "Daniele Gammelli", "Marco Pavone", "Simone D'Amico"], "title": "Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers", "categories": ["math.OC", "cs.AI", "cs.RO"], "comment": "14 pages, 7 figures", "summary": "Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed\ndocking, forming the foundation for a scalable space economy. Designing such\nmissions requires rapid exploration of the tradespace between control cost and\nflight time across multiple candidate targets. However, multi-objective\noptimization in this setting is challenging, as the underlying constraints are\noften highly nonconvex, and mission designers must balance accuracy (e.g.,\nsolving the full problem) with efficiency (e.g., convex relaxations), slowing\niteration and limiting design agility. To address these challenges, this paper\nproposes an AI-powered framework that enables agile mission design for a wide\nrange of Earth orbit rendezvous scenarios. Given the orbital information of the\ntarget spacecraft, boundary conditions, and a range of flight times, this work\nproposes a Transformer-based architecture that generates, in a single\nparallelized inference step, a set of near-Pareto optimal trajectories across\nvarying flight times, thereby enabling rapid mission trade studies. The model\nis further extended to accommodate variable flight times and perturbed orbital\ndynamics, supporting realistic multi-objective trade-offs. Validation on\nchance-constrained rendezvous problems with passive safety constraints\ndemonstrates that the model generalizes across both flight times and dynamics,\nconsistently providing high-quality initial guesses that converge to superior\nsolutions in fewer iterations. Moreover, the framework efficiently approximates\nthe Pareto front, achieving runtimes comparable to convex relaxation by\nexploiting parallelized inference. Together, these results position the\nproposed framework as a practical surrogate for nonconvex trajectory generation\nand mark an important step toward AI-driven trajectory design for accelerating\npreliminary mission planning in real-world rendezvous applications."}
{"id": "2510.03548", "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios."}
{"id": "2510.03555", "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications."}
{"id": "2510.03561", "pdf": "https://arxiv.org/pdf/2510.03561", "abs": "https://arxiv.org/abs/2510.03561", "authors": ["Adam Filipek"], "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 13 figures", "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity ($O(L^2)$) with respect to sequence length $L$.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to\nthe number of interactions $N$. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size."}
{"id": "2510.03569", "pdf": "https://arxiv.org/pdf/2510.03569", "abs": "https://arxiv.org/abs/2510.03569", "authors": ["Mohammad Mohaiminul Islam", "Thijs P. Kuipers", "Sharvaree Vadgama", "Coen de Vente", "Afsana Khan", "Clara I. Sánchez", "Erik J. Bekkers"], "title": "Longitudinal Flow Matching for Trajectory Modeling", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Generative models for sequential data often struggle with sparsely sampled\nand high-dimensional trajectories, typically reducing the learning of dynamics\nto pairwise transitions. We propose \\textit{Interpolative Multi-Marginal Flow\nMatching} (IMMFM), a framework that learns continuous stochastic dynamics\njointly consistent with multiple observed time points. IMMFM employs a\npiecewise-quadratic interpolation path as a smooth target for flow matching and\njointly optimizes drift and a data-driven diffusion coefficient, supported by a\ntheoretical condition for stable learning. This design captures intrinsic\nstochasticity, handles irregular sparse sampling, and yields subject-specific\ntrajectories. Experiments on synthetic benchmarks and real-world longitudinal\nneuroimaging datasets show that IMMFM outperforms existing methods in both\nforecasting accuracy and further downstream tasks."}
{"id": "2510.03570", "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization."}
{"id": "2510.03571", "pdf": "https://arxiv.org/pdf/2510.03571", "abs": "https://arxiv.org/abs/2510.03571", "authors": ["Burak Karabulut", "Carlo Manna", "Chris Develder"], "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.2.7; C.2.1"], "comment": "This paper has been submitted and accepted for IEEE SmartGridComm\n  2025", "summary": "Fault detection in power distribution grids is critical for ensuring system\nreliability and preventing costly outages. Moreover, fault detection\nmethodologies should remain robust to evolving grid topologies caused by\nfactors such as reconfigurations, equipment failures, and Distributed Energy\nResource (DER) integration. Current data-driven state-of-the-art methods use\nRecurrent Neural Networks (RNNs) for temporal modeling and Graph Neural\nNetworks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in\nshort). Specifically, for power system fault diagnosis, Graph Convolutional\nNetworks (GCNs) have been adopted. Yet, various more advanced GNN architectures\nhave been proposed and adopted in domains outside of power systems. In this\npaper, we set out to systematically and consistently benchmark various GNN\narchitectures in an RNN+GNN pipeline model. Specifically, to the best of our\nknowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention\n(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive\nbenchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN\nmodels (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring\ntheir generalization potential for deployment in different settings than those\nused for training them. Our experimental results on the IEEE 123-node\ndistribution network show that RGATv2 has superior generalization capabilities,\nmaintaining high performance with an F1-score reduction of $\\sim$12% across\ndifferent topology settings. In contrast, pure RNN models largely fail,\nexperiencing an F1-score reduction of up to $\\sim$60%, while other RGNN\nvariants also exhibit significant performance degradation, i.e., up to\n$\\sim$25% lower F1-scores."}
{"id": "2510.03578", "pdf": "https://arxiv.org/pdf/2510.03578", "abs": "https://arxiv.org/abs/2510.03578", "authors": ["Haoran Li", "Chenhan Xiao", "Muhao Guo", "Yang Weng"], "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "30 pages, 6 figures", "summary": "Learning dynamics is essential for model-based control and Reinforcement\nLearning in engineering systems, such as robotics and power systems. However,\nlimited system measurements, such as those from low-resolution sensors, demand\nsample-efficient learning. Symmetry provides a powerful inductive bias by\ncharacterizing equivariant relations in system states to improve sample\nefficiency. While recent methods attempt to discover symmetries from data, they\ntypically assume a single global symmetry group and treat symmetry discovery\nand dynamic learning as separate tasks, leading to limited expressiveness and\nerror accumulation. In this paper, we propose the Latent Mixture of Symmetries\n(Latent MoS), an expressive model that captures a mixture of symmetry-governed\nlatent factors from complex dynamical measurements. Latent MoS focuses on\ndynamic learning while locally and provably preserving the underlying symmetric\ntransformations. To further capture long-term equivariance, we introduce a\nhierarchical architecture that stacks MoS blocks. Numerical experiments in\ndiverse physical systems demonstrate that Latent MoS outperforms\nstate-of-the-art baselines in interpolation and extrapolation tasks while\noffering interpretable latent representations suitable for future geometric and\nsafety-critical analyses."}
{"id": "2510.03582", "pdf": "https://arxiv.org/pdf/2510.03582", "abs": "https://arxiv.org/abs/2510.03582", "authors": ["Lin Yao", "Da Yang", "James P. C. Duncan", "Ashesh Chattopadhyay", "Pedram Hassanzadeh", "Wahid Bhimji", "Bin Yu"], "title": "Deep learning the sources of MJO predictability: a spectral view of learned features", "categories": ["physics.ao-ph", "cs.AI"], "comment": null, "summary": "The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal\ntropical rainfall phenomenon crucial for global weather and climate; however,\nits dynamics and predictability remain poorly understood. Here, we leverage\ndeep learning (DL) to investigate the sources of MJO predictability, motivated\nby a central difference in MJO theories: which spatial scales are essential for\ndriving the MJO? We first develop a deep convolutional neural network (DCNN) to\nforecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to\n21 and 33 days, respectively, achieving skills comparable to leading\nsubseasonal-to-seasonal models such as NCEP. To identify the spatial scales\nmost relevant for MJO forecasting, we conduct spectral analysis of the latent\nfeature space and find that large-scale patterns dominate the learned signals.\nAdditional experiments show that models using only large-scale signals as the\ninput have the same skills as those using all the scales, supporting the\nlarge-scale view of the MJO. Meanwhile, we find that small-scale signals remain\ninformative: surprisingly, models using only small-scale input can still\nproduce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved\nby reconstructing the large-scale envelope of the small-scale activities, which\naligns with the multi-scale view of the MJO. Altogether, our findings support\nthat large-scale patterns--whether directly included or reconstructed--may be\nthe primary source of MJO predictability."}
{"id": "2510.03591", "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game."}
{"id": "2510.03592", "pdf": "https://arxiv.org/pdf/2510.03592", "abs": "https://arxiv.org/abs/2510.03592", "authors": ["Kehinde O. Aina", "Sehoon Ha"], "title": "Deep Reinforcement Learning for Multi-Agent Coordination", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "comment": "11 pages, 8 figures, 1 table, presented at SWARM 2022, to be\n  published in Journal of Artificial Life and Robotics", "summary": "We address the challenge of coordinating multiple robots in narrow and\nconfined environments, where congestion and interference often hinder\ncollective task performance. Drawing inspiration from insect colonies, which\nachieve robust coordination through stigmergy -- modifying and interpreting\nenvironmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement\nLearning (S-MADRL) framework that leverages virtual pheromones to model local\nand social interactions, enabling decentralized emergent coordination without\nexplicit communication. To overcome the convergence and scalability limitations\nof existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum\nlearning, which decomposes complex tasks into progressively harder\nsub-problems. Simulation results show that our framework achieves the most\neffective coordination of up to eight agents, where robots self-organize into\nasymmetric workload distributions that reduce congestion and modulate group\nperformance. This emergent behavior, analogous to strategies observed in\nnature, demonstrates a scalable solution for decentralized multi-agent\ncoordination in crowded environments with communication constraints."}
{"id": "2510.03597", "pdf": "https://arxiv.org/pdf/2510.03597", "abs": "https://arxiv.org/abs/2510.03597", "authors": ["Sina Alemohammad", "Zhangyang Wang", "Richard G. Baraniuk"], "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": null, "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/SinaAlemohammad/Neon"}
{"id": "2510.03604", "pdf": "https://arxiv.org/pdf/2510.03604", "abs": "https://arxiv.org/abs/2510.03604", "authors": ["Yucheng Wang", "Mohamed Ragab", "Yubo Hou", "Zhenghua Chen", "Min Wu", "Xiaoli Li"], "title": "Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Remaining Useful Life (RUL) prediction for turbofan engines plays a vital\nrole in predictive maintenance, ensuring operational safety and efficiency in\naviation. Although data-driven approaches using machine learning and deep\nlearning have shown potential, they face challenges such as limited data and\ndistribution shifts caused by varying operating conditions. Domain Adaptation\n(DA) has emerged as a promising solution, enabling knowledge transfer from\nsource domains with abundant data to target domains with scarce data while\nmitigating distributional shifts. Given the unique properties of turbofan\nengines, such as complex operating conditions, high-dimensional sensor data,\nand slower-changing signals, it is essential to conduct a focused review of DA\ntechniques specifically tailored to turbofan engines. To address this need,\nthis paper provides a comprehensive review of DA solutions for turbofan engine\nRUL prediction, analyzing key methodologies, challenges, and recent\nadvancements. A novel taxonomy tailored to turbofan engines is introduced,\norganizing approaches into methodology-based (how DA is applied),\nalignment-based (where distributional shifts occur due to operational\nvariations), and problem-based (why certain adaptations are needed to address\nspecific challenges). This taxonomy offers a multidimensional view that goes\nbeyond traditional classifications by accounting for the distinctive\ncharacteristics of turbofan engine data and the standard process of applying DA\ntechniques to this area. Additionally, we evaluate selected DA techniques on\nturbofan engine datasets, providing practical insights for practitioners and\nidentifying key challenges. Future research directions are identified to guide\nthe development of more effective DA techniques, advancing the state of RUL\nprediction for turbofan engines."}
{"id": "2510.03610", "pdf": "https://arxiv.org/pdf/2510.03610", "abs": "https://arxiv.org/abs/2510.03610", "authors": ["Zachary Ezetta", "Wu-chang Feng"], "title": "PentestMCP: A Toolkit for Agentic Penetration Testing", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Agentic AI is transforming security by automating many tasks being performed\nmanually. While initial agentic approaches employed a monolithic architecture,\nthe Model-Context-Protocol has now enabled a remote-procedure call (RPC)\nparadigm to agentic applications, allowing for the flexible construction and\ncomposition of multi-function agents. This paper describes PentestMCP, a\nlibrary of MCP server implementations that support agentic penetration testing.\nBy supporting common penetration testing tasks such as network scanning,\nresource enumeration, service fingerprinting, vulnerability scanning,\nexploitation, and post-exploitation, PentestMCP allows a developer to customize\nmulti-agent workflows for performing penetration tests."}
{"id": "2510.03611", "pdf": "https://arxiv.org/pdf/2510.03611", "abs": "https://arxiv.org/abs/2510.03611", "authors": ["Raquib Bin Yousuf", "Aadyant Khatri", "Shengzhe Xu", "Mandar Sharma", "Naren Ramakrishnan"], "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2025 IEEE International Conference on Knowledge Graph (ICKG)", "summary": "Recently proposed evaluation benchmarks aim to characterize the effective\ncontext length and the forgetting tendencies of large language models (LLMs).\nHowever, these benchmarks often rely on simplistic 'needle in a haystack'\nretrieval or continuation tasks that may not accurately reflect the performance\nof these models in information-dense scenarios. Thus, rather than simple next\ntoken prediction, we argue for evaluating these models on more complex\nreasoning tasks that requires them to induce structured relational knowledge\nfrom the text - such as graphs from potentially noisy natural language content.\nWhile the input text can be viewed as generated in terms of a graph, its\nstructure is not made explicit and connections must be induced from distributed\ntextual cues, separated by long contexts and interspersed with irrelevant\ninformation. Our findings reveal that LLMs begin to exhibit memory drift and\ncontextual forgetting at much shorter effective lengths when tasked with this\nform of relational reasoning, compared to what existing benchmarks suggest.\nWith these findings, we offer recommendations for the optimal use of popular\nLLMs for complex reasoning tasks. We further show that even models specialized\nfor reasoning, such as OpenAI o1, remain vulnerable to early memory drift in\nthese settings. These results point to significant limitations in the models'\nability to abstract structured knowledge from unstructured input and highlight\nthe need for architectural adaptations to improve long-range reasoning."}
{"id": "2510.03614", "pdf": "https://arxiv.org/pdf/2510.03614", "abs": "https://arxiv.org/abs/2510.03614", "authors": ["Christopher Solinas", "Radovan Haluska", "David Sychrovsky", "Finbarr Timbers", "Nolan Bard", "Michael Buro", "Martin Schmid", "Nathan R. Sturtevant", "Michael Bowling"], "title": "Neural Bayesian Filtering", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining\ndistributions over hidden states, called beliefs, in partially observable\nsystems. NBF is trained to find a good latent representation of the beliefs\ninduced by a task. It maps beliefs to fixed-length embedding vectors, which\ncondition generative models for sampling. During filtering, particle-style\nupdates compute posteriors in this embedding space using incoming observations\nand the environment's dynamics. NBF combines the computational efficiency of\nclassical filters with the expressiveness of deep generative models - tracking\nrapidly shifting, multimodal beliefs while mitigating the risk of particle\nimpoverishment. We validate NBF in state estimation tasks in three partially\nobservable environments."}
{"id": "2510.03623", "pdf": "https://arxiv.org/pdf/2510.03623", "abs": "https://arxiv.org/abs/2510.03623", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom"], "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications", "categories": ["cs.CR", "cs.AI"], "comment": "10 pages, 9 figures, 4 tables", "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."}
{"id": "2510.03633", "pdf": "https://arxiv.org/pdf/2510.03633", "abs": "https://arxiv.org/abs/2510.03633", "authors": ["An Vuong", "Susan Gauch"], "title": "Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "17th International Conference on Knowledge Discovery, Knowledge\n  Engineering and Knowledge Management (KDIR 2025), Marbella, Spain, Oct.\n  22-24, 2025 (to appear) Best Student Paper Finalist", "summary": "Accurately predicting short-term stock price movement remains a challenging\ntask due to the market's inherent volatility and sensitivity to investor\nsentiment. This paper discusses a deep learning framework that integrates\nemotion features extracted from tweet data with historical stock price\ninformation to forecast significant price changes on the following day. We\nutilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby\nenhancing the quality of emotion features derived from three emotion analysis\napproaches: a transformer-based DistilRoBERTa classifier from the Hugging Face\nlibrary and two lexicon-based methods using National Research Council Canada\n(NRC) resources. These features are combined with previous-day stock price data\nto train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,\nAAPL, and AMZN stocks show that all three emotion analysis methods improve the\naverage accuracy for predicting significant price movements, compared to the\nbaseline model using only historical stock prices, which yields an accuracy of\n13.5%. The DistilRoBERTa-based stock prediction model achieves the best\nperformance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced\nemotion analysis. These results demonstrate that using large language models to\npreprocess tweet content enhances the effectiveness of emotion analysis which\nin turn improves the accuracy of predicting significant stock price movements."}
{"id": "2510.03638", "pdf": "https://arxiv.org/pdf/2510.03638", "abs": "https://arxiv.org/abs/2510.03638", "authors": ["Jialin Liu", "Lisang Ding", "Stanley Osher", "Wotao Yin"], "title": "Implicit Models: Expressive Power Scales with Test-Time Compute", "categories": ["cs.LG", "cs.AI", "math.RT", "stat.ML"], "comment": null, "summary": "Implicit models, an emerging model class, compute outputs by iterating a\nsingle parameter block to a fixed point. This architecture realizes an\ninfinite-depth, weight-tied network that trains with constant memory,\nsignificantly reducing memory needs for the same level of performance compared\nto explicit models. While it is empirically known that these compact models can\noften match or even exceed larger explicit networks by allocating more\ntest-time compute, the underlying mechanism remains poorly understood.\n  We study this gap through a nonparametric analysis of expressive power. We\nprovide a strict mathematical characterization, showing that a simple and\nregular implicit operator can, through iteration, progressively express more\ncomplex mappings. We prove that for a broad class of implicit models, this\nprocess lets the model's expressive power scale with test-time compute,\nultimately matching a much richer function class. The theory is validated\nacross three domains: image reconstruction, scientific computing, and\noperations research, demonstrating that as test-time iterations increase, the\ncomplexity of the learned mapping rises, while the solution quality\nsimultaneously improves and stabilizes."}
{"id": "2510.03639", "pdf": "https://arxiv.org/pdf/2510.03639", "abs": "https://arxiv.org/abs/2510.03639", "authors": ["Liming Wang", "Junrui Ni", "Kai-Wei Chang", "Saurabhchand Bhati", "David Harwath", "Mark Hasegawa-Johnson", "James R. Glass"], "title": "Towards Unsupervised Speech Recognition at the Syllable-Level", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training speech recognizers with unpaired speech and text -- known as\nunsupervised speech recognition (UASR) -- is a crucial step toward extending\nASR to low-resource languages in the long-tail distribution and enabling\nmultimodal learning from non-parallel data. However, existing approaches based\non phones often rely on costly resources such as grapheme-to-phoneme converters\n(G2Ps) and struggle to generalize to languages with ambiguous phoneme\nboundaries due to training instability. In this paper, we address both\nchallenges by introducing a syllable-level UASR framework based on masked\nlanguage modeling, which avoids the need for G2P and the instability of\nGAN-based methods. Our approach achieves up to a 40\\% relative reduction in\ncharacter error rate (CER) on LibriSpeech and generalizes effectively to\nMandarin, a language that has remained particularly difficult for prior\nmethods. Code will be released upon acceptance."}
{"id": "2510.03650", "pdf": "https://arxiv.org/pdf/2510.03650", "abs": "https://arxiv.org/abs/2510.03650", "authors": ["Amir Sadikov"], "title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "cs.NE", "math.NA"], "comment": null, "summary": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo\n(QMC) methods for high-dimensional integration. We cast two long-standing QMC\ndesign problems as program synthesis and solve them with an LLM-guided\nevolutionary loop that mutates and selects code under task-specific fitness:\n(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)\nchoosing Sobol' direction numbers that minimize randomized QMC error on\ndownstream integrands. Our two-phase procedure combines constructive code\nproposals with iterative numerical refinement. On finite sets, we rediscover\nknown optima in small 2D cases and set new best-known 2D benchmarks for N >=\n40, while matching most known 3D optima up to the proven frontier (N <= 8) and\nreporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'\nparameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)\nmean-squared error for several 32-dimensional option-pricing tasks relative to\nwidely used Joe--Kuo parameters, while preserving extensibility to any sample\nsize and compatibility with standard randomizations. Taken together, the\nresults demonstrate that LLM-driven evolutionary program synthesis can automate\nthe discovery of high-quality QMC constructions, recovering classical designs\nwhere they are optimal and improving them where finite-N structure matters.\nData and code are available at\nhttps://github.com/hockeyguy123/openevolve-star-discrepancy.git."}
{"id": "2510.03659", "pdf": "https://arxiv.org/pdf/2510.03659", "abs": "https://arxiv.org/abs/2510.03659", "authors": ["Xu Wang", "Yan Hu", "Benyou Wang", "Difan Zou"], "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "24 pages", "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models\n(LLMs), based on the assumption that their interpretable features naturally\nenable effective model behavior steering. Yet, a fundamental question remains\nunanswered: does higher interpretability indeed imply better steering utility?\nTo answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,\nQwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,\nand evaluate their interpretability and steering utility based on SAEBench\n(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a\nrank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis\nreveals only a relatively weak positive association (tau b approx 0.298),\nindicating that interpretability is an insufficient proxy for steering\nperformance. We conjecture the interpretability utility gap may stem from the\nselection of SAE features, as not all of them are equally effective for\nsteering. To further find features that truly steer the behavior of LLMs, we\npropose a novel selection criterion called Delta Token Confidence, which\nmeasures how much amplifying a feature changes the next token distribution. We\nshow that our method improves the steering performance of three LLMs by 52.52\npercent compared to the current best output score based criterion\n(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token\nConfidence, the correlation between interpretability and utility vanishes (tau\nb approx 0), and can even become negative. This further highlights the\ndivergence between interpretability and utility for the most effective steering\nfeatures."}
{"id": "2510.03662", "pdf": "https://arxiv.org/pdf/2510.03662", "abs": "https://arxiv.org/abs/2510.03662", "authors": ["Jijie Zhou", "Niloofar Mireshghallah", "Tianshi Li"], "title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid deployment of large language models (LLMs) in consumer applications\nhas led to frequent exchanges of personal information. To obtain useful\nresponses, users often share more than necessary, increasing privacy risks via\nmemorization, context-based personalization, or security breaches. We present a\nframework to formally define and operationalize data minimization: for a given\nuser prompt and response model, quantifying the least privacy-revealing\ndisclosure that maintains utility, and we propose a priority-queue tree search\nto locate this optimal point within a privacy-ordered transformation space. We\nevaluated the framework on four datasets spanning open-ended conversations\n(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth\nanswers (CaseHold, MedQA), quantifying achievable data minimization with nine\nLLMs as the response model. Our results demonstrate that larger frontier LLMs\ncan tolerate stronger data minimization while maintaining task quality than\nsmaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for\nQwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that\nLLMs struggle to predict optimal data minimization directly, showing a bias\ntoward abstraction that leads to oversharing. This suggests not just a privacy\ngap, but a capability gap: models may lack awareness of what information they\nactually need to solve a task."}
{"id": "2510.03666", "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond."}
{"id": "2510.03687", "pdf": "https://arxiv.org/pdf/2510.03687", "abs": "https://arxiv.org/abs/2510.03687", "authors": ["Yue Huang", "Yanyuan Chen", "Dexuan Xu", "Weihua Yue", "Huamin Zhang", "Meikang Qiu", "Yu Huang"], "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical problem solving demands expert knowledge and intricate reasoning.\nRecent studies of large language models (LLMs) attempt to ease this complexity\nby introducing external knowledge verification through retrieval-augmented\ngeneration or by training on reasoning datasets. However, these approaches\nsuffer from drawbacks such as retrieval overhead and high annotation costs, and\nthey heavily rely on substituted external assistants to reach limited\nperformance in medical field. In this paper, we introduce MedReflect, a\ngeneralizable framework designed to inspire LLMs with a physician-like\nreflective thinking mode. MedReflect generates a single-pass reflection chain\nthat includes initial hypothesis generation, self-questioning, self-answering\nand decision refinement. This self-verified and self-reflective nature releases\nlarge language model's latent capability in medical problem-solving without\nexternal retrieval or heavy annotation. We demonstrate that MedReflect enables\ncost-efficient medical dataset construction: with merely 2,000 randomly sampled\ntraining examples and a light fine-tuning, this approach achieves notable\nabsolute accuracy improvements across a series of medical benchmarks while\ncutting annotation requirements. Our results provide evidence that LLMs can\nlearn to solve specialized medical problems via self-reflection and\nself-improve, reducing reliance on external supervision and extensive\ntask-specific fine-tuning data."}
{"id": "2510.03691", "pdf": "https://arxiv.org/pdf/2510.03691", "abs": "https://arxiv.org/abs/2510.03691", "authors": ["Zehua Liu", "Han Wu", "Xiaojin Fu", "Shuqi Liu", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "title": "REG: A Regularization Optimizer for Robust Training Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizers are crucial for the efficient training of Large Language Models\n(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers\nlike Muon have emerged, which regularize gradient updates by operating on\nentire weight matrices. The Muon optimizer balances the gradient updates along\nall the directions. However, Muon's reliance on the matrix sign function can\nlead to training instability, exhibits incompatibility when fine-tuning models\npre-trained with AdamW. To address these limitations, we propose \\textbf{REG},\na novel optimizer that replaces Muon's aggressive matrix sign operator with the\nRow-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a\nmatrix, the RACS operator regularizes the update steps in a less drastic\nmanner, making it simpler to implement and more compatible with established\ntraining dynamics. Through extensive empirical experiments on LLM training, we\ndemonstrate that our REG optimizer not only achieves superior performance and\nstability over AdamW, but also maintains consistency with the AdamW training\nparadigm. This consistency is particularly evident during the fine-tuning\nstage, where REG optimizer avoids the performance degradation observed with\nMuon."}
{"id": "2510.03699", "pdf": "https://arxiv.org/pdf/2510.03699", "abs": "https://arxiv.org/abs/2510.03699", "authors": ["Raaghav Malik", "Satpreet H. Singh", "Sonja Johnson-Yu", "Nathan Wu", "Roy Harpaz", "Florian Engert", "Kanaka Rajan"], "title": "Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE", "cs.SY", "eess.SY", "I.2.6; I.2.0; I.5.1"], "comment": null, "summary": "Larval zebrafish hunting provides a tractable setting to study how ecological\nand energetic constraints shape adaptive behavior in both biological brains and\nartificial agents. Here we develop a minimal agent-based model, training\nrecurrent policies with deep reinforcement learning in a bout-based zebrafish\nsimulator. Despite its simplicity, the model reproduces hallmark hunting\nbehaviors -- including eye vergence-linked pursuit, speed modulation, and\nstereotyped approach trajectories -- that closely match real larval zebrafish.\nQuantitative trajectory analyses show that pursuit bouts systematically reduce\nprey angle by roughly half before strike, consistent with measurements. Virtual\nexperiments and parameter sweeps vary ecological and energetic constraints,\nbout kinematics (coupled vs. uncoupled turns and forward motion), and\nenvironmental factors such as food density, food speed, and vergence limits.\nThese manipulations reveal how constraints and environments shape pursuit\ndynamics, strike success, and abort rates, yielding falsifiable predictions for\nneuroscience experiments. These sweeps identify a compact set of constraints --\nbinocular sensing, the coupling of forward speed and turning in bout\nkinematics, and modest energetic costs on locomotion and vergence -- that are\nsufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors\narise in minimal agents without detailed biomechanics, fluid dynamics, circuit\nrealism, or imitation learning from real zebrafish data. Taken together, this\nwork provides a normative account of zebrafish hunting as the optimal balance\nbetween energetic cost and sensory benefit, highlighting the trade-offs that\nstructure vergence and trajectory dynamics. We establish a virtual lab that\nnarrows the experimental search space and generates falsifiable predictions\nabout behavior and neural coding."}
{"id": "2510.03701", "pdf": "https://arxiv.org/pdf/2510.03701", "abs": "https://arxiv.org/abs/2510.03701", "authors": ["Kanoko Goto", "Takumi Hirose", "Mahiro Ukai", "Shuhei Kurita", "Nakamasa Inoue"], "title": "Referring Expression Comprehension for Small Objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page."}
{"id": "2510.03706", "pdf": "https://arxiv.org/pdf/2510.03706", "abs": "https://arxiv.org/abs/2510.03706", "authors": ["Eadom Dessalene", "Pavan Mantripragada", "Michael Maynord", "Yiannis Aloimonos"], "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Video link:\n  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing", "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption."}
{"id": "2510.03717", "pdf": "https://arxiv.org/pdf/2510.03717", "abs": "https://arxiv.org/abs/2510.03717", "authors": ["Sharan SK", "Subin Sahayam", "Umarani Jayaraman", "Lakshmi Priya A"], "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 6 figures, preprint under review", "summary": "Segmenting of clinically important retinal blood vessels into arteries and\nveins is a prerequisite for retinal vessel analysis. Such analysis can provide\npotential insights and bio-markers for identifying and diagnosing various\nretinal eye diseases. Alteration in the regularity and width of the retinal\nblood vessels can act as an indicator of the health of the vasculature system\nall over the body. It can help identify patients at high risk of developing\nvasculature diseases like stroke and myocardial infarction. Over the years,\nvarious Deep Learning architectures have been proposed to perform retinal\nvessel segmentation. Recently, attention mechanisms have been increasingly used\nin image segmentation tasks. The work proposes a new Deep Learning approach for\nartery-vein segmentation. The new approach is based on the Attention mechanism\nthat is incorporated into the WNet Deep Learning model, and we call the model\nas Attention-WNet. The proposed approach has been tested on publicly available\ndatasets such as HRF and DRIVE datasets. The proposed approach has outperformed\nother state-of-art models available in the literature."}
{"id": "2510.03734", "pdf": "https://arxiv.org/pdf/2510.03734", "abs": "https://arxiv.org/abs/2510.03734", "authors": ["Nirjhar Das", "Mohit Sharma", "Praharsh Nanavati", "Kirankumar Shiragur", "Amit Deshpande"], "title": "Cost Efficient Fairness Audit Under Partial Feedback", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted at NeurIPS 2025 RegML Workshop; Reliable ML Workshop", "summary": "We study the problem of auditing the fairness of a given classifier under\npartial feedback, where true labels are available only for positively\nclassified individuals, (e.g., loan repayment outcomes are observed only for\napproved applicants). We introduce a novel cost model for acquiring additional\nlabeled data, designed to more accurately reflect real-world costs such as\ncredit assessment, loan processing, and potential defaults. Our goal is to find\noptimal fairness audit algorithms that are more cost-effective than random\nexploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no\nassumptions on the data distribution, and a mixture model, where features and\ntrue labels follow a mixture of exponential family distributions. In the\nblack-box setting, we propose a near-optimal auditing algorithm under mild\nassumptions and show that a natural baseline can be strictly suboptimal. In the\nmixture model setting, we design a novel algorithm that achieves significantly\nlower audit cost than the black-box case. Our approach leverages prior work on\nlearning from truncated samples and maximum-a-posteriori oracles, and extends\nknown results on spherical Gaussian mixtures to handle exponential family\nmixtures, which may be of independent interest. Moreover, our algorithms apply\nto popular fairness metrics including demographic parity, equal opportunity,\nand equalized odds. Empirically, we demonstrate strong performance of our\nalgorithms on real-world fair classification datasets like Adult Income and Law\nSchool, consistently outperforming natural baselines by around 50% in terms of\naudit cost."}
{"id": "2510.03744", "pdf": "https://arxiv.org/pdf/2510.03744", "abs": "https://arxiv.org/abs/2510.03744", "authors": ["Qianfei Fan", "Jiayu Wei", "Peijun Zhu", "Wensheng Ye", "Meie Fang"], "title": "HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "physics.geo-ph"], "comment": "V1", "summary": "Accurate decade-scale daily runoff forecasting in small watersheds is\ndifficult because signals blend drifting trends, multi-scale seasonal cycles,\nregime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,\nPatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single\nfacets and under-utilize unlabeled spans, limiting regime adaptivity. We\npropose HydroFusion-LMF, a unified framework that (i) performs a learnable\ntrend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes\nresiduals through a compact heterogeneous expert set (linear refinement,\nfrequency kernel, patch Transformer, recurrent memory, dynamically normalized\nattention), (iii) fuses expert outputs via a hydrologic context-aware gate\nconditioned on day-of-year phase, antecedent precipitation, local variance,\nflood indicators, and static basin attributes, and (iv) augments supervision\nwith a semi-supervised multi-task objective (composite MSE/MAE + extreme\nemphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,\naugmentation consistency, variance-filtered pseudo-labeling). Optional adapter\n/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a\n~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,\nimproving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean\nbaseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions\nrelative to baselines. The framework balances interpretability (explicit\ncomponents, sparse gating) with performance, advancing label-efficient\nhydrologic forecasting under non-stationarity."}
{"id": "2510.03748", "pdf": "https://arxiv.org/pdf/2510.03748", "abs": "https://arxiv.org/abs/2510.03748", "authors": ["Ramtin Kakavand", "Ebrahim Ansari"], "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have consistently demonstrated strong\nperformance in machine translation, especially when guided by high-quality\nprompts. Few-shot prompting is an effective technique to improve translation\nquality; however, most existing example selection methods focus solely on\nquery-to-example similarity and do not account for the quality of the examples.\nIn this work, we propose TreePrompt, a novel example selection approach that\nlearns LLM preferences to identify high-quality, contextually relevant examples\nwithin a tree-structured framework. To further explore the balance between\nsimilarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)\nand Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -\nEnglish-Persian (MIZAN) and English-German (WMT19) - show that integrating\nTreePrompt with AFSP or Random selection leads to improved translation\nperformance."}
{"id": "2510.03755", "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "categories": ["cs.SE", "cs.AI"], "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me."}
{"id": "2510.03760", "pdf": "https://arxiv.org/pdf/2510.03760", "abs": "https://arxiv.org/abs/2510.03760", "authors": ["Ping Guo", "Chenyu Zhu", "Siyuan Chen", "Fei Liu", "Xi Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review of ICLR 2026", "summary": "CUDA kernel optimization has become a critical bottleneck for AI performance,\nas deep learning training and inference efficiency directly depends on highly\noptimized GPU kernels.\n  Despite the promise of Large Language Models (LLMs) for automating kernel\noptimization, this field suffers from a fragmented ecosystem of isolated and\nincomparable approaches with unclear problem formulations.\n  Furthermore, general-purpose LLM code evolution methods cannot meet strict\ncorrectness requirements of CUDA kernel optimization.\n  We address these fundamental challenges by first formalizing CUDA kernel\noptimization as a code optimization task with a clear objective, constraints,\nand evaluation metrics.\n  We then establish the first systematic LLM-based code evolution framework,\nEvoEngineer, that provides guidance for designing and adapting optimization\nstrategies to achieve a balance between performance and correctness.\n  Finally, we implement a kernel optimization system based on this framework\nand conduct extensive experiments on 91 real-world CUDA kernels.\n  Our results demonstrate that EvoEngineer achieves a principled balance\nbetween performance and correctness, with the highest averaged median speedup\nof \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of\n\\textbf{69.8}\\%, outperforming existing methods on both dimensions.\n  Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all\noperations over PyTorch kernels and delivers the highest speedup on \\textbf{28}\n(\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$}\nacceleration."}
{"id": "2510.03761", "pdf": "https://arxiv.org/pdf/2510.03761", "abs": "https://arxiv.org/abs/2510.03761", "authors": ["Richard A. Dubniczky", "Bertalan Borsos", "Tihanyi Norbert"], "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The widespread use of preprint repositories such as arXiv has accelerated the\ncommunication of scientific results but also introduced overlooked security\nrisks. Beyond PDFs, these platforms provide unrestricted access to original\nsource materials, including LaTeX sources, auxiliary code, figures, and\nembedded comments. In the absence of sanitization, submissions may disclose\nsensitive information that adversaries can harvest using open-source\nintelligence. In this work, we present the first large-scale security audit of\npreprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv\nsubmissions. We introduce LaTeXpOsEd, a four-stage framework that integrates\npattern matching, logical filtering, traditional harvesting techniques, and\nlarge language models (LLMs) to uncover hidden disclosures within\nnon-referenced files and LaTeX comments. To evaluate LLMs' secret-detection\ncapabilities, we introduce LLMSec-DB, a benchmark on which we tested 25\nstate-of-the-art models. Our analysis uncovered thousands of PII leaks,\nGPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,\neditable private SharePoint links, exposed GitHub and Google credentials, and\ncloud API keys. We also uncovered confidential author communications, internal\ndisagreements, and conference submission credentials, exposing information that\nposes serious reputational risks to both researchers and institutions. We urge\nthe research community and repository operators to take immediate action to\nclose these hidden security gaps. To support open science, we release all\nscripts and methods from this study but withhold sensitive findings that could\nbe misused, in line with ethical principles. The source code and related\nmaterial are available at the project website https://github.com/LaTeXpOsEd"}
{"id": "2510.03763", "pdf": "https://arxiv.org/pdf/2510.03763", "abs": "https://arxiv.org/abs/2510.03763", "authors": ["Jiaxin Deng", "Junbiao Pang"], "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles\nthe computational cost of Stochastic Gradient Descent (SGD) by requiring twice\nthe gradient calculations per optimization step. To mitigate this, we propose\nAdaptively sampling-Reusing-mixing decomposed gradients to significantly\naccelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can\nbe decomposed into the SGD gradient and the Projection of the Second-order\ngradient onto the First-order gradient (PSF). Furthermore, we observe that the\nSGD gradient and PSF dynamically evolve during training, emphasizing the\ngrowing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed\nto the reused PSF and the timely updated PSF still maintain the model's\ngeneralization ability. Extensive experiments show that ARSAM achieves\nstate-of-the-art accuracies comparable to SAM across diverse network\narchitectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a\nspeedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various\nchallenge tasks (\\textit{e.g.}, human pose estimation, and model quantization)\nwithout sacrificing performance, demonstrating its broad practicality.% The\ncode is publicly accessible at: https://github.com/ajiaaa/ARSAM."}
{"id": "2510.03781", "pdf": "https://arxiv.org/pdf/2510.03781", "abs": "https://arxiv.org/abs/2510.03781", "authors": ["Majid Asgari-Bidhendi", "Muhammad Amin Ghaseminia", "Alireza Shahbazi", "Sayyed Ali Hossayni", "Najmeh Torabian", "Behrouz Minaei-Bidgoli"], "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "This paper presents the development of Rezwan, a large-scale AI-assisted\nHadith corpus comprising over 1.2M narrations, extracted and structured through\na fully automated pipeline. Building on digital repositories such as Maktabat\nAhl al-Bayt, the pipeline employs Large Language Models (LLMs) for\nsegmentation, chain--text separation, validation, and multi-layer enrichment.\nEach narration is enhanced with machine translation into twelve languages,\nintelligent diacritization, abstractive summarization, thematic tagging, and\ncross-text semantic analysis. This multi-step process transforms raw text into\na richly annotated research-ready infrastructure for digital humanities and\nIslamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled\nnarrations, assessed by six domain experts. Results show near-human accuracy in\nstructured tasks such as chain--text separation (9.33/10) and summarization\n(9.33/10), while highlighting ongoing challenges in diacritization and semantic\nsimilarity detection. Comparative analysis against the manually curated Noor\nCorpus demonstrates the superiority of Najm in both scale and quality, with a\nmean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis\nconfirms the economic feasibility of the AI approach: tasks requiring over\n229,000 hours of expert labor were completed within months at a fraction of the\ncost. The work introduces a new paradigm in religious text processing by\nshowing how AI can augment human expertise, enabling large-scale, multilingual,\nand semantically enriched access to Islamic heritage."}
{"id": "2510.03788", "pdf": "https://arxiv.org/pdf/2510.03788", "abs": "https://arxiv.org/abs/2510.03788", "authors": ["Abukar Ali"], "title": "Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Following the success of Transformer architectures in language modeling,\nparticularly their ability to capture long-range dependencies, researchers have\nexplored how these architectures can be adapted for time-series forecasting.\nTransformer-based models have been proposed to handle both short- and long-term\ndependencies when predicting future values from historical data. However,\nstudies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have\nreported mixed results in long-term forecasting tasks. In this work, we\nevaluate the Gaussian-based Linear architecture introduced by Rizvi et al.\n(2025) and present an enhanced version called the Residual Stacked Gaussian\nLinear (RSGL) model. We also investigate the broader applicability of the RSGL\nmodel in additional domains, including financial time series and\nepidemiological data. Experimental results show that the RSGL model achieves\nimproved prediction accuracy and robustness compared to both the baseline\nGaussian Linear and Transformer-based models."}
{"id": "2510.03799", "pdf": "https://arxiv.org/pdf/2510.03799", "abs": "https://arxiv.org/abs/2510.03799", "authors": ["Hadi Asghari", "Sami Nenno"], "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Peer-reviewed and presented at Advances in Interpretable Machine\n  Learning and Artificial Intelligence (AIMLAI) Workshop at ECML/PKDD 2024", "summary": "This paper explores the ability of large language models to generate and\nrecognize deep cognitive frames, particularly in socio-political contexts. We\ndemonstrate that LLMs are highly fluent in generating texts that evoke specific\nframes and can recognize these frames in zero-shot settings. Inspired by\nmechanistic interpretability research, we investigate the location of the\n`strict father' and `nurturing parent' frames within the model's hidden\nrepresentation, identifying singular dimensions that correlate strongly with\ntheir presence. Our findings contribute to understanding how LLMs capture and\nexpress meaningful human concepts."}
{"id": "2510.03805", "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}."}
{"id": "2510.03807", "pdf": "https://arxiv.org/pdf/2510.03807", "abs": "https://arxiv.org/abs/2510.03807", "authors": ["Vaskar Chakma", "Wooyeol Choi"], "title": "6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)\ntechnology face critical limitations in achieving real-time performance for\nmission-critical industrial applications. Existing 5G-enabled systems suffer\nfrom latencies exceeding 10ms, which are inadequate for applications requiring\nsub-millisecond response times, such as autonomous industrial control and\npredictive maintenance. This research aims to develop and validate a 6G-enabled\nDigital Twin framework that achieves ultra-low latency communication and\nreal-time synchronization between physical industrial assets and their digital\ncounterparts, specifically targeting bearing fault detection as a critical\nindustrial use case. The proposed framework integrates terahertz communications\n(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence\nwithin a five-layer architecture. Experimental validation was conducted using\nthe Case Western Reserve University (CWRU) bearing dataset, implementing\ncomprehensive feature extraction (15 time and frequency domain features) and\nRandom Forest classification algorithms. The system performance was evaluated\nagainst traditional WiFi-6 and 5G networks across multiple metrics, including\nclassification accuracy, end-to-end latency, and scalability. It achieved 97.7%\nfault classification accuracy with 0.8ms end-to-end latency, representing a\n15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)\nnetworks. The system demonstrated superior scalability with sub-linear\nprocessing time growth and maintained consistent performance across four\nbearing fault categories (normal, inner race, outer race, and ball faults) with\nmacro-averaged F1-scores exceeding 97%."}
{"id": "2510.03812", "pdf": "https://arxiv.org/pdf/2510.03812", "abs": "https://arxiv.org/abs/2510.03812", "authors": ["Changhong Li", "Clément Bled", "Rosa Fernandez", "Shreejith Shanker"], "title": "ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs", "categories": ["eess.IV", "cs.AI"], "comment": "This paper has been accepted by the 22nd ACM SIGGRAPH European\n  Conference on Visual Media Production (CVMP 2025)", "summary": "Denoising is a core operation in modern video pipelines. In codecs, in-loop\nfilters suppress sensor noise and quantisation artefacts to improve\nrate-distortion performance; in cinema post-production, denoisers are used for\nrestoration, grain management, and plate clean-up. However, state-of-the-art\ndeep denoisers are computationally intensive and, at scale, are typically\ndeployed on GPUs, incurring high power and cost for real-time, high-resolution\nstreams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated\ndenoising system that serves inference on data-centre Field Programmable Gate\nArrays (FPGAs). A compact convolutional model is quantised (post-training\nquantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD\nDeep Learning Processor Unit (DPU)-based FPGAs. A client-server integration\noffloads computation from the host CPU/GPU to a networked FPGA service, while\nremaining callable from existing workflows, e.g., NUKE, without disrupting\nartist tooling. On representative benchmarks, ReTiDe delivers 37.71$\\times$\nGiga Operations Per Second (GOPS) throughput and 5.29$\\times$ higher energy\nefficiency than prior FPGA denoising accelerators, with negligible degradation\nin Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These\nresults indicate that specialised accelerators can provide practical, scalable\ndenoising for both encoding pipelines and post-production, reducing energy per\nframe without sacrificing quality or workflow compatibility. Code is available\nat https://github.com/RCSL-TCD/ReTiDe."}
{"id": "2510.03813", "pdf": "https://arxiv.org/pdf/2510.03813", "abs": "https://arxiv.org/abs/2510.03813", "authors": ["Byungjun Kim", "Soobin Um", "Jong Chul Ye"], "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices."}
{"id": "2510.03814", "pdf": "https://arxiv.org/pdf/2510.03814", "abs": "https://arxiv.org/abs/2510.03814", "authors": ["Lukas Eisenmann", "Alena Brändle", "Zahra Monfared", "Daniel Durstewitz"], "title": "Detecting Invariant Manifolds in ReLU-Based RNNs", "categories": ["cs.LG", "cs.AI", "math.DS"], "comment": null, "summary": "Recurrent Neural Networks (RNNs) have found widespread applications in\nmachine learning for time series prediction and dynamical systems\nreconstruction, and experienced a recent renaissance with improved training\nalgorithms and architectural designs. Understanding why and how trained RNNs\nproduce their behavior is important for scientific and medical applications,\nand explainable AI more generally. An RNN's dynamical repertoire depends on the\ntopological and geometrical properties of its state space. Stable and unstable\nmanifolds of periodic points play a particularly important role: They dissect a\ndynamical system's state space into different basins of attraction, and their\nintersections lead to chaotic dynamics with fractal geometry. Here we introduce\na novel algorithm for detecting these manifolds, with a focus on\npiecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as\ntheir activation function. We demonstrate how the algorithm can be used to\ntrace the boundaries between different basins of attraction, and hence to\ncharacterize multistability, a computationally important property. We further\nshow its utility in finding so-called homoclinic points, the intersections\nbetween stable and unstable manifolds, and thus establish the existence of\nchaos in PLRNNs. Finally we show for an empirical example, electrophysiological\nrecordings from a cortical neuron, how insights into the underlying dynamics\ncould be gained through our method."}
{"id": "2510.03824", "pdf": "https://arxiv.org/pdf/2510.03824", "abs": "https://arxiv.org/abs/2510.03824", "authors": ["Wei Guo", "Jaemoo Choi", "Yuchen Zhu", "Molei Tao", "Yongxin Chen"], "title": "Proximal Diffusion Neural Sampler", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "31 pages, 12 figures", "summary": "The task of learning a diffusion-based neural sampler for drawing samples\nfrom an unnormalized target distribution can be viewed as a stochastic optimal\ncontrol problem on path measures. However, the training of neural samplers can\nbe challenging when the target distribution is multimodal with significant\nbarriers separating the modes, potentially leading to mode collapse. We propose\na framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that\naddresses these challenges by tackling the stochastic optimal control problem\nvia proximal point method on the space of path measures. PDNS decomposes the\nlearning process into a series of simpler subproblems that create a path\ngradually approaching the desired distribution. This staged procedure traces a\nprogressively refined path to the desired distribution and promotes thorough\nexploration across modes. For a practical and efficient realization, we\ninstantiate each proximal step with a proximal weighted denoising cross-entropy\n(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS\nthrough extensive experiments on both continuous and discrete sampling tasks,\nincluding challenging scenarios in molecular dynamics and statistical physics."}
{"id": "2510.03829", "pdf": "https://arxiv.org/pdf/2510.03829", "abs": "https://arxiv.org/abs/2510.03829", "authors": ["André Coelho", "Pedro Ribeiro", "Helder Fontes", "Rui Campos"], "title": "A4FN: an Agentic AI Architecture for Autonomous Flying Networks", "categories": ["cs.NI", "cs.AI"], "comment": "This paper has been accepted for presentation in the Auto ML for\n  Zero-Touch Network Management Workshop (WS04-01) at the IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "This position paper presents A4FN, an Agentic Artificial Intelligence (AI)\narchitecture for intent-driven automation in Flying Networks (FNs) using\nUnmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI\nand Large Language Models (LLMs) to enable real-time, context-aware network\ncontrol via a distributed agentic system. It comprises two components: the\nPerception Agent (PA), which semantically interprets multimodal input --\nincluding imagery, audio, and telemetry data -- from UAV-mounted sensors to\nderive Service Level Specifications (SLSs); and the Decision-and-Action Agent\n(DAA), which reconfigures the network based on inferred intents. A4FN embodies\nkey properties of Agentic AI, including autonomy, goal-driven reasoning, and\ncontinuous perception-action cycles. Designed for mission-critical,\ninfrastructure-limited scenarios such as disaster response, it supports\nadaptive reconfiguration, dynamic resource management, and interoperability\nwith emerging wireless technologies. The paper details the A4FN architecture,\nits core innovations, and open research challenges in multi-agent coordination\nand Agentic AI integration in next-generation FNs."}
{"id": "2510.03856", "pdf": "https://arxiv.org/pdf/2510.03856", "abs": "https://arxiv.org/abs/2510.03856", "authors": ["Sanhita Basu", "Tomas Fröding", "Ali Teymur Kahraman", "Dimitris Toumpanakis", "Tobias Sjöblom"], "title": "AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Background: Pleural Effusions (PE) is a common finding in many different\nclinical conditions, but accurately measuring their volume from CT scans is\nchallenging. Purpose: To improve PE segmentation and quantification for\nenhanced clinical management, we have developed and trained a semi-supervised\ndeep learning framework on contrast-enhanced CT volumes. Materials and Methods:\nThis retrospective study collected CT Pulmonary Angiogram (CTPA) data from\ninternal and external datasets. A subset of 100 cases was manually annotated\nfor model training, while the remaining cases were used for testing and\nvalidation. A novel semi-supervised deep learning framework, Teacher-Teaching\nAssistant-Student (TTAS), was developed and used to enable efficient training\nin non-segmented examinations. Segmentation performance was compared to that of\nstate-of-the-art models. Results: 100 patients (mean age, 72 years, 28\n[standard deviation]; 55 men) were included in the study. The TTAS model\ndemonstrated superior segmentation performance compared to state-of-the-art\nmodels, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73\nfor nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a\nfour-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80\n- 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The\ndeveloped TTAS framework offered superior PE segmentation, aiding accurate\nvolume determination from CT scans."}
{"id": "2510.03862", "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "categories": ["cs.SE", "cs.AI", "500"], "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts."}
{"id": "2510.03865", "pdf": "https://arxiv.org/pdf/2510.03865", "abs": "https://arxiv.org/abs/2510.03865", "authors": ["Wenhao Deng", "Long Wei", "Chenglei Yu", "Tailin Wu"], "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced\nthe reasoning capabilities of large language models (LLMs), particularly for\nmathematical problem solving. However, a fundamental limitation remains: as the\nsampling budget increases, the advantage of RLVR-trained models over their\npretrained bases often diminishes or even vanishes, revealing a strong\ndependence on the base model's restricted search space. We attribute this\nphenomenon to the widespread use of the reverse Kullback-Leibler (KL)\ndivergence regularizer, whose mode-seeking behavior keeps the policy trapped\ninside the base model's support region and hampers wider exploration. To\naddress this issue, we propose RAPO (Rewards-Aware Policy Optimization), an\nalgorithm to promote broader yet focused exploration. Our method (i) utilizes\nthe forward KL penalty to replace the reverse KL penalty for\nout-of-distribution exploration, and (ii) reweights the reference policy to\nfacilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B\nmodels with RAPO on the 8K SimpleRL-Zero dataset, without supervised\nfine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO\nconsistently improves problem-solving performance. Notably, RAPO enables models\nto surpass the base model's performance ceiling and solves previously\nintractable problems, advancing the frontier of RLVR for challenging reasoning\ntasks."}
{"id": "2510.03868", "pdf": "https://arxiv.org/pdf/2510.03868", "abs": "https://arxiv.org/abs/2510.03868", "authors": ["Dalia Ali", "Muneeb Ahmed", "Hailan Wang", "Arfa Khan", "Naira Paola Arnez Jordan", "Sunnie S. Y. Kim", "Meet Dilip Muchhala", "Anne Kathrin Merkle", "Orestis Papakyriakopoulos"], "title": "AI Adoption Across Mission-Driven Organizations", "categories": ["cs.CY", "cs.AI"], "comment": "16 pages, Submitted for CHI 2026", "summary": "Despite AI's promise for addressing global challenges, empirical\nunderstanding of AI adoption in mission-driven organizations (MDOs) remains\nlimited. While research emphasizes individual applications or ethical\nprinciples, little is known about how resource-constrained, values-driven\norganizations navigate AI integration across operations. We conducted thematic\nanalysis of semi-structured interviews with 15 practitioners from\nenvironmental, humanitarian, and development organizations across the Global\nNorth and South contexts. Our analysis examines how MDOs currently deploy AI,\nwhat barriers constrain adoption, and how practitioners envision future\nintegration. MDOs adopt AI selectively, with sophisticated deployment in\ncontent creation and data analysis while maintaining human oversight for\nmission-critical applications. When AI's efficiency benefits conflict with\norganizational values, decision-making stalls rather than negotiating\ntrade-offs. This study contributes empirical evidence that AI adoption in MDOs\nshould be understood as conditional rather than inevitable, proceeding only\nwhere it strengthens organizational sovereignty and mission integrity while\npreserving human-centered approaches essential to their missions."}
{"id": "2510.03871", "pdf": "https://arxiv.org/pdf/2510.03871", "abs": "https://arxiv.org/abs/2510.03871", "authors": ["Oleg Filatov", "Jiangtao Wang", "Jan Ebert", "Stefan Kesselheim"], "title": "Optimal Scaling Needs Optimal Norm", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Despite recent progress in optimal hyperparameter transfer under model and\ndataset scaling, no unifying explanatory principle has been established. Using\nthe Scion optimizer, we discover that joint optimal scaling across model and\ndataset sizes is governed by a single invariant: the operator norm of the\noutput layer. Across models with up to 1.3B parameters trained on up to 138B\ntokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$\nconsistently has the same operator norm value - a phenomenon we term norm\ntransfer. This constant norm condition is necessary but not sufficient: while\nfor each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a\nunique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient\ncondition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$\nscaling with dataset size for Scion, and find that the scaling rules are\nconsistent with those of the Adam optimizer. Tuning per-layer-group learning\nrates also improves model performance, with the output layer being the most\nsensitive and hidden layers benefiting from lower learning rates. We provide\npractical insights on norm-guided optimal scaling and release our Distributed\nScion (Disco) implementation with logs from over two thousand runs to support\nresearch on LLM training dynamics at scale."}
{"id": "2510.03873", "pdf": "https://arxiv.org/pdf/2510.03873", "abs": "https://arxiv.org/abs/2510.03873", "authors": ["Saja Al-Dabet", "Sherzod Turaev", "Nazar Zaki", "Arif O. Khan", "Luai Eldweik"], "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "This is a preprint version of a manuscript under review. All rights\n  reserved by the authors", "summary": "Diagnosing ocular-induced abnormal head posture (AHP) requires a\ncomprehensive analysis of both head pose and ocular movements. However,\nexisting datasets focus on these aspects separately, limiting the development\nof integrated diagnostic approaches and restricting AI-driven advancements in\nAHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D\ndataset that synchronously captures head pose and gaze movement information for\nocular-induced AHP assessment. Structured clinical data were extracted from\nmedical literature using large language models (LLMs) through an iterative\nprocess with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and\ncomplex prompting strategies. The extracted records were systematically imputed\nand transformed into 3D representations using the Neural Head Avatar (NHA)\nframework. The dataset includes 7,920 images generated from two head textures,\ncovering a broad spectrum of ocular conditions. The extraction method achieved\nan overall accuracy of 91.92%, demonstrating its reliability for clinical\ndataset construction. PoseGaze-AHP is the first publicly available resource\ntailored for AI-driven ocular-induced AHP diagnosis, supporting the development\nof accurate and privacy-compliant diagnostic tools."}
{"id": "2510.03878", "pdf": "https://arxiv.org/pdf/2510.03878", "abs": "https://arxiv.org/abs/2510.03878", "authors": ["Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R"], "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes\nsignificantly to its high global mortality rate, with over 50\\% of cases\ndetected at advanced stages and a 5-year survival rate below 50\\% according to\nWHO statistics. This study aims to improve early detection of OSCC by\ndeveloping a multimodal deep learning framework that integrates clinical,\nradiological, and histopathological images using a weighted ensemble of\nDenseNet-121 convolutional neural networks (CNNs). Material and Methods A\nretrospective study was conducted using publicly available datasets\nrepresenting three distinct medical imaging modalities. Each modality-specific\ndataset was used to train a DenseNet-121 CNN via transfer learning.\nAugmentation and modality-specific preprocessing were applied to increase\nrobustness. Predictions were fused using a validation-weighted ensemble\nstrategy. Evaluation was performed using accuracy, precision, recall, F1-score.\nResults High validation accuracy was achieved for radiological (100\\%) and\nhistopathological (95.12\\%) modalities, with clinical images performing lower\n(63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved\ndiagnostic robustness with an overall accuracy of 84.58\\% on a multimodal\nvalidation dataset of 55 samples. Conclusion The multimodal ensemble framework\nbridges gaps in the current diagnostic workflow by offering a non-invasive,\nAI-assisted triage tool that enhances early identification of high-risk\nlesions. It supports clinicians in decision-making, aligning with global\noncology guidelines to reduce diagnostic delays and improve patient outcomes."}
{"id": "2510.03879", "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."}
{"id": "2510.03914", "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "categories": ["cs.SE", "cs.AI"], "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality."}
{"id": "2510.03921", "pdf": "https://arxiv.org/pdf/2510.03921", "abs": "https://arxiv.org/abs/2510.03921", "authors": ["Arushi Dashore", "Aryan Anumala", "Emily Hui", "Olivia Yang"], "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.10; I.5.4; I.2.7"], "comment": "10 pages, 4 figures, 2 tables", "summary": "Automated tennis stroke analysis has advanced significantly with the\nintegration of biomechanical motion cues alongside deep learning techniques,\nenhancing stroke classification accuracy and player performance evaluation.\nDespite these advancements, existing systems often fail to connect\nbiomechanical insights with actionable language feedback that is both\naccessible and meaningful to players and coaches. This research project\naddresses this gap by developing a novel framework that extracts key\nbiomechanical features (such as joint angles, limb velocities, and kinetic\nchain patterns) from motion data using Convolutional Neural Network Long\nShort-Term Memory (CNN-LSTM)-based models. These features are analyzed for\nrelationships influencing stroke effectiveness and injury risk, forming the\nbasis for feedback generation using large language models (LLMs). Leveraging\nthe THETIS dataset and feature extraction techniques, our approach aims to\nproduce feedback that is technically accurate, biomechanically grounded, and\nactionable for end-users. The experimental setup evaluates this framework on\nclassification performance and interpretability, bridging the gap between\nexplainable AI and sports biomechanics."}
{"id": "2510.03923", "pdf": "https://arxiv.org/pdf/2510.03923", "abs": "https://arxiv.org/abs/2510.03923", "authors": ["Mingsong Yan", "Charles Kulick", "Sui Tang"], "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continuous-depth graph neural networks, also known as Graph Neural\nDifferential Equations (GNDEs), combine the structural inductive bias of Graph\nNeural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,\noffering a scalable and principled framework for modeling dynamics on graphs.\nIn this paper, we present a rigorous convergence analysis of GNDEs with\ntime-varying parameters in the infinite-node limit, providing theoretical\ninsights into their size transferability. To this end, we introduce Graphon\nNeural Differential Equations (Graphon-NDEs) as the infinite-node limit of\nGNDEs and establish their well-posedness. Leveraging tools from graphon theory\nand dynamical systems, we prove the trajectory-wise convergence of GNDE\nsolutions to Graphon-NDE solutions. Moreover, we derive explicit convergence\nrates under two deterministic graph sampling regimes: (1) weighted graphs\nsampled from smooth graphons, and (2) unweighted graphs sampled from\n$\\{0,1\\}$-valued (discontinuous) graphons. We further establish size\ntransferability bounds, providing theoretical justification for the practical\nstrategy of transferring GNDE models trained on moderate-sized graphs to\nlarger, structurally similar graphs without retraining. Numerical experiments\nusing synthetic and real data support our theoretical findings."}
{"id": "2510.03930", "pdf": "https://arxiv.org/pdf/2510.03930", "abs": "https://arxiv.org/abs/2510.03930", "authors": ["Huascar Sanchez", "Briland Hitaj"], "title": "LLM Chemistry Estimation for Multi-LLM Recommendation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "20 pages, 5 figures, 5 tables", "summary": "Multi-LLM collaboration promises accurate, robust, and context-aware\nsolutions, yet existing approaches rely on implicit selection and output\nassessment without analyzing whether collaborating models truly complement or\nconflict. We introduce LLM Chemistry -- a framework that measures when LLM\ncombinations exhibit synergistic or antagonistic behaviors that shape\ncollective performance beyond individual capabilities. We formalize the notion\nof chemistry among LLMs, propose algorithms that quantify it by analyzing\ninteraction dependencies, and recommend optimal model ensembles accordingly.\nOur theoretical analysis shows that chemistry among collaborating LLMs is most\nevident under heterogeneous model profiles, with its outcome impact shaped by\ntask type, group size, and complexity. Evaluation on classification,\nsummarization, and program repair tasks provides initial evidence for these\ntask-dependent effects, thereby reinforcing our theoretical results. This\nestablishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and\na foundation for ensemble recommendation."}
{"id": "2510.03952", "pdf": "https://arxiv.org/pdf/2510.03952", "abs": "https://arxiv.org/abs/2510.03952", "authors": ["Raven Beutner", "Bernd Finkbeiner"], "title": "Strategy Logic, Imperfect Information, and Hyperproperties", "categories": ["cs.LO", "cs.AI", "cs.MA"], "comment": "KR 2025", "summary": "Strategy logic (SL) is a powerful temporal logic that enables first-class\nreasoning over strategic behavior in multi-agent systems (MAS). In many MASs,\nthe agents (and their strategies) cannot observe the global state of the\nsystem, leading to many extensions of SL centered around imperfect information,\nsuch as strategy logic with imperfect information (SL$_\\mathit{ii}$). Along\northogonal lines, researchers have studied the combination of strategic\nbehavior and hyperproperties. Hyperproperties are system properties that relate\nmultiple executions in a system and commonly arise when specifying security\npolicies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines\nquantification over strategies with the ability to express hyperproperties on\nthe executions of different strategy profiles. In this paper, we study the\nrelation between SL$_\\mathit{ii}$ and HyperSL. Our main result is that both\nlogics (restricted to formulas where no state formulas are nested within path\nformulas) are equivalent in the sense that we can encode SL$_\\mathit{ii}$\ninstances into HyperSL instances and vice versa. For the former direction, we\nbuild on the well-known observation that imperfect information is a\nhyperproperty. For the latter direction, we construct a self-composition of\nMASs and show how we can simulate hyperproperties using imperfect information."}
{"id": "2510.03962", "pdf": "https://arxiv.org/pdf/2510.03962", "abs": "https://arxiv.org/abs/2510.03962", "authors": ["Hanzhe Wei", "Jiajun Wu", "Jialin Yang", "Henry Leung", "Steve Drew"], "title": "SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Time series anomaly detection plays a crucial role in a wide range of fields,\nsuch as healthcare and internet traffic monitoring. The emergence of large\nlanguage models (LLMs) offers new opportunities for detecting anomalies in the\nubiquitous time series data. Traditional approaches struggle with\nvariable-length time series sequences and context-based anomalies. We propose\nSoft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage\nLLMs for anomaly detection with soft prompts and quantization. Our methodology\ninvolves quantizing and transforming the time series data into input embeddings\nand combining them with learnable soft prompt embeddings. These combined\nembeddings are then fed into a frozen LLM. The soft prompts are updated\niteratively based on a cross-entropy loss, allowing the model to adapt to time\nseries anomaly detection. The use of soft prompts helps adapt LLMs effectively\nto time series tasks, while quantization ensures optimal handling of sequences,\nas LLMs are designed to handle discrete sequences. Our experimental results\ndemonstrate that soft prompts effectively increase LLMs' performance in\ndownstream tasks regarding time series anomaly detection."}
{"id": "2510.03970", "pdf": "https://arxiv.org/pdf/2510.03970", "abs": "https://arxiv.org/abs/2510.03970", "authors": ["Zainab Saad", "Jialin Yang", "Henry Leung", "Steve Drew"], "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted to 2025 IEEE Smart World Congress (SWC 2025)", "summary": "The growing reliance on large-scale data centers to run resource-intensive\nworkloads has significantly increased the global carbon footprint, underscoring\nthe need for sustainable computing solutions. While container orchestration\nplatforms like Kubernetes help optimize workload scheduling to reduce carbon\nemissions, existing methods often depend on centralized machine learning models\nthat raise privacy concerns and struggle to generalize across diverse\nenvironments. In this paper, we propose a federated learning approach for\nenergy consumption prediction that preserves data privacy by keeping sensitive\noperational data within individual enterprises. By extending the Kubernetes\nEfficient Power Level Exporter (Kepler), our framework trains XGBoost models\ncollaboratively across distributed clients using Flower's FedXgbBagging\naggregation using a bagging strategy, eliminating the need for centralized data\nsharing. Experimental results on the SPECPower benchmark dataset show that our\nFL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a\ncentralized baseline. This work addresses the unresolved trade-off between data\nprivacy and energy prediction efficiency in prior systems such as Kepler and\nCASPER and offers enterprises a viable pathway toward sustainable cloud\ncomputing without compromising operational privacy."}
{"id": "2510.03971", "pdf": "https://arxiv.org/pdf/2510.03971", "abs": "https://arxiv.org/abs/2510.03971", "authors": ["Jatin Prakash", "Anirudh Buvanesh"], "title": "What Can You Do When You Have Zero Rewards During RL?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective\nfor improving large language models (LLMs) on complex reasoning tasks. However,\nits success often depends on the base model occasionally sampling correct\nsolutions. When no correct solutions are sampled, training encounters a\nzero-reward barrier where learning stalls due to zero gradients. We study this\nscenario through the graph search task introduced in Bachmann et al. (2024) and\nevaluate recent methods that incorporate desirable components such as dense\nrewards, diversity incentives, and improved credit assignment. Our experiments\nshow that none of these approaches overcome the zero-reward barrier if the base\nmodel never produces a correct answer. In contrast, we find that a simple\ndata-centric intervention of adding easier samples to the training set enables\nthe model to eventually solve the original hard task despite starting from zero\nreward. Importantly, this succeeds without modifying the RL algorithm itself.\nBecause official implementations of several baselines were unavailable, we\ndeveloped our own, which allowed us to conduct a detailed analysis of their\nfailure modes. We release these implementations to support further research at:\nhttps://github.com/rl4reasoning/rl-baselines"}
{"id": "2510.03988", "pdf": "https://arxiv.org/pdf/2510.03988", "abs": "https://arxiv.org/abs/2510.03988", "authors": ["Hoang Anh Just", "Myeongseob Ko", "Ruoxi Jia"], "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models\ninto smaller student LLMs via SFT has emerged as a standard paradigm. This\napproach is practical and efficient: it leverages the ease of generating\nabundant reasoning data from stronger models and provides a direct, data-driven\nway to teach less capable models better reasoning. While previous work has\nlargely focused on prompt selection with responses from a single teacher, the\nequally important problem of choosing the best response when multiple teacher\noutputs are available for a single prompt remains underexplored. This challenge\nbecomes important in a multi-teacher setting, where different students may\nbenefit from the outputs of different teachers. This paper fills that gap with\na systematic study of response selection for reasoning distillation. We first\nshow that the current method, which picks responses the student assigns the\nhighest global log-probability (global naturalness), fails when responses come\nfrom multiple teachers, i.e., global naturalness no longer correlates with\ndownstream performance, especially as the reasoning traces from strong teachers\nbecome longer. To overcome this problem, we introduce Local Naturalness, which\nmeasures the student's log-probabilities over short, sequential reasoning steps\nconditioned only on a small local window. Local Naturalness enables two\napplications: 1) Teacher Selection: Aggregating local scores across prompts\nreliably identifies the most helpful teacher. 2) Response Selection from a\nMultiple Teachers: When mixing answers from many teachers, Local Naturalness\nboosts a 32B student's accuracy on math benchmarks by 9.4pp over global\nselection, also surpassing the performance achieved by training on data from\nthe single best teacher. These results highlight the power of localized data\nquality evaluation and data mixing for more effective reasoning distillation."}
{"id": "2510.03989", "pdf": "https://arxiv.org/pdf/2510.03989", "abs": "https://arxiv.org/abs/2510.03989", "authors": ["Xue-Cheng Tai", "Hao Liu", "Lingfeng Li", "Raymond H. Chan"], "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "The Transformer architecture has revolutionized the field of sequence\nmodeling and underpins the recent breakthroughs in large language models\n(LLMs). However, a comprehensive mathematical theory that explains its\nstructure and operations remains elusive. In this work, we propose a novel\ncontinuous framework that rigorously interprets the Transformer as a\ndiscretization of a structured integro-differential equation. Within this\nformulation, the self-attention mechanism emerges naturally as a non-local\nintegral operator, and layer normalization is characterized as a projection to\na time-dependent constraint. This operator-theoretic and variational\nperspective offers a unified and interpretable foundation for understanding the\narchitecture's core components, including attention, feedforward layers, and\nnormalization. Our approach extends beyond previous theoretical analyses by\nembedding the entire Transformer operation in continuous domains for both token\nindices and feature dimensions. This leads to a principled and flexible\nframework that not only deepens theoretical insight but also offers new\ndirections for architecture design, analysis, and control-based\ninterpretations. This new interpretation provides a step toward bridging the\ngap between deep learning architectures and continuous mathematical modeling,\nand contributes a foundational perspective to the ongoing development of\ninterpretable and theoretically grounded neural network models."}
{"id": "2510.03992", "pdf": "https://arxiv.org/pdf/2510.03992", "abs": "https://arxiv.org/abs/2510.03992", "authors": ["Jehyeok Yeon", "Isha Chaudhary", "Gagandeep Singh"], "title": "Quantifying Distributional Robustness of Agentic Tool-Selection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."}
{"id": "2510.03995", "pdf": "https://arxiv.org/pdf/2510.03995", "abs": "https://arxiv.org/abs/2510.03995", "authors": ["Nges Brian Njungle", "Eric Jahns", "Milan Stojkov", "Michel A. Kinsy"], "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks", "categories": ["cs.CR", "cs.AI", "I.2; E.m"], "comment": "13 pages, 5 figures", "summary": "Deep learning has become a cornerstone of modern machine learning. It relies\nheavily on vast datasets and significant computational resources for high\nperformance. This data often contains sensitive information, making privacy a\nmajor concern in deep learning. Spiking Neural Networks (SNNs) have emerged as\nan energy-efficient alternative to conventional deep learning approaches.\nNevertheless, SNNs still depend on large volumes of data, inheriting all the\nprivacy challenges of deep learning. Homomorphic encryption addresses this\nchallenge by allowing computations to be performed on encrypted data, ensuring\ndata confidentiality throughout the entire processing pipeline. In this paper,\nwe introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using\nthe CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs\nand introduces two key algorithms for evaluating the Leaky Integrate-and-Fire\nactivation function: (1) a polynomial approximation algorithm designed for\nhigh-performance SNN inference, and (2) a novel scheme-switching algorithm that\noptimizes precision at a higher computational cost. We evaluate PRIVSPIKE on\nMNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5\nand ResNet-19 architectures, achieving encrypted inference accuracies of\n98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN\nLeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds\non Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on\nCIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as\na viable and efficient solution for secure SNN inference, bridging the gap\nbetween energy-efficient deep neural networks and strong cryptographic privacy\nguarantees while outperforming prior encrypted SNN solutions."}
{"id": "2510.03998", "pdf": "https://arxiv.org/pdf/2510.03998", "abs": "https://arxiv.org/abs/2510.03998", "authors": ["Songmei Yu", "Andrew Zagula"], "title": "AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "Accepted at the 23rd International Conference on Education and\n  Information Systems, Technologies and Applications (EISTA 2025)", "summary": "Collaborative group projects are integral to computer science education, as\nthey foster teamwork, problem-solving skills, and industry-relevant\ncompetencies. However, assessing individual contributions within group settings\nhas long been a challenge. Traditional assessment strategies, such as the equal\ndistribution of grades or subjective peer assessments, often fall short in\nterms of fairness, objectivity, and scalability, particularly in large\nclassrooms. This paper introduces a semi-automated, AI-assisted grading system\nthat evaluates both project quality and individual effort using repository\nmining, communication analytics, and machine learning models. The system\ncomprises modules for project evaluation, contribution analysis, and grade\ncomputation, integrating seamlessly with platforms like GitHub. A pilot\ndeployment in a senior-level course demonstrated high alignment with instructor\nassessments, increased student satisfaction, and reduced instructor grading\neffort. We conclude by discussing implementation considerations, ethical\nimplications, and proposed enhancements to broaden applicability."}
{"id": "2510.04001", "pdf": "https://arxiv.org/pdf/2510.04001", "abs": "https://arxiv.org/abs/2510.04001", "authors": ["Xuankang Zhang", "Jiangming Liu"], "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "The COVID-19 pandemic causes severe social and economic disruption around the\nworld, raising various subjects that are discussed over social media.\nIdentifying pandemic-related named entities as expressed on social media is\nfundamental and important to understand the discussions about the pandemic.\nHowever, there is limited work on named entity recognition on this topic due to\nthe following challenges: 1) COVID-19 texts in social media are informal and\ntheir annotations are rare and insufficient to train a robust recognition\nmodel, and 2) named entity recognition in COVID-19 requires extensive\ndomain-specific knowledge. To address these issues, we propose a novel entity\nknowledge augmentation approach for COVID-19, which can also be applied in\ngeneral biomedical named entity recognition in both informal text format and\nformal text format. Experiments carried out on the COVID-19 tweets dataset and\nPubMed dataset show that our proposed entity knowledge augmentation improves\nNER performance in both fully-supervised and few-shot settings. Our source code\nis publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master"}
{"id": "2510.04008", "pdf": "https://arxiv.org/pdf/2510.04008", "abs": "https://arxiv.org/abs/2510.04008", "authors": ["Sahil Joshi", "Agniva Chowdhury", "Amar Kanakamedala", "Ekam Singh", "Evan Tu", "Anshumali Shrivastava"], "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 7 figures", "summary": "Softmax Attention has a quadratic time complexity, which becomes prohibitive\nto run at long contexts, even with highly optimized GPU kernels. For example,\nFlashAttention (an exact, GPU-optimized implementation of Softmax Attention)\ncannot complete a single forward-backward pass of a multi-head attention layer\nonce the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We\nintroduce RACE Attention, a kernel-inspired alternative to Softmax Attention\nthat is linear in sequence length and embedding dimension. RACE Attention\nreplaces the exponential kernel with a sharpened angular (cosine) similarity,\nand approximates attention outputs via randomized projections and soft\nLocality-Sensitive Hashing (LSH). Across language modeling, masked language\nmodeling, and text classification, RACE Attention matches the accuracy of\nstrong baselines while reducing runtime and memory. In a controlled scale test,\nit processes up to 12 million tokens during a single forward-backward pass on\nan NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well\nbeyond the practical limits of the current state-of-the-art attention\nimplementations. RACE Attention thus offers a practical, theoretically grounded\nmechanism for outrageously long context windows on today's hardware. We hope\nthat it gets adopted in practice."}
{"id": "2510.04016", "pdf": "https://arxiv.org/pdf/2510.04016", "abs": "https://arxiv.org/abs/2510.04016", "authors": ["Thanapol Popit", "Natthapath Rungseesiripak", "Monthol Charattrakool", "Saksorn Ruangtanusak"], "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents", "categories": ["cs.CL", "cs.AI"], "comment": "IEEE ICSEC 2025", "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents."}
{"id": "2510.04019", "pdf": "https://arxiv.org/pdf/2510.04019", "abs": "https://arxiv.org/abs/2510.04019", "authors": ["Anthony Zhan"], "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Diffusion large language models (dLLMs) are a new paradigm of\nnon-autoregressive language models that are trained to predict multiple tokens\nin parallel and generate text via iterative unmasking. Recent works have\nsuccessfully pretrained dLLMs to parity with autoregressive LLMs at the 8B\nscale, but dLLMs have yet to benefit from modern post-training techniques, e.g.\nreinforcement learning (RL), that have proven effective for autoregressive\nmodels. Crucially, algorithms designed for traditional LLMs aren't directly\ncompatible with diffusion frameworks due to inherent differences in modeling\nassumptions. Moreover, existing attempts at dLLM post-training with RL rely on\nheuristic-based objectives with no theoretical grounding. In this work, we\npresent Amortized Group Relative Policy Optimization (AGRPO), a principled\non-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo\nsampling to compute an unbiased policy gradient estimate, making it the first\ntractable, faithful adaptation of policy gradient methods for dLLMs. We\ndemonstrate AGRPO's effectiveness on different math/reasoning tasks, a common\nsetting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x\nperformance on the Countdown task over the baseline LLaDA-8B-Instruct model and\n1.3x performance gains over comparable RL methods such as diffu-GRPO.\nFurthermore, these gains persist across different numbers of sampling steps at\ninference time, achieving better tradeoffs between compute and performance. Our\nresults demonstrate that online RL algorithms can be extended to diffusion LLMs\nin principled ways, maintaining both theoretical soundness and practical\neffectiveness."}
{"id": "2510.04020", "pdf": "https://arxiv.org/pdf/2510.04020", "abs": "https://arxiv.org/abs/2510.04020", "authors": ["Hao Wu", "Yuan Gao", "Xingjian Shi", "Shuaipeng Li", "Fan Xu", "Fan Zhang", "Zhihong Zhu", "Weiyan Wang", "Xiao Luo", "Kun Wang", "Xian Wu", "Xiaomeng Huang"], "title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To address the dual challenges of inherent stochasticity and\nnon-differentiable metrics in physical spatiotemporal forecasting, we propose\nSpatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in\nModel-Based Reinforcement Learning. SFP constructs a novel Generative World\nModel to simulate diverse, high-fidelity future states, enabling an\n\"imagination-based\" environmental simulation. Within this framework, a base\nforecasting model acts as an agent, guided by a beam search-based planning\nalgorithm that leverages non-differentiable domain metrics as reward signals to\nexplore high-return future sequences. These identified high-reward candidates\nthen serve as pseudo-labels to continuously optimize the agent's policy through\niterative self-training, significantly reducing prediction error and\ndemonstrating exceptional performance on critical domain metrics like capturing\nextreme events."}
{"id": "2510.04028", "pdf": "https://arxiv.org/pdf/2510.04028", "abs": "https://arxiv.org/abs/2510.04028", "authors": ["Xinhao Yao", "Lu Yu", "Xiaolin Hu", "Fengwei Teng", "Qing Cui", "Jun Zhou", "Yong Liu"], "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ongoing debate on whether reinforcement learning with verifiable rewards\n(RLVR) expands or shrinks the reasoning capabilities of large language models\n(LLMs) remains unresolved. Some studies contend that RLVR mainly improves\nsampling efficiency but at the expense of diversity and exploratory capacity,\nresulting in capability boundary shrinkage. In contrast, others demonstrate\nthat prolonged training can lead to the emergence of novel reasoning\nstrategies, suggesting capability boundary expansion. To reconcile these\ncontradictory findings, we theoretically and empirically show that both\nperspectives are partially valid-each aligning with a separate phase in an\ninherent two-stage probability mass dynamic: (1) Exploitation stage: initially,\nthe model primarily samples explored high-reward and low-reward tokens, while\nrarely selecting the potentially optimal token. Positive advantage estimates\nincrease the probability of high-reward tokens and decrease those of low-reward\ntokens, yet the optimal token's probability remains largely unchanged during\nthis stage. (2) Exploration stage: as training advances, the growth rate of\npreviously acquired high-reward tokens slows as their probabilities approach\nsaturation. When a potentially optimal token-now receiving positive advantage\nestimates-is occasionally sampled, its probability increases, while those of\nthe originally high-reward tokens decrease. This dynamic suggests that\nover-exploitation during the exploitation stage may lead to capability boundary\nshrinkage, whereas prolonged training into the exploration stage can promote an\nexpansion of the reasoning capability boundary. Building upon our insights, we\nrevisit the potential of only using relative negative gradients for prolonging\ntraining, providing a theoretical and empirical foundation for the development\nof more advanced reasoning capabilities."}
{"id": "2510.04031", "pdf": "https://arxiv.org/pdf/2510.04031", "abs": "https://arxiv.org/abs/2510.04031", "authors": ["Nelvin Tan", "James Asikin Cheung", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their\nimpressive abilities that arise from large training datasets and large model\nsizes. More recently, they have been shown to be very effective in textual\nclassification tasks, motivating the need to explain the LLMs' decisions.\nMotivated by practical constrains where LLMs are black-boxed and LLM calls are\nexpensive, we study how incorporating counterfactuals into LLM reasoning can\naffect the LLM's ability to identify the top words that have contributed to its\nclassification decision. To this end, we introduce a framework called the\ndecision changing rate that helps us quantify the importance of the top words\nin classification. Our experimental results show that using counterfactuals can\nbe helpful."}
{"id": "2510.04032", "pdf": "https://arxiv.org/pdf/2510.04032", "abs": "https://arxiv.org/abs/2510.04032", "authors": ["Zirui Wang", "Jiajun Wu", "Braden Teitge", "Jessalyn Holodinsky", "Steve Drew"], "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Large language models (LLMs) have become increasingly popular in medical\ndomains to assist physicians with a variety of clinical and operational tasks.\nGiven the fast-paced and high-stakes environment of emergency departments\n(EDs), small language models (SLMs), characterized by a reduction in parameter\ncount compared to LLMs, offer significant potential due to their inherent\nreasoning capability and efficient performance. This enables SLMs to support\nphysicians by providing timely and accurate information synthesis, thereby\nimproving clinical decision-making and workflow efficiency. In this paper, we\npresent a comprehensive benchmark designed to identify SLMs suited for ED\ndecision support, taking into account both specialized medical expertise and\nbroad general problem-solving capabilities. In our evaluations, we focus on\nSLMs that have been trained on a mixture of general-domain and medical corpora.\nA key motivation for emphasizing SLMs is the practical hardware limitations,\noperational cost constraints, and privacy concerns in the typical real-world\ndeployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and\nPubMedQA, with the medical abstracts dataset emulating tasks aligned with real\nED physicians' daily tasks. Experimental results reveal that general-domain\nSLMs surprisingly outperform their medically fine-tuned counterparts across\nthese diverse benchmarks for ED. This indicates that for ED, specialized\nmedical fine-tuning of the model may not be required."}
{"id": "2510.04034", "pdf": "https://arxiv.org/pdf/2510.04034", "abs": "https://arxiv.org/abs/2510.04034", "authors": ["Linn Bieske", "Carla Lorente"], "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in image editing have shifted from manual pixel manipulation\nto employing deep learning methods like stable diffusion models, which now\nleverage cross-attention mechanisms for text-driven control. This transition\nhas simplified the editing process but also introduced variability in results,\nsuch as inconsistent hair color changes. Our research aims to enhance the\nprecision and reliability of prompt-to-prompt image editing frameworks by\nexploring and optimizing hyperparameters. We present a comprehensive study of\nthe \"word swap\" method, develop an \"attention re-weight method\" for better\nadaptability, and propose the \"CL P2P\" framework to address existing\nlimitations like cycle inconsistency. This work contributes to understanding\nand improving the interaction between hyperparameter settings and the\narchitectural choices of neural network models, specifically their attention\nmechanisms, which significantly influence the composition and quality of the\ngenerated images."}
{"id": "2510.04039", "pdf": "https://arxiv.org/pdf/2510.04039", "abs": "https://arxiv.org/abs/2510.04039", "authors": ["Bin Lei", "Nuo Xu", "Ali Payani", "Mingyi Hong", "Chunhua Liao", "Yu Cao", "Caiwen Ding"], "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples)."}
{"id": "2510.04044", "pdf": "https://arxiv.org/pdf/2510.04044", "abs": "https://arxiv.org/abs/2510.04044", "authors": ["Bingtao Yang", "Yujia Wang", "Mengzhi Jiao", "Hongwei Huo"], "title": "Quantization Range Estimation for Convolutional Neural Networks", "categories": ["cs.CV", "cs.AI", "00-01", "I.2.6; K.3.2"], "comment": "11 pages, 5 tables, research report", "summary": "Post-training quantization for reducing the storage of deep neural network\nmodels has been demonstrated to be an effective way in various tasks. However,\nlow-bit quantization while maintaining model accuracy is a challenging problem.\nIn this paper, we present a range estimation method to improve the quantization\nperformance for post-training quantization. We model the range estimation into\nan optimization problem of minimizing quantization errors by layer-wise local\nminima. We prove this problem is locally convex and present an efficient search\nalgorithm to find the optimal solution. We propose the application of the above\nsearch algorithm to the transformed weights space to do further improvement in\npractice. Our experiments demonstrate that our method outperforms\nstate-of-the-art performance generally on top-1 accuracy for image\nclassification tasks on the ResNet series models and Inception-v3 model. The\nexperimental results show that the proposed method has almost no loss of top-1\naccuracy in 8-bit and 6-bit settings for image classifications, and the\naccuracy of 4-bit quantization is also significantly improved. The code is\navailable at https://github.com/codeiscommitting/REQuant."}
{"id": "2510.04057", "pdf": "https://arxiv.org/pdf/2510.04057", "abs": "https://arxiv.org/abs/2510.04057", "authors": ["Zhenyu Pan", "Yucheng Lu", "Han Liu"], "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation", "categories": ["cs.CV", "cs.AI"], "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We present MetaFind, a scene-aware tri-modal compositional retrieval\nframework designed to enhance scene generation in the metaverse by retrieving\n3D assets from large-scale repositories. MetaFind addresses two core\nchallenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,\nand stylistic constraints, and (ii) the absence of a standardized retrieval\nparadigm specifically tailored for 3D asset retrieval, as existing approaches\nmainly rely on general-purpose 3D shape representation models. Our key\ninnovation is a flexible retrieval mechanism that supports arbitrary\ncombinations of text, image, and 3D modalities as queries, enhancing spatial\nreasoning and style consistency by jointly modeling object-level features\n(including appearance) and scene-level layout structures. Methodologically,\nMetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that\ncaptures spatial relationships and object appearance features, ensuring\nretrieved 3D assets are contextually and stylistically coherent with the\nexisting scene, regardless of coordinate frame transformations. The framework\nsupports iterative scene construction by continuously adapting retrieval\nresults to current scene updates. Empirical evaluations demonstrate the\nimproved spatial and stylistic consistency of MetaFind in various retrieval\ntasks compared to baseline methods."}
{"id": "2510.04067", "pdf": "https://arxiv.org/pdf/2510.04067", "abs": "https://arxiv.org/abs/2510.04067", "authors": ["Junxi Yan", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"], "title": "What Scales in Cross-Entropy Scaling Law?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The cross-entropy scaling law has long served as a key tool for guiding the\ndevelopment of large language models. It shows that cross-entropy loss\ndecreases in a predictable power-law rate as the model size increases. However,\nrecent evidence indicates that this law breaks down at very large scales: the\nloss decreases more slowly than expected, which causes significant trouble for\ndeveloping large language models. In this paper, we hypothesize that the root\ncause lies in the fact that cross-entropy itself does not truly scale; instead,\nonly one of its hidden components does. To investigate this, we introduce a\nnovel decomposition of cross-entropy into three parts: Error-Entropy,\nSelf-Alignment, and Confidence. We show both theoretically and empirically that\nthis decomposition precisely captures the training dynamics and optimization\nobjectives. Through extensive experiments on multiple datasets and 32 models\nspanning five orders of magnitude in size, we find that only error-entropy\nfollows a robust power-law scaling, while the other two terms remain largely\ninvariant. Moreover, error-entropy constitutes the dominant share of\ncross-entropy in small models but diminishes in proportion as models grow\nlarger. This explains why the cross-entropy scaling law appears accurate at\nsmall scales but fails at very large ones. Our findings establish the\nerror-entropy scaling law as a more accurate description of model behavior. We\nbelieve it will have wide applications in the training, understanding, and\nfuture development of large language models."}
{"id": "2510.04072", "pdf": "https://arxiv.org/pdf/2510.04072", "abs": "https://arxiv.org/abs/2510.04072", "authors": ["Ziyan Wang", "Zheng Wang", "Jie Fu", "Xingwei Qu", "Qi Cheng", "Shengpu Tang", "Minjia Zhang", "Xiaoming Huo"], "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in\nlarge language models (LLMs). Yet on-policy algorithms such as Group Relative\nPolicy Optimization (GRPO) often suffer in early training: noisy gradients from\nlow-quality rollouts lead to unstable updates and inefficient exploration. We\nintroduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient\nframework to address these limitations via decomposing each step into three\nstages: a short fast trajectory of inner steps on the same batch, a reposition\nmechanism to control off-policy drift, and a final slow correction. This\nreposition-before-update design preserves the objective and rollout process\nunchanged, making SFPO plug-compatible with existing policy-gradient pipelines.\nExtensive experiments demonstrate that SFPO consistently improves stability,\nreduces rollouts, and accelerates convergence of reasoning RL training.\nSpecifically, it outperforms GRPO by up to 2.80 points in average on math\nreasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts\nand a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best\naccuracy."}
{"id": "2510.04087", "pdf": "https://arxiv.org/pdf/2510.04087", "abs": "https://arxiv.org/abs/2510.04087", "authors": ["Hyung Gyu Rho"], "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling", "categories": ["stat.ME", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling,\nrely on reward models trained with pairwise comparison data. While effective at\nlearning relative preferences, this paradigm fails to capture a signal of\nresponse acceptability, leaving systems vulnerable to selecting the least bad\nof many unacceptable options. This is particularly problematic for hard\nprompts, where the risk of such false acceptances increases with the number of\nsamples. In this paper, we address this critical reliability gap by introducing\na new data collection and modeling framework. By augmenting preference data\nwith an outside option, inspired by discrete choice models, we train a reward\nmodel that can distinguish not just what is \\textit{better}, but what is\n\\textit{good enough}. We leverage this capability to create an adaptive\ninference strategy, best of mini-N in-loop, which partitions the generation\nbudget into sequential loops with a calibrated, early-exit condition. Our\nexperiments show that when tuned as an alignment guardrail, it reduces\nreliability failures by 70\\%, and when tuned as an inference accelerator, it\nimproves average inference speed by over 22\\% in IMDB-sentiment setting. We\nthus provide a principled and flexible framework for practitioners to\nexplicitly manage the trade-off between reliability and computational\nefficiency."}
{"id": "2510.04088", "pdf": "https://arxiv.org/pdf/2510.04088", "abs": "https://arxiv.org/abs/2510.04088", "authors": ["Nan Jiang", "Tengyang Xie"], "title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To appear in Statistical Science", "summary": "This article introduces the theory of offline reinforcement learning in large\nstate spaces, where good policies are learned from historical data without\nonline interactions with the environment. Key concepts introduced include\nexpressivity assumptions on function approximation (e.g., Bellman completeness\nvs. realizability) and data coverage (e.g., all-policy vs. single-policy\ncoverage). A rich landscape of algorithms and results is described, depending\non the assumptions one is willing to make and the sample and computational\ncomplexity guarantees one wishes to achieve. We also discuss open questions and\nconnections to adjacent areas."}
{"id": "2510.04090", "pdf": "https://arxiv.org/pdf/2510.04090", "abs": "https://arxiv.org/abs/2510.04090", "authors": ["Nikita Gabdullin"], "title": "Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "28 pages, 12 figures, 10 tables, 12 equations, 1 algorithm", "summary": "Supervised learning (SL) methods are indispensable for neural network (NN)\ntraining used to perform classification tasks. While resulting in very high\naccuracy, SL training often requires making NN parameter number dependent on\nthe number of classes, limiting their applicability when the number of classes\nis extremely large or unknown in advance. In this paper we propose a\nmethodology that allows one to train the same NN architecture regardless of the\nnumber of classes. This is achieved by using predefined vector systems as the\ntarget latent space configuration (LSC) during NN training. We discuss the\ndesired properties of target configurations and choose randomly perturbed\nvectors of An root system for our experiments. These vectors are used to\nsuccessfully train encoders and visual transformers (ViT) on Cinic-10 and\nImageNet-1K in low- and high-dimensional cases by matching NN predictions with\nthe predefined vectors. Finally, ViT is trained on a dataset with 1.28 million\nclasses illustrating the applicability of the method to training on datasets\nwith extremely large number of classes. In addition, potential applications of\nLSC in lifelong learning and NN distillation are discussed illustrating\nversatility of the proposed methodology."}
{"id": "2510.04098", "pdf": "https://arxiv.org/pdf/2510.04098", "abs": "https://arxiv.org/abs/2510.04098", "authors": ["Chenxiang Ma", "Xinyi Chen", "Yujie Wu", "Kay Chen Tan", "Jibin Wu"], "title": "Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Spiking neural networks (SNNs), recognized as an energy-efficient alternative\nto traditional artificial neural networks (ANNs), have advanced rapidly through\nthe scaling of models and datasets. However, such scaling incurs considerable\ntraining overhead, posing challenges for researchers with limited computational\nresources and hindering the sustained development of SNNs. Data pruning is a\npromising strategy for accelerating training by retaining the most informative\nexamples and discarding redundant ones, but it remains largely unexplored in\nSNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture\nthe intrinsic importance of examples and suffers from high gradient variance.\nTo address these challenges, we propose a novel spike-aware data pruning (SADP)\nmethod. SADP reduces gradient variance by determining each example's selection\nprobability to be proportional to its gradient norm, while avoiding the high\ncost of direct gradient computation through an efficient upper bound, termed\nspike-aware importance score. This score accounts for the influence of\nall-or-nothing spikes on the gradient norm and can be computed with negligible\noverhead. Extensive experiments across diverse datasets and architectures\ndemonstrate that SADP consistently outperforms data pruning baselines and\nachieves training speedups close to the theoretical maxima at different pruning\nratios. Notably, SADP reduces training time by 35% on ImageNet while\nmaintaining accuracy comparable to that of full-data training. This work,\ntherefore, establishes a data-centric paradigm for efficient SNN training and\npaves the way for scaling SNNs to larger models and datasets. The source code\nwill be released publicly after the review process."}
{"id": "2510.04100", "pdf": "https://arxiv.org/pdf/2510.04100", "abs": "https://arxiv.org/abs/2510.04100", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Harold Soh"], "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing", "categories": ["cs.CV", "cs.AI"], "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally", "summary": "Topological mapping offers a compact and robust representation for\nnavigation, but progress in the field is hindered by the lack of standardized\nevaluation metrics, datasets, and protocols. Existing systems are assessed\nusing different environments and criteria, preventing fair and reproducible\ncomparisons. Moreover, a key challenge - perceptual aliasing - remains\nunder-quantified, despite its strong influence on system performance. We\naddress these gaps by (1) formalizing topological consistency as the\nfundamental property of topological maps and showing that localization accuracy\nprovides an efficient and interpretable surrogate metric, and (2) proposing the\nfirst quantitative measure of dataset ambiguity to enable fair comparisons\nacross environments. To support this protocol, we curate a diverse benchmark\ndataset with calibrated ambiguity levels, implement and release deep-learned\nbaseline systems, and evaluate them alongside classical methods. Our\nexperiments and analysis yield new insights into the limitations of current\napproaches under perceptual aliasing. All datasets, baselines, and evaluation\ntools are fully open-sourced to foster consistent and reproducible research in\ntopological mapping."}
{"id": "2510.04120", "pdf": "https://arxiv.org/pdf/2510.04120", "abs": "https://arxiv.org/abs/2510.04120", "authors": ["Fengying Ye", "Shanshan Wang", "Lidia S. Chao", "Derek F. Wong"], "title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Metaphor analysis is a complex linguistic phenomenon shaped by context and\nexternal factors. While Large Language Models (LLMs) demonstrate advanced\ncapabilities in knowledge integration, contextual reasoning, and creative\ngeneration, their mechanisms for metaphor comprehension remain insufficiently\nexplored. This study examines LLMs' metaphor-processing abilities from three\nperspectives: (1) Concept Mapping: using embedding space projections to\nevaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall\nin love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing\nmetaphorical words and their literal counterparts to identify inherent\nmetaphorical knowledge; and (3) Syntactic Sensitivity: assessing how\nmetaphorical syntactic structures influence LLMs' performance. Our findings\nreveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations,\ndepend on metaphorical indicators in training data rather than contextual cues,\nand are more sensitive to syntactic irregularities than to structural\ncomprehension. These insights underline the limitations of LLMs in metaphor\nanalysis and call for more robust computational approaches."}
{"id": "2510.04126", "pdf": "https://arxiv.org/pdf/2510.04126", "abs": "https://arxiv.org/abs/2510.04126", "authors": ["Ziying Zhang", "Yaqing Wang", "Yuxuan Sun", "Min Ye", "Quanming Yao"], "title": "Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cold-start drug-target interaction (DTI) prediction focuses on interaction\nbetween novel drugs and proteins. Previous methods typically learn transferable\ninteraction patterns between structures of drug and proteins to tackle it.\nHowever, insight from proteomics suggest that protein have multi-level\nstructures and they all influence the DTI. Existing works usually represent\nprotein with only primary structures, limiting their ability to capture\ninteractions involving higher-level structures. Inspired by this insight, we\npropose ColdDTI, a framework attending on protein multi-level structure for\ncold-start DTI prediction. We employ hierarchical attention mechanism to mine\ninteraction between multi-level protein structures (from primary to quaternary)\nand drug structures at both local and global granularities. Then, we leverage\nmined interactions to fuse structure representations of different levels for\nfinal prediction. Our design captures biologically transferable priors,\navoiding the risk of overfitting caused by excessive reliance on representation\nlearning. Experiments on benchmark datasets demonstrate that ColdDTI\nconsistently outperforms previous methods in cold-start settings."}
{"id": "2510.04127", "pdf": "https://arxiv.org/pdf/2510.04127", "abs": "https://arxiv.org/abs/2510.04127", "authors": ["Sean Moran"], "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in\ninformation retrieval, underpinning large-scale applications in computer\nvision, natural language processing, and cross-modal search. Hashing-based\nmethods provide an efficient solution by mapping high-dimensional data into\ncompact binary codes that enable fast similarity computations in Hamming space.\nOver the past two decades, a substantial body of work has explored learning to\nhash, where projection and quantisation functions are optimised from data\nrather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing\nmethods, with an emphasis on the core ideas that shaped the field. We review\nsupervised, unsupervised, and semi-supervised approaches, highlighting how\nprojection functions are designed to generate meaningful embeddings and how\nquantisation strategies convert these embeddings into binary codes. We also\nexamine extensions to multi-bit and multi-threshold models, as well as early\nadvances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our\ngoal is to introduce the conceptual foundations of learning-based hashing for\nANN search. By situating these early models in their historical context, we aim\nto equip readers with a structured understanding of the principles, trade-offs,\nand open challenges that continue to inform current research in this area."}
{"id": "2510.04130", "pdf": "https://arxiv.org/pdf/2510.04130", "abs": "https://arxiv.org/abs/2510.04130", "authors": ["Yang Chen", "Yitao Liang", "Zhouchen Lin"], "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In Transformers, Position Embeddings (PEs) significantly influence Length\nGeneralization (LG) performance, yet their fundamental role remains unclear. In\nthis work, we investigate the limitations and capabilities of PEs in achieving\nLG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),\nintroducing Linear Representation Complexity (LRC) to characterize when PEs\nenable LG. Our analysis shows that PEs do not expand computational capabilities\nbut structure learned computations across positions. Extending to practical\nTransformers, we propose Sequential Representation Complexity (SRC) and\nconjecture that LG is possible if and only if SRC remains invariant across\nscales. We support this hypothesis with empirical evidence in various reasoning\ntasks. To enhance LG, we introduce Scale Hint, allowing flexible instance\nscaling, and a Learning-Based Position Embedding framework that automatically\nlearns positional relations. Our work provides theoretical insights and\npractical strategies for improving LG in Transformers."}
{"id": "2510.04134", "pdf": "https://arxiv.org/pdf/2510.04134", "abs": "https://arxiv.org/abs/2510.04134", "authors": ["Yiming Niu", "Jinliang Deng", "Yongxin Tong"], "title": "PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Periodicity is a fundamental characteristic of time series data and has long\nplayed a central role in forecasting. Recent deep learning methods strengthen\nthe exploitation of periodicity by treating patches as basic tokens, thereby\nimproving predictive effectiveness. However, their efficiency remains a\nbottleneck due to large parameter counts and heavy computational costs. This\npaper provides, for the first time, a clear explanation of why patch-level\nprocessing is inherently inefficient, supported by strong evidence from\nreal-world data. To address these limitations, we introduce a phase perspective\nfor modeling periodicity and present an efficient yet effective solution,\nPhaseFormer. PhaseFormer features phase-wise prediction through compact phase\nembeddings and efficient cross-phase interaction enabled by a lightweight\nrouting mechanism. Extensive experiments demonstrate that PhaseFormer achieves\nstate-of-the-art performance with around 1k parameters, consistently across\nbenchmark datasets. Notably, it excels on large-scale and complex datasets,\nwhere models with comparable efficiency often struggle. This work marks a\nsignificant step toward truly efficient and effective time series forecasting.\nCode is available at this repository:\nhttps://github.com/neumyor/PhaseFormer_TSL"}
{"id": "2510.04135", "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment."}
{"id": "2510.04142", "pdf": "https://arxiv.org/pdf/2510.04142", "abs": "https://arxiv.org/abs/2510.04142", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/."}
{"id": "2510.04146", "pdf": "https://arxiv.org/pdf/2510.04146", "abs": "https://arxiv.org/abs/2510.04146", "authors": ["Minseo Kim", "Coleman Hooper", "Aditya Tomar", "Chenfeng Xu", "Mehrdad Farajtabar", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance on a\nbroad range of Natural Language Processing (NLP) tasks, including document\nprocessing and coding. Autoregressive Language Models (ARMs), which generate\ntokens sequentially conditioned on all previous tokens, have been the\npredominant paradigm for LLMs. However, while these networks have achieved high\naccuracy across a range of downstream tasks, they exhibit low arithmetic\nintensity due to the inherent sequential dependency with next-token prediction.\nRecently, Diffusion Language Models (DLMs) have emerged as a promising\nalternative architecture. DLMs generate output text in parallel, breaking the\nlimitations of sequential dependency. However, the performance implications of\nDLMs relative to commonly deployed ARMs are not fully understood. In this work,\nwe present a comprehensive performance study analyzing the performance\ncharacteristics of ARMs and DLMs, using both theoretical analysis and profiling\ndata to characterize the trade-offs between these approaches. We illustrate\nthat although DLMs exhibit higher arithmetic intensity compared to ARMs because\nof their capability to utilize parallelism across sequence lengths, they fail\nto scale effectively to longer contexts. We then explore DLMs with block-wise\ndecoding, outlining how this approach allows for increased arithmetic\nintensity, while still scaling well to long contexts (similar to ARMs). We also\nshow interesting trade-offs for batched inference, where we find that ARMs\nexhibit superior throughput, as they benefit more from parallelism across\nsequences in the batch. Finally, we highlight opportunities for accelerating\nDLM inference, and, in particular, highlight the importance of reducing the\nnumber of sampling steps for allowing open-source DLMs to provide improved\nlatency relative to ARMs."}
{"id": "2510.04166", "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages."}
{"id": "2510.04182", "pdf": "https://arxiv.org/pdf/2510.04182", "abs": "https://arxiv.org/abs/2510.04182", "authors": ["Wengao Ye", "Yan Liang", "Lianlei Shan"], "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shifted from\nexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,\nwhere intermediate thoughts are represented as vectors rather than text.\nHowever, latent reasoning can be brittle on challenging, out-of-distribution\ntasks where robust reasoning is most critical. To overcome these limitations,\nwe introduce Latent Thought Policy Optimization (LTPO), a parameter-free\nframework that enhances LLM reasoning entirely at test time, without requiring\nmodel parameter updates. LTPO treats intermediate latent \"thought\" vectors as\ndynamic parameters that are actively optimized for each problem instance. It\nemploys an online policy gradient method guided by an intrinsic,\nconfidence-based reward signal computed directly from the frozen LLM's own\noutput distributions, eliminating the need for external supervision or\nexpensive text generation during optimization. Extensive experiments on five\nreasoning benchmarks show that LTPO not only matches or surpasses strong\nbaselines on standard tasks but also demonstrates remarkable robustness where\nothers fail. Most notably, on highly challenging AIME benchmarks where existing\nlatent reasoning baselines collapse to near-zero accuracy, LTPO delivers\nsubstantial improvements, showcasing a unique capability for complex reasoning."}
{"id": "2510.04187", "pdf": "https://arxiv.org/pdf/2510.04187", "abs": "https://arxiv.org/abs/2510.04187", "authors": ["Hagen Holthusen", "Ellen Kuhl"], "title": "A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains", "categories": ["cs.CE", "cs.AI", "65, 74", "I.6; J.2"], "comment": "40 pages, 19 figures", "summary": "We propose a complement to constitutive modeling that augments neural\nnetworks with material principles to capture anisotropy and inelasticity at\nfinite strains. The key element is a dual potential that governs dissipation,\nconsistently incorporates anisotropy, and-unlike conventional convex\nformulations-satisfies the dissipation inequality without requiring convexity.\n  Our neural network architecture employs invariant-based input representations\nin terms of mixed elastic, inelastic and structural tensors. It adapts Input\nConvex Neural Networks, and introduces Input Monotonic Neural Networks to\nbroaden the admissible potential class. To bypass exponential-map time\nintegration in the finite strain regime and stabilize the training of inelastic\nmaterials, we employ recurrent Liquid Neural Networks.\n  The approach is evaluated at both material point and structural scales. We\nbenchmark against recurrent models without physical constraints and validate\npredictions of deformation and reaction forces for unseen boundary value\nproblems. In all cases, the method delivers accurate and stable performance\nbeyond the training regime. The neural network and finite element\nimplementations are available as open-source and are accessible to the public\nvia https://doi.org/10.5281/zenodo.17199965."}
{"id": "2510.04189", "pdf": "https://arxiv.org/pdf/2510.04189", "abs": "https://arxiv.org/abs/2510.04189", "authors": ["Prashansa Panda", "Shalabh Bhatnagar"], "title": "Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent studies have increasingly focused on non-asymptotic convergence\nanalyses for actor-critic (AC) algorithms. One such effort introduced a\ntwo-timescale critic-actor algorithm for the discounted cost setting using a\ntabular representation, where the usual roles of the actor and critic are\nreversed. However, only asymptotic convergence was established there.\nSubsequently, both asymptotic and non-asymptotic analyses of the critic-actor\nalgorithm with linear function approximation were conducted. In our work, we\nintroduce the first natural critic-actor algorithm with function approximation\nfor the long-run average cost setting and under inequality constraints. We\nprovide the non-asymptotic convergence guarantees for this algorithm. Our\nanalysis establishes optimal learning rates and we also propose a modification\nto enhance sample complexity. We further show the results of experiments on\nthree different Safety-Gym environments where our algorithm is found to be\ncompetitive in comparison with other well known algorithms."}
{"id": "2510.04192", "pdf": "https://arxiv.org/pdf/2510.04192", "abs": "https://arxiv.org/abs/2510.04192", "authors": ["Rabiya Khalid", "Evangelos Pournaras"], "title": "Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The growing electricity demand and increased use of smart appliances are\nplacing new pressures on power grids, making efficient energy management more\nimportant than ever. The existing energy management systems often prioritize\nsystem efficiency (balanced energy demand and supply) at the expense of user\ncomfort. This paper addresses this gap by proposing a novel decentralized\nmulti-agent coordination-based demand-side management system. The proposed\nsystem enables individual agents to coordinate for demand-side energy\noptimization while improving the user comfort and maintaining the system\nefficiency. A key innovation of this work is the introduction of a slot\nexchange mechanism, where agents first receive optimized appliance-level energy\nconsumption schedules and then coordinate with each other to adjust these\nschedules through slot exchanges. This approach improves user comfort even when\nagents show non-altruistic behaviour, and it scales well with large\npopulations. The system also promotes fairness by balancing satisfaction levels\nacross users. For performance evaluation, a real-world dataset is used, and the\nresults demonstrate that the proposed slot exchange mechanism increases user\ncomfort and fairness without raising system inefficiency cost, making it a\npractical and scalable solution for future smart grids."}
{"id": "2510.04201", "pdf": "https://arxiv.org/pdf/2510.04201", "abs": "https://arxiv.org/abs/2510.04201", "authors": ["Moo Hyun Son", "Jintaek Oh", "Sun Bin Mun", "Jaechul Roh", "Sehyun Choi"], "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}."}
{"id": "2510.04204", "pdf": "https://arxiv.org/pdf/2510.04204", "abs": "https://arxiv.org/abs/2510.04204", "authors": ["Zhengyang Tang", "Zihan Ye", "Chenyu Huang", "Xuhan Huang", "Chengpeng Li", "Sihang Li", "Guanhua Chen", "Ming Yan", "Zizhuo Wang", "Hongyuan Zha", "Dayiheng Liu", "Benyou Wang"], "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": "Work in progress", "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional \\textit{non-reflective} datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\n\\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\n\\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks."}
{"id": "2510.04205", "pdf": "https://arxiv.org/pdf/2510.04205", "abs": "https://arxiv.org/abs/2510.04205", "authors": ["Di Zhang"], "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "math.OC", "68T07, 41A15, 52B11", "F.2.2; G.1.2; I.2.6"], "comment": "10", "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\ntraditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability\nand a strong mathematical foundation. However, their parameter efficiency\nremains a significant challenge for practical deployment. This paper introduces\nPolyKAN, a novel theoretical framework for KAN compression that provides formal\nguarantees on both model size reduction and approximation error. By leveraging\nthe inherent piecewise polynomial structure of KANs, we formulate the\ncompression problem as one of optimal polyhedral region merging. We establish a\nrigorous polyhedral characterization of KANs, develop a complete theory of\n$\\epsilon$-equivalent compression, and design an optimal dynamic programming\nalgorithm that guarantees minimal compression under specified error bounds. Our\ntheoretical analysis demonstrates that PolyKAN achieves provably minimal\ncompression while maintaining strict error control, with polynomial-time\ncomplexity in all network parameters. The framework provides the first formal\nfoundation for KAN compression with mathematical guarantees, opening new\ndirections for efficient deployment of interpretable neural architectures."}
{"id": "2510.04212", "pdf": "https://arxiv.org/pdf/2510.04212", "abs": "https://arxiv.org/abs/2510.04212", "authors": ["Haiquan Qiu", "Quanming Yao"], "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem."}
{"id": "2510.04217", "pdf": "https://arxiv.org/pdf/2510.04217", "abs": "https://arxiv.org/abs/2510.04217", "authors": ["Chenlu Ding", "Jiancan Wu", "Leheng Sheng", "Fan Zhang", "Yancheng Yuan", "Xiang Wang", "Xiangnan He"], "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities across vision-language tasks, yet their large-scale deployment\nraises pressing concerns about memorized private data, outdated knowledge, and\nharmful content. Existing unlearning approaches for MLLMs typically adapt\ntraining-based strategies such as gradient ascent or preference optimization,\nbut these methods are computationally expensive, irreversible, and often\ndistort retained knowledge. In this work, we propose MLLMEraser, an\ninput-aware, training-free framework for test-time unlearning. Our approach\nleverages activation steering to enable dynamic knowledge erasure without\nparameter updates. Specifically, we construct a multimodal erasure direction by\ncontrasting adversarially perturbed, knowledge-recall image-text pairs with\nknowledge-erasure counterparts, capturing both textual and visual\ndiscrepancies. To prevent unnecessary interference, we further design an\ninput-aware steering mechanism that adaptively determines when and how the\nerasure direction should be applied, preserving utility on retained knowledge\nwhile enforcing forgetting on designated content. Experiments on LLaVA-1.5 and\nQwen-2.5-VL demonstrate that MLLMEraser consistently outperforms\nstate-of-the-art MLLM unlearning baselines, achieving stronger forgetting\nperformance with lower computational cost and minimal utility degradation."}
{"id": "2510.04220", "pdf": "https://arxiv.org/pdf/2510.04220", "abs": "https://arxiv.org/abs/2510.04220", "authors": ["Lixuan He", "Shikang Zheng", "Linfeng Zhang"], "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling."}
{"id": "2510.04225", "pdf": "https://arxiv.org/pdf/2510.04225", "abs": "https://arxiv.org/abs/2510.04225", "authors": ["Yikun Ji", "Yan Hong", "Bowen Deng", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Zoom-In to Sort AI-Generated Images Out", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T45", "I.2.10; I.2.7"], "comment": "9 pages, 6 images (19 pages, 11 figures including appendix)", "summary": "The rapid growth of AI-generated imagery has blurred the boundary between\nreal and synthetic content, raising critical concerns for digital integrity.\nVision-language models (VLMs) offer interpretability through explanations but\noften fail to detect subtle artifacts in high-quality synthetic images. We\npropose ZoomIn, a two-stage forensic framework that improves both accuracy and\ninterpretability. Mimicking human visual inspection, ZoomIn first scans an\nimage to locate suspicious regions and then performs a focused analysis on\nthese zoomed-in areas to deliver a grounded verdict. To support training, we\nintroduce MagniFake, a dataset of 20,000 real and high-quality synthetic images\nannotated with bounding boxes and forensic explanations, generated through an\nautomated VLM-based pipeline. Our method achieves 96.39% accuracy with robust\ngeneralization, while providing human-understandable explanations grounded in\nvisual evidence."}
{"id": "2510.04226", "pdf": "https://arxiv.org/pdf/2510.04226", "abs": "https://arxiv.org/abs/2510.04226", "authors": ["Dustin Wright", "Sarah Masud", "Jared Moore", "Srishti Yadav", "Maria Antoniak", "Chan Young Park", "Isabelle Augenstein"], "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "comment": "16 pages; 8 figures, 4 tables", "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation"}
{"id": "2510.04229", "pdf": "https://arxiv.org/pdf/2510.04229", "abs": "https://arxiv.org/abs/2510.04229", "authors": ["Rikuo Sasaki", "Michimasa Inaba"], "title": "When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue", "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "23 pages, 19 figures. International Conference on Human-Agent\n  Interaction (HAI 2025), November 10-13, 2025, Yokohama, Japan", "summary": "Recent advancements in AI have highlighted its application in captology, the\nfield of using computers as persuasive technologies. We hypothesized that the\n\"conformity effect,\" where individuals align with others' actions, also occurs\nwith AI agents. This study verifies this hypothesis by introducing a \"Persuadee\nAgent\" that is persuaded alongside a human participant in a three-party\npersuasive dialogue with a Persuader Agent. We conducted a text-based dialogue\nexperiment with human participants. We compared four conditions manipulating\nthe Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and\nthe presence of an icebreaker session. Results showed that when the Persuadee\nAgent accepted persuasion, both perceived persuasiveness and actual attitude\nchange significantly improved. Attitude change was greatest when an icebreaker\nwas also used, whereas an unpersuaded AI agent suppressed attitude change.\nAdditionally, it was confirmed that the persuasion acceptance of participants\nincreased at the moment the Persuadee Agent was persuaded. These results\nsuggest that appropriately designing a Persuadee Agent can improve persuasion\nthrough the conformity effect."}
{"id": "2510.04233", "pdf": "https://arxiv.org/pdf/2510.04233", "abs": "https://arxiv.org/abs/2510.04233", "authors": ["Kai Yang", "Yuqi Huang", "Junheng Tao", "Wanyu Wang", "Qitian Wu"], "title": "Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling 3D dynamics is a fundamental problem in multi-body systems across\nscientific and engineering domains and has important practical implications in\ntrajectory prediction and simulation. While recent GNN-based approaches have\nachieved strong performance by enforcing geometric symmetries, encoding\nhigh-order features or incorporating neural-ODE mechanics, they typically\ndepend on explicitly observed structures and inherently fail to capture the\nunobserved interactions that are crucial to complex physical behaviors and\ndynamics mechanism. In this paper, we propose PAINET, a principled\nSE(3)-equivariant neural architecture for learning all-pair interactions in\nmulti-body systems. The model comprises: (1) a novel physics-inspired attention\nnetwork derived from the minimization trajectory of an energy function, and (2)\na parallel decoder that preserves equivariance while enabling efficient\ninference. Empirical results on diverse real-world benchmarks, including human\nmotion capture, molecular dynamics, and large-scale protein simulations, show\nthat PAINET consistently outperforms recently proposed models, yielding 4.7% to\n41.5% error reductions in 3D dynamics prediction with comparable computation\ncosts in terms of time and memory."}
{"id": "2510.04234", "pdf": "https://arxiv.org/pdf/2510.04234", "abs": "https://arxiv.org/abs/2510.04234", "authors": ["Runhan Huang", "Haldun Balim", "Heng Yang", "Yilun Du"], "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "Legged locomotion demands controllers that are both robust and adaptable,\nwhile remaining compatible with task and safety considerations. However,\nmodel-free reinforcement learning (RL) methods often yield a fixed policy that\ncan be difficult to adapt to new behaviors at test time. In contrast, Model\nPredictive Control (MPC) provides a natural approach to flexible behavior\nsynthesis by incorporating different objectives and constraints directly into\nits optimization process. However, classical MPC relies on accurate dynamics\nmodels, which are often difficult to obtain in complex environments and\ntypically require simplifying assumptions. We present Diffusion-MPC, which\nleverages a learned generative diffusion model as an approximate dynamics prior\nfor planning, enabling flexible test-time adaptation through reward and\nconstraint based optimization. Diffusion-MPC jointly predicts future states and\nactions; at each reverse step, we incorporate reward planning and impose\nconstraint projection, yielding trajectories that satisfy task objectives while\nremaining within physical limits. To obtain a planning model that adapts beyond\nimitation pretraining, we introduce an interactive training algorithm for\ndiffusion based planner: we execute our reward-and-constraint planner in\nenvironment, then filter and reweight the collected trajectories by their\nrealized returns before updating the denoiser. Our design enables strong\ntest-time adaptability, allowing the planner to adjust to new reward\nspecifications without retraining. We validate Diffusion-MPC on real world,\ndemonstrating strong locomotion and flexible adaptation."}
{"id": "2510.04239", "pdf": "https://arxiv.org/pdf/2510.04239", "abs": "https://arxiv.org/abs/2510.04239", "authors": ["Tongzhou Wu", "Yuhao Wang", "Maolin Wang", "Chi Zhang", "Xiangyu Zhao"], "title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted by CIKM2025", "summary": "Sequential recommendation aims to capture user preferences by modeling\nsequential patterns in user-item interactions. However, these models are often\ninfluenced by noise such as accidental interactions, leading to suboptimal\nperformance. Therefore, to reduce the effect of noise, some works propose\nexplicitly identifying and removing noisy items. However, we find that simply\nrelying on collaborative information may result in an over-denoising problem,\nespecially for cold items. To overcome these limitations, we propose a novel\nframework: Interest Alignment for Denoising Sequential Recommendation (IADSR)\nwhich integrates both collaborative and semantic information. Specifically,\nIADSR is comprised of two stages: in the first stage, we obtain the\ncollaborative and semantic embeddings of each item from a traditional\nsequential recommendation model and an LLM, respectively. In the second stage,\nwe align the collaborative and semantic embeddings and then identify noise in\nthe interaction sequence based on long-term and short-term interests captured\nin the collaborative and semantic modalities. Our extensive experiments on four\npublic datasets validate the effectiveness of the proposed framework and its\ncompatibility with different sequential recommendation systems."}
{"id": "2510.04241", "pdf": "https://arxiv.org/pdf/2510.04241", "abs": "https://arxiv.org/abs/2510.04241", "authors": ["Seong Jin Ahn", "Myoung-Ho Kim"], "title": "Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "For large-scale applications, there is growing interest in replacing Graph\nNeural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via\nknowledge distillation. However, distilling GNNs for self-supervised graph\nrepresentation learning into MLPs is more challenging. This is because the\nperformance of self-supervised learning is more related to the model's\ninductive bias than supervised learning. This motivates us to design a new\ndistillation method to bridge a huge capacity gap between GNNs and MLPs in\nself-supervised graph representation learning. In this paper, we propose\n\\textbf{D}iffusion-\\textbf{A}ssisted \\textbf{D}istillation for\n\\textbf{S}elf-supervised \\textbf{G}raph representation learning with\n\\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion\nmodel as a teacher assistant to better distill the knowledge from the teacher\nGNN into the student MLP. This approach enhances the generalizability and\nrobustness of MLPs in self-supervised graph representation learning. Extensive\nexperiments demonstrate that DAD-SGM effectively distills the knowledge of\nself-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation\nmethods. Our implementation is available at\nhttps://github.com/SeongJinAhn/DAD-SGM."}
{"id": "2510.04245", "pdf": "https://arxiv.org/pdf/2510.04245", "abs": "https://arxiv.org/abs/2510.04245", "authors": ["Ayushi Mehrotra", "Derek Peng", "Dipkamal Bhusal", "Nidhi Rastogi"], "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks", "categories": ["cs.CV", "cs.AI"], "comment": "neurips workshop", "summary": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks."}
{"id": "2510.04246", "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "categories": ["cs.RO", "cs.AI"], "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times."}
{"id": "2510.04257", "pdf": "https://arxiv.org/pdf/2510.04257", "abs": "https://arxiv.org/abs/2510.04257", "authors": ["Yanjie Li", "Yiming Cao", "Dong Wang", "Bin Xiao"], "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information\n  Forensics & Security", "summary": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."}
{"id": "2510.04263", "pdf": "https://arxiv.org/pdf/2510.04263", "abs": "https://arxiv.org/abs/2510.04263", "authors": ["Joseph Ramsey", "Bryan Andrews"], "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 23 figures, 6 tables", "summary": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery."}
{"id": "2510.04268", "pdf": "https://arxiv.org/pdf/2510.04268", "abs": "https://arxiv.org/abs/2510.04268", "authors": ["Robin Algayres", "Charles-Éric Saint-James", "Mahi Luthra", "Jiayi Shen", "Dongyan Lin", "Youssef Benchekroun", "Rashel Moritz", "Juan Pino", "Emmanuel Dupoux"], "title": "LongTail-Swap: benchmarking language models' abilities on rare words", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Children learn to speak with a low amount of data and can be taught new words\non a few-shot basis, making them particularly data-efficient learners. The\nBabyLM challenge aims at exploring language model (LM) training in the low-data\nregime but uses metrics that concentrate on the head of the word distribution.\nHere, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the\ntail of the distribution, i.e., measures the ability of LMs to learn new words\nwith very little exposure, like infants do. LT-Swap is a pretraining\ncorpus-specific test set of acceptable versus unacceptable sentence pairs that\nisolate semantic and syntactic usage of rare words. Models are evaluated in a\nzero-shot fashion by computing the average log probabilities over the two\nmembers of each pair. We built two such test sets associated with the 10M words\nand 100M words BabyLM training sets, respectively, and evaluated 16 models from\nthe BabyLM leaderboard. Our results not only highlight the poor performance of\nlanguage models on rare words but also reveal that performance differences\nacross LM architectures are much more pronounced in the long tail than in the\nhead. This offers new insights into which architectures are better at handling\nrare word generalization. We've also made the code publicly avail"}
{"id": "2510.04276", "pdf": "https://arxiv.org/pdf/2510.04276", "abs": "https://arxiv.org/abs/2510.04276", "authors": ["Joseph Ramsey", "Bryan Andrews"], "title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests", "categories": ["stat.ML", "cs.AI"], "comment": "30 pages, 11 figures, 5 tables", "summary": "Learning graphical conditional independence structures from nonlinear,\ncontinuous or mixed data is a central challenge in machine learning and the\nsciences, and many existing methods struggle to scale to thousands of samples\nor hundreds of variables. We introduce two basis-expansion tools for scalable\ncausal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated\nadditive expansions to approximate nonlinear dependencies. BF-BIC is\ntheoretically consistent under additive models and extends to post-nonlinear\n(PNL) models via an invertible reparameterization. It remains robust under\nmoderate interactions and supports mixed data through a degenerate-Gaussian\nembedding for discrete variables. In simulations with fully nonlinear neural\ncausal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods\n(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function\nLikelihood Ratio Test (BF-LRT) provides an approximate conditional independence\ntest that is substantially faster than kernel tests while retaining competitive\naccuracy. Extensive simulations and a real-data application to Canadian\nwildfire risk show that, when integrated into hybrid searches, BF-based methods\nenable interpretable and scalable causal discovery. Implementations are\navailable in Python, R, and Java."}
{"id": "2510.04280", "pdf": "https://arxiv.org/pdf/2510.04280", "abs": "https://arxiv.org/abs/2510.04280", "authors": ["Álvaro Serra-Gomez", "Daniel Jarne Ornia", "Dhruva Tirumala", "Thomas Moerland"], "title": "A KL-regularization framework for learning to plan with adaptive priors", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Preprint", "summary": "Effective exploration remains a central challenge in model-based\nreinforcement learning (MBRL), particularly in high-dimensional continuous\ncontrol tasks where sample efficiency is crucial. A prominent line of recent\nwork leverages learned policies as proposal distributions for Model-Predictive\nPath Integral (MPPI) planning. Initial approaches update the sampling policy\nindependently of the planner distribution, typically maximizing a learned value\nfunction with deterministic policy gradient and entropy regularization.\nHowever, because the states encountered during training depend on the MPPI\nplanner, aligning the sampling policy with the planner improves the accuracy of\nvalue estimation and long-term performance. To this end, recent methods update\nthe sampling policy by minimizing KL divergence to the planner distribution or\nby introducing planner-guided regularization into the policy update. In this\nwork, we unify these MPPI-based reinforcement learning methods under a single\nframework by introducing Policy Optimization-Model Predictive Control (PO-MPC),\na family of KL-regularized MBRL methods that integrate the planner's action\ndistribution as a prior in policy optimization. By aligning the learned policy\nwith the planner's behavior, PO-MPC allows more flexibility in the policy\nupdates to trade off Return maximization and KL divergence minimization. We\nclarify how prior approaches emerge as special cases of this family, and we\nexplore previously unstudied variations. Our experiments show that these\nextended configurations yield significant performance improvements, advancing\nthe state of the art in MPPI-based RL."}
{"id": "2510.04286", "pdf": "https://arxiv.org/pdf/2510.04286", "abs": "https://arxiv.org/abs/2510.04286", "authors": ["Harshil Vejendla"], "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main, 8 pages, 9 figures", "summary": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a\nsparse subset of feed-forward experts. Token-level routing, however, assigns an\nentire semantic spectrum to each expert, creating capacity bottlenecks,\nload-balancing pathologies, and limited specialization. We introduce SliceMoE,\nan architecture that routes contiguous slices of a token's hidden vector. A\nd-dimensional embedding is partitioned into S slices, and for each slice, a\nlightweight shared router predicts the top-k experts. Experts operate on their\nassigned slices independently, and outputs are reassembled, maintaining\nper-token FLOP efficiency. Because slices from different tokens interleave\nwithin an expert, utilization is naturally smoother. We propose a slice-level\ncapacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.\nExperiments on WikiText-103 language modeling, WMT En-De translation, and three\ntext-classification datasets show SliceMoE attains up to 1.7x faster inference\nthan dense baselines, 12 to 18 percent lower perplexity than parameter-matched\ntoken-MoE, and improved expert balance, with interpretable expertise over\nsyntactic versus semantic subspaces."}
{"id": "2510.04303", "pdf": "https://arxiv.org/pdf/2510.04303", "abs": "https://arxiv.org/abs/2510.04303", "authors": ["Om Tailor"], "title": "Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs", "categories": ["cs.MA", "cs.AI"], "comment": "8 pages, 0 figures", "summary": "Multi-agent deployments of large language models (LLMs) are increasingly\nembedded in market, allocation, and governance workflows, yet covert\ncoordination among agents can silently erode trust and social welfare. Existing\naudits are dominated by heuristics that lack theoretical guarantees, struggle\nto transfer across tasks, and seldom ship with the infrastructure needed for\nindependent replication. We introduce \\emph{Audit the Whisper}, a\nconference-grade research artifact that spans theory, benchmark design,\ndetection, and reproducibility. Our contributions are: (i) a channel-capacity\nanalysis showing how interventions such as paraphrase, rate limiting, and role\npermutation impose quantifiable capacity penalties -- operationalized via\npaired-run Kullback--Leibler diagnostics -- that tighten mutual-information\nthresholds with finite-sample guarantees; (ii) \\textsc{ColludeBench}-v0,\ncovering pricing, first-price auctions, and peer review with configurable\ncovert schemes, deterministic manifests, and reward instrumentation; and (iii)\na calibrated auditing pipeline that fuses cross-run mutual information,\npermutation invariance, watermark variance, and fairness-aware acceptance bias,\neach tuned to a \\(10^{-3}\\) false-positive budget. Across 600 audited runs\nspanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with\nzero observed false alarms, while ablations surface the price-of-auditing\ntrade-off and highlight fairness-driven colluders invisible to MI alone. We\nrelease regeneration scripts, seed-stamped manifests, and documentation so that\nexternal auditors can reproduce every figure and extend the framework with\nminimal effort."}
{"id": "2510.04317", "pdf": "https://arxiv.org/pdf/2510.04317", "abs": "https://arxiv.org/abs/2510.04317", "authors": ["Yucong Dai", "Lu Zhang", "Feng Luo", "Mashrur Chowdhury", "Yongkai Wu"], "title": "FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICDM 2025 Demo Workshop", "summary": "Training fair and unbiased machine learning models is crucial for high-stakes\napplications, yet it presents significant challenges. Effective bias mitigation\nrequires deep expertise in fairness definitions, metrics, data preprocessing,\nand machine learning techniques. In addition, the complex process of balancing\nmodel performance with fairness requirements while properly handling sensitive\nattributes makes fairness-aware model development inaccessible to many\npractitioners. To address these challenges, we introduce FairAgent, an\nLLM-powered automated system that significantly simplifies fairness-aware model\ndevelopment. FairAgent eliminates the need for deep technical expertise by\nautomatically analyzing datasets for potential biases, handling data\npreprocessing and feature engineering, and implementing appropriate bias\nmitigation strategies based on user requirements. Our experiments demonstrate\nthat FairAgent achieves significant performance improvements while\nsignificantly reducing development time and expertise requirements, making\nfairness-aware machine learning more accessible to practitioners."}
{"id": "2510.04339", "pdf": "https://arxiv.org/pdf/2510.04339", "abs": "https://arxiv.org/abs/2510.04339", "authors": ["Christian Limberg", "Fares Schulz", "Zhe Zhang", "Stefan Weinzierl"], "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on\n  Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com", "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com"}
{"id": "2510.04340", "pdf": "https://arxiv.org/pdf/2510.04340", "abs": "https://arxiv.org/abs/2510.04340", "authors": ["Daniel Tan", "Anders Woodruff", "Niels Warncke", "Arun Jose", "Maxime Riché", "David Demitri Africa", "Mia Taylor"], "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time", "categories": ["cs.CL", "cs.AI"], "comment": "40 pages, 22 figures In proceedings at ICLR 2026", "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize."}
{"id": "2510.04341", "pdf": "https://arxiv.org/pdf/2510.04341", "abs": "https://arxiv.org/abs/2510.04341", "authors": ["G. Niklas Noren", "Eva-Lisa Meldau", "Johan Ellenius"], "title": "Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies", "categories": ["cs.LG", "cs.AI", "I.2.0"], "comment": "28 pages, 2 figures", "summary": "Many high-stakes AI applications target low-prevalence events, where apparent\naccuracy can conceal limited real-world value. Relevant AI models range from\nexpert-defined rules and traditional machine learning to generative LLMs\nconstrained for classification. We outline key considerations for critical\nappraisal of AI in rare-event recognition, including problem framing and test\nset design, prevalence-aware statistical evaluation, robustness assessment, and\nintegration into human workflows. In addition, we propose an approach to\nstructured case-level examination (SCLE), to complement statistical performance\nevaluation, and a comprehensive checklist to guide procurement or development\nof AI models for rare-event recognition. We instantiate the framework in\npharmacovigilance, drawing on three studies: rule-based retrieval of\npregnancy-related reports; duplicate detection combining machine learning with\nprobabilistic record linkage; and automated redaction of person names using an\nLLM. We highlight pitfalls specific to the rare-event setting including\noptimism from unrealistic class balance and lack of difficult positive controls\nin test sets - and show how cost-sensitive targets align model performance with\noperational value. While grounded in pharmacovigilance practice, the principles\ngeneralize to domains where positives are scarce and error costs may be\nasymmetric."}
{"id": "2510.04349", "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop."}
{"id": "2510.04354", "pdf": "https://arxiv.org/pdf/2510.04354", "abs": "https://arxiv.org/abs/2510.04354", "authors": ["Apurva Badithela", "David Snyder", "Lihan Zha", "Joseph Mikhail", "Matthew O'Kelly", "Anushri Dixit", "Anirudha Majumdar"], "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance."}
{"id": "2510.04363", "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation."}
{"id": "2510.04368", "pdf": "https://arxiv.org/pdf/2510.04368", "abs": "https://arxiv.org/abs/2510.04368", "authors": ["Shashank Mangla", "Chris Hokamp", "Jack Boylan", "Demian Gholipour Ghalandari", "Yuuv Jauhari", "Lauren Cassidy", "Oisin Duffy"], "title": "NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment", "categories": ["cs.MA", "cs.AI"], "comment": "SocialSim Workshop at COLM 2025", "summary": "We design and implement NegotiationGym, an API and user interface for\nconfiguring and running multi-agent social simulations focused upon negotiation\nand cooperation. The NegotiationGym codebase offers a user-friendly,\nconfiguration-driven API that enables easy design and customization of\nsimulation scenarios. Agent-level utility functions encode optimization\ncriteria for each agent, and agents can self-optimize by conducting multiple\ninteraction rounds with other agents, observing outcomes, and modifying their\nstrategies for future rounds."}
{"id": "2510.04374", "pdf": "https://arxiv.org/pdf/2510.04374", "abs": "https://arxiv.org/abs/2510.04374", "authors": ["Tejal Patwardhan", "Rachel Dias", "Elizabeth Proehl", "Grace Kim", "Michele Wang", "Olivia Watkins", "Simón Posada Fishman", "Marwan Aljubeh", "Phoebe Thacker", "Laurance Fauconnet", "Natalie S. Kim", "Patrick Chao", "Samuel Miserendino", "Gildas Chabot", "David Li", "Michael Sharman", "Alexandra Barr", "Amelia Glaese", "Jerry Tworek"], "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "We introduce GDPval, a benchmark evaluating AI model capabilities on\nreal-world economically valuable tasks. GDPval covers the majority of U.S.\nBureau of Labor Statistics Work Activities for 44 occupations across the top 9\nsectors contributing to U.S. GDP (Gross Domestic Product). Tasks are\nconstructed from the representative work of industry professionals with an\naverage of 14 years of experience. We find that frontier model performance on\nGDPval is improving roughly linearly over time, and that the current best\nfrontier models are approaching industry experts in deliverable quality. We\nanalyze the potential for frontier models, when paired with human oversight, to\nperform GDPval tasks cheaper and faster than unaided experts. We also\ndemonstrate that increased reasoning effort, increased task context, and\nincreased scaffolding improves model performance on GDPval. Finally, we\nopen-source a gold subset of 220 tasks and provide a public automated grading\nservice at evals.openai.com to facilitate future research in understanding\nreal-world model capabilities."}
{"id": "2510.04375", "pdf": "https://arxiv.org/pdf/2510.04375", "abs": "https://arxiv.org/abs/2510.04375", "authors": ["Akshay Mittal", "Vinay Venkatesh", "Krishna Kandi", "Shalini Sudarshan"], "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The effectiveness of single-model sequential recommendation architectures,\nwhile scalable, is often limited when catering to \"power users\" in sparse or\nniche domains. Our previous research, PinnerFormerLite, addressed this by using\na fixed weighted loss to prioritize specific domains. However, this approach\ncan be sub-optimal, as a single, uniform weight may not be sufficient for\ndomains with very few interactions, where the training signal is easily diluted\nby the vast, generic dataset.\n  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss\nfunction with comprehensive theoretical foundations and extensive empirical\nvalidation. We introduce an adaptive algorithm that adjusts the loss weight for\neach domain based on its sparsity in the training data, assigning a higher\nweight to sparser domains and a lower weight to denser ones. This ensures that\neven rare user interests contribute a meaningful gradient signal, preventing\nthem from being overshadowed.\n  We provide rigorous theoretical analysis including convergence proofs,\ncomplexity analysis, and bounds analysis to establish the stability and\nefficiency of our approach. Our comprehensive empirical validation across four\ndiverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)\nwith state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that\nthis dynamic weighting system significantly outperforms all comparison methods,\nparticularly for sparse domains, achieving substantial lifts in key metrics\nlike Recall at 10 and NDCG at 10 while maintaining performance on denser\ndomains and introducing minimal computational overhead."}
{"id": "2510.04380", "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko Mäkitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development."}
{"id": "2510.04390", "pdf": "https://arxiv.org/pdf/2510.04390", "abs": "https://arxiv.org/abs/2510.04390", "authors": ["Xuehai He", "Shijie Zhou", "Thivyanth Venkateswaran", "Kaizhi Zheng", "Ziyu Wan", "Achuta Kadambi", "Xin Eric Wang"], "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D."}
{"id": "2510.04392", "pdf": "https://arxiv.org/pdf/2510.04392", "abs": "https://arxiv.org/abs/2510.04392", "authors": ["Faisal Hamman", "Chenyang Zhu", "Anoop Kumar", "Xujun Peng", "Sanghamitra Dutta", "Daben Liu", "Alfy Samuel"], "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted at NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "RAG systems are increasingly deployed in high-stakes domains where users\nexpect outputs to be consistent across semantically equivalent queries.\nHowever, existing systems often exhibit significant inconsistencies due to\nvariability in both the retriever and generator (LLM), undermining trust and\nreliability. In this work, we focus on information consistency, i.e., the\nrequirement that outputs convey the same core content across semantically\nequivalent inputs. We introduce a principled evaluation framework that\ndecomposes RAG consistency into retriever-level, generator-level, and\nend-to-end components, helping identify inconsistency sources. To improve\nconsistency, we propose Paraphrased Set Group Relative Policy Optimization\n(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased\nset to assign group similarity rewards. We leverage PS-GRPO to achieve\nInformation Consistent RAG (Con-RAG), training the generator to produce\nconsistent outputs across paraphrased queries and remain robust to\nretrieval-induced variability. Because exact reward computation over paraphrase\nsets is computationally expensive, we also introduce a scalable approximation\nmethod that retains effectiveness while enabling efficient, large-scale\ntraining. Empirical evaluations across short-form, multi-hop, and long-form QA\nbenchmarks demonstrate that Con-RAG significantly improves both consistency and\naccuracy over strong baselines, even in the absence of explicit ground-truth\nsupervision. Our work provides practical solutions for evaluating and building\nreliable RAG systems for safety-critical deployments."}
{"id": "2510.04397", "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods."}
{"id": "2510.04398", "pdf": "https://arxiv.org/pdf/2510.04398", "abs": "https://arxiv.org/abs/2510.04398", "authors": ["Buyun Liang", "Liangzu Peng", "Jinqi Luo", "Darshan Thaker", "Kwan Ho Ryan Chan", "René Vidal"], "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Accepted at NeurIPS 2025. Code is available at\n  https://github.com/Buyun-Liang/SECA", "summary": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA."}
{"id": "2510.04400", "pdf": "https://arxiv.org/pdf/2510.04400", "abs": "https://arxiv.org/abs/2510.04400", "authors": ["Marc Cavazza"], "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we explore the relevance of textual semantics to Large Language\nModels (LLMs), extending previous insights into the connection between\ndistributional semantics and structural semantics. We investigate whether\nLLM-generated texts preserve semantic isotopies. We design a story continuation\nexperiment using 10,000 ROCStories prompts completed by five LLMs. We first\nvalidate GPT-4o's ability to extract isotopies from a linguistic benchmark,\nthen apply it to the generated stories. We then analyze structural (coverage,\ndensity, spread) and semantic properties of isotopies to assess how they are\naffected by completion. Results show that LLM completion within a given token\nhorizon preserves semantic isotopies across multiple properties."}
{"id": "2510.04401", "pdf": "https://arxiv.org/pdf/2510.04401", "abs": "https://arxiv.org/abs/2510.04401", "authors": ["Xuyang Guo", "Zekai Huang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research."}
{"id": "2510.04417", "pdf": "https://arxiv.org/pdf/2510.04417", "abs": "https://arxiv.org/abs/2510.04417", "authors": ["Wenyuan Zhao", "Adithya Balachandran", "Chao Tian", "Paul Pu Liang"], "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IT", "math.IT"], "comment": "NeurIPS 2025", "summary": "The study of multimodality has garnered significant interest in fields where\nthe analysis of interactions among multiple information sources can enhance\npredictive modeling, data fusion, and interpretability. Partial information\ndecomposition (PID) has emerged as a useful information-theoretic framework to\nquantify the degree to which individual modalities independently, redundantly,\nor synergistically convey information about a target variable. However,\nexisting PID methods depend on optimizing over a joint distribution constrained\nby estimated pairwise probability distributions, which are costly and\ninaccurate for continuous and high-dimensional modalities. Our first key\ninsight is that the problem can be solved efficiently when the pairwise\ndistributions are multivariate Gaussians, and we refer to this problem as\nGaussian PID (GPID). We propose a new gradient-based algorithm that\nsubstantially improves the computational efficiency of GPID based on an\nalternative formulation of the underlying optimization problem. To generalize\nthe applicability to non-Gaussian data, we learn information-preserving\nencoders to transform random variables of arbitrary input distributions into\npairwise Gaussian random variables. Along the way, we resolved an open problem\nregarding the optimality of joint Gaussian solutions for GPID. Empirical\nvalidation in diverse synthetic examples demonstrates that our proposed method\nprovides more accurate and efficient PID estimates than existing baselines. We\nfurther evaluate a series of large-scale multimodal benchmarks to show its\nutility in real-world applications of quantifying PID in multimodal datasets\nand selecting high-performing models."}
{"id": "2510.04455", "pdf": "https://arxiv.org/pdf/2510.04455", "abs": "https://arxiv.org/abs/2510.04455", "authors": ["Akira Kitaoka"], "title": "Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions", "categories": ["math.OC", "cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "33 pages", "summary": "In mixed-integer linear programming, data-driven inverse optimization that\nlearns the objective function and the constraints from observed data plays an\nimportant role in constructing appropriate mathematical models for various\nfields, including power systems and scheduling. However, to the best of our\nknowledge, there is no known method for learning both the objective functions\nand the constraints. In this paper, we propose a two-stage method for a class\nof problems where the objective function is expressed as a linear combination\nof functions and the constraints are represented by functions and thresholds.\nSpecifically, our method first learns the constraints and then learns the\nobjective function. On the theoretical side, we show the proposed method can\nsolve inverse optimization problems in finite dataset, develop statistical\nlearning theory in pseudometric spaces and sub-Gaussian distributions, and\nconstruct a statistical learning for inverse optimization. On the experimental\nside, we demonstrate that our method is practically applicable for scheduling\nproblems formulated as integer linear programmings with up to 100 decision\nvariables, which are typical in real-world settings."}
{"id": "2510.04465", "pdf": "https://arxiv.org/pdf/2510.04465", "abs": "https://arxiv.org/abs/2510.04465", "authors": ["Zhiping Zhang", "Yi Evie Zhang", "Freda Shi", "Tianshi Li"], "title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents", "categories": ["cs.HC", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Model (LLM) agents require personal information for\npersonalization in order to better act on users' behalf in daily tasks, but\nthis raises privacy concerns and a personalization-privacy dilemma. Agent's\nautonomy introduces both risks and opportunities, yet its effects remain\nunclear. To better understand this, we conducted a 3$\\times$3 between-subjects\nexperiment ($N=450$) to study how agent's autonomy level and personalization\ninfluence users' privacy concerns, trust and willingness to use, as well as the\nunderlying psychological processes. We find that personalization without\nconsidering users' privacy preferences increases privacy concerns and decreases\ntrust and willingness to use. Autonomy moderates these effects: Intermediate\nautonomy flattens the impact of personalization compared to No- and Full\nautonomy conditions. Our results suggest that rather than aiming for perfect\nmodel alignment in output generation, balancing autonomy of agent's action and\nuser control offers a promising path to mitigate the personalization-privacy\ndilemma."}
{"id": "2510.04472", "pdf": "https://arxiv.org/pdf/2510.04472", "abs": "https://arxiv.org/abs/2510.04472", "authors": ["Baber Jan", "Saeed Anwar", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais"], "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}."}
{"id": "2510.04476", "pdf": "https://arxiv.org/pdf/2510.04476", "abs": "https://arxiv.org/abs/2510.04476", "authors": ["Tomas Figliolia", "Nicholas Alonso", "Rishi Iyer", "Quentin Anthony", "Beren Millidge"], "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."}
{"id": "2510.04477", "pdf": "https://arxiv.org/pdf/2510.04477", "abs": "https://arxiv.org/abs/2510.04477", "authors": ["Soo Yong Kim", "Suin Cho", "Vincent-Daniel Yun", "Gyeongyeon Hwang"], "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models."}
{"id": "2510.04484", "pdf": "https://arxiv.org/pdf/2510.04484", "abs": "https://arxiv.org/abs/2510.04484", "authors": ["Amin Banayeeanzade", "Ala N. Tak", "Fatemeh Bahrani", "Anahita Bolourani", "Leonardo Blas", "Emilio Ferrara", "Jonathan Gratch", "Sai Praneeth Karimireddy"], "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to ARR - October 2025", "summary": "The ability to control LLMs' emulated emotional states and personality traits\nis essential for enabling rich, human-centered interactions in socially\ninteractive settings. We introduce PsySET, a Psychologically-informed benchmark\nto evaluate LLM Steering Effectiveness and Trustworthiness across the emotion\nand personality domains. Our study spans four models from different LLM\nfamilies paired with various steering strategies, including prompting,\nfine-tuning, and representation engineering. Our results indicate that\nprompting is consistently effective but limited in intensity control, whereas\nvector injections achieve finer controllability while slightly reducing output\nquality. Moreover, we explore the trustworthiness of steered LLMs by assessing\nsafety, truthfulness, fairness, and ethics, highlighting potential side effects\nand behavioral shifts. Notably, we observe idiosyncratic effects; for instance,\neven a positive emotion like joy can degrade robustness to adversarial\nfactuality, lower privacy awareness, and increase preferential bias. Meanwhile,\nanger predictably elevates toxicity yet strengthens leakage resistance. Our\nframework establishes the first holistic evaluation of emotion and personality\nsteering, offering insights into its interpretability and reliability for\nsocially interactive applications."}
{"id": "2510.04498", "pdf": "https://arxiv.org/pdf/2510.04498", "abs": "https://arxiv.org/abs/2510.04498", "authors": ["Qiao Wang", "Adnan Labib", "Robert Swier", "Michael Hofmeyr", "Zheng Yuan"], "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners", "categories": ["cs.CL", "cs.AI"], "comment": "Workshop on Wordplay: When Language Meets Games, EMNLP 2025", "summary": "GenQuest is a generative text adventure game that leverages Large Language\nModels (LLMs) to facilitate second language learning through immersive,\ninteractive storytelling. The system engages English as a Foreign Language\n(EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative,\ndynamically generated in response to learner choices. Game mechanics such as\nbranching decision points and story milestones are incorporated to maintain\nnarrative coherence while allowing learner-driven plot development. Key\npedagogical features include content generation tailored to each learner's\nproficiency level, and a vocabulary assistant that provides in-context\nexplanations of learner-queried text strings, ranging from words and phrases to\nsentences. Findings from a pilot study with university EFL students in China\nindicate promising vocabulary gains and positive user perceptions. Also\ndiscussed are suggestions from participants regarding the narrative length and\nquality, and the request for multi-modal content such as illustrations."}
{"id": "2510.04503", "pdf": "https://arxiv.org/pdf/2510.04503", "abs": "https://arxiv.org/abs/2510.04503", "authors": ["Shuai Zhao", "Xinyi Wu", "Shiqian Zhao", "Xiaobao Wu", "Zhongliang Guo", "Yanhao Jia", "Anh Tuan Luu"], "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."}
{"id": "2510.04506", "pdf": "https://arxiv.org/pdf/2510.04506", "abs": "https://arxiv.org/abs/2510.04506", "authors": ["Jiashuo Sun", "Shixuan Liu", "Zhaochen Su", "Xianrui Zhong", "Pengcheng Jiang", "Bowen Jin", "Peiran Li", "Weijia Shi", "Jiawei Han"], "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "23 pages, 7 figures, 7 tables", "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE."}
{"id": "2510.04522", "pdf": "https://arxiv.org/pdf/2510.04522", "abs": "https://arxiv.org/abs/2510.04522", "authors": ["Yisen Gao", "Xingcheng Fu", "Qingyun Sun", "Jianxin Li", "Xianxian Li"], "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by NeuIPS 2025", "summary": "Graph diffusion models have made significant progress in learning structured\ngraph data and have demonstrated strong potential for predictive tasks.\nExisting approaches typically embed node, edge, and graph-level features into a\nunified latent space, modeling prediction tasks including classification and\nregression as a form of conditional generation. However, due to the\nnon-Euclidean nature of graph data, features of different curvatures are\nentangled in the same latent space without releasing their geometric potential.\nTo address this issue, we aim to construt an ideal Riemannian diffusion model\nto capture distinct manifold signatures of complex graph data and learn their\ndistribution. This goal faces two challenges: numerical instability caused by\nexponential mapping during the encoding proces and manifold deviation during\ndiffusion generation. To address these challenges, we propose GeoMancer: a\nnovel Riemannian graph diffusion framework for both generation and prediction\ntasks. To mitigate numerical instability, we replace exponential mapping with\nan isometric-invariant Riemannian gyrokernel approach and decouple multi-level\nfeatures onto their respective task-specific manifolds to learn optimal\nrepresentations. To address manifold deviation, we introduce a\nmanifold-constrained diffusion method and a self-guided strategy for\nunconditional generation, ensuring that the generated data remains aligned with\nthe manifold signature. Extensive experiments validate the effectiveness of our\napproach, demonstrating superior performance across a variety of tasks."}
{"id": "2510.04528", "pdf": "https://arxiv.org/pdf/2510.04528", "abs": "https://arxiv.org/abs/2510.04528", "authors": ["Santhosh KumarRavindran"], "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration."}
{"id": "2510.04536", "pdf": "https://arxiv.org/pdf/2510.04536", "abs": "https://arxiv.org/abs/2510.04536", "authors": ["Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Satoshi Ohshima", "Takahiro Katagiri"], "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources."}
{"id": "2510.04567", "pdf": "https://arxiv.org/pdf/2510.04567", "abs": "https://arxiv.org/abs/2510.04567", "authors": ["Weishuo Ma", "Yanbo Wang", "Xiyuan Wang", "Lei Zou", "Muhan Zhang"], "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are powerful tools for precessing relational\ndata but often struggle to generalize to unseen graphs, giving rise to the\ndevelopment of Graph Foundational Models (GFMs). However, current GFMs are\nchallenged by the extreme heterogeneity of graph data, where each graph can\npossess a unique feature space, label set, and topology. To address this, two\nmain paradigms have emerged. The first leverages Large Language Models (LLMs),\nbut is fundamentally text-dependent, thus struggles to handle the numerical\nfeatures in vast graphs. The second pre-trains a structure-based model, but the\nadaptation to new tasks typically requires a costly, per-graph tuning stage,\ncreating a critical efficiency bottleneck. In this work, we move beyond these\nlimitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning\n\\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free\narchitecture. GILT introduces a novel token-based framework for in-context\nlearning (ICL) on graphs, reframing classification tasks spanning node, edge\nand graph levels in a unified framework. This mechanism is the key to handling\nheterogeneity, as it is designed to operate on generic numerical features.\nFurther, its ability to understand class semantics dynamically from the context\nenables tuning-free adaptation. Comprehensive experiments show that GILT\nachieves stronger few-shot performance with significantly less time than\nLLM-based or tuning-based baselines, validating the effectiveness of our\napproach."}
{"id": "2510.04573", "pdf": "https://arxiv.org/pdf/2510.04573", "abs": "https://arxiv.org/abs/2510.04573", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Nicklas Majamaki", "Navdeep Jaitly", "Yi-An Ma", "Lianhui Qin"], "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion."}
{"id": "2510.04574", "pdf": "https://arxiv.org/pdf/2510.04574", "abs": "https://arxiv.org/abs/2510.04574", "authors": ["Wenchao He", "Tao Jia"], "title": "Deep learning framework for predicting stochastic take-off and die-out of early spreading", "categories": ["cs.SI", "cs.AI", "physics.soc-ph", "05C82, 68T05, 92C42", "G.2.2; I.2.6"], "comment": "29 pages, 11 figures", "summary": "Large-scale outbreaks of epidemics, misinformation, or other harmful\ncontagions pose significant threats to human society, yet the fundamental\nquestion of whether an emerging outbreak will escalate into a major epidemic or\nnaturally die out remains largely unaddressed. This problem is challenging,\npartially due to inadequate data during the early stages of outbreaks and also\nbecause established models focus on average behaviors of large epidemics rather\nthan the stochastic nature of small transmission chains. Here, we introduce the\nfirst systematic framework for forecasting whether initial transmission events\nwill amplify into major outbreaks or fade into extinction during early stages,\nwhen intervention strategies can still be effectively implemented. Using\nextensive data from stochastic spreading models, we developed a deep learning\nframework that predicts early-stage spreading outcomes in real-time. Validation\nacross Erd\\H{o}s-R\\'enyi and Barab\\'asi-Albert networks with varying\ninfectivity levels shows our method accurately forecasts stochastic spreading\nevents well before potential outbreaks, demonstrating robust performance across\ndifferent network structures and infectivity scenarios.To address the challenge\nof sparse data during early outbreak stages, we further propose a\npretrain-finetune framework that leverages diverse simulation data for\npretraining and adapts to specific scenarios through targeted fine-tuning. The\npretrain-finetune framework consistently outperforms baseline models, achieving\nsuperior performance even when trained on limited scenario-specific data. To\nour knowledge, this work presents the first framework for predicting stochastic\ntake-off versus die-out. This framework provides valuable insights for epidemic\npreparedness and public health decision-making, enabling more informed early\nintervention strategies."}
{"id": "2510.04576", "pdf": "https://arxiv.org/pdf/2510.04576", "abs": "https://arxiv.org/abs/2510.04576", "authors": ["Yuhta Takida", "Satoshi Hayakawa", "Takashi Shibuya", "Masaaki Imaizumi", "Naoki Murata", "Bac Nguyen", "Toshimitsu Uesaka", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "24 pages with 9 figures", "summary": "Deep generative models have made significant advances in generating complex\ncontent, yet conditional generation remains a fundamental challenge. Existing\nconditional generative adversarial networks often struggle to balance the dual\nobjectives of assessing authenticity and conditional alignment of input samples\nwithin their conditional discriminators. To address this, we propose a novel\ndiscriminator design that integrates three key capabilities: unconditional\ndiscrimination, matching-aware supervision to enhance alignment sensitivity,\nand adaptive weighting to dynamically balance all objectives. Specifically, we\nintroduce Sum of Naturalness and Alignment (SONA), which employs separate\nprojections for naturalness (authenticity) and alignment in the final layer\nwith an inductive bias, supported by dedicated objective functions and an\nadaptive weighting mechanism. Extensive experiments on class-conditional\ngeneration tasks show that \\ours achieves superior sample quality and\nconditional alignment compared to state-of-the-art methods. Furthermore, we\ndemonstrate its effectiveness in text-to-image generation, confirming the\nversatility and robustness of our approach."}
{"id": "2510.04602", "pdf": "https://arxiv.org/pdf/2510.04602", "abs": "https://arxiv.org/abs/2510.04602", "authors": ["Eduardo Fernandes Montesuma", "Yassir Bendou", "Mike Gartrell"], "title": "Computing Wasserstein Barycenters through Gradient Flows", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "4 Figures, 3 Tables, under review", "summary": "Wasserstein barycenters provide a powerful tool for aggregating probability\nmeasures, while leveraging the geometry of their ambient space. Existing\ndiscrete methods suffer from poor scalability, as they require access to the\ncomplete set of samples from input measures. We address this issue by recasting\nthe original barycenter problem as a gradient flow in the Wasserstein space.\nOur approach offers two advantages. First, we achieve scalability by sampling\nmini-batches from the input measures. Second, we incorporate functionals over\nprobability measures, which regularize the barycenter problem through internal,\npotential, and interaction energies. We present two algorithms for empirical\nand Gaussian mixture measures, providing convergence guarantees under the\nPolyak-{\\L}ojasiewicz inequality. Experimental validation on toy datasets and\ndomain adaptation benchmarks show that our methods outperform previous discrete\nand neural net-based methods for computing Wasserstein barycenters."}
{"id": "2510.04607", "pdf": "https://arxiv.org/pdf/2510.04607", "abs": "https://arxiv.org/abs/2510.04607", "authors": ["Yuan Wang", "Mingyu Li", "Haibo Chen"], "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents", "categories": ["cs.OS", "cs.AI", "cs.LG"], "comment": null, "summary": "Computer-use agents (CUAs) powered by large language models (LLMs) have\nemerged as a promising approach to automating computer tasks, yet they struggle\nwith graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to\ndecompose high-level goals into lengthy, error-prone sequences of fine-grained\nactions, resulting in low success rates and an excessive number of LLM calls.\n  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms\nexisting GUIs into three declarative primitives: access, state, and\nobservation, which are better suited for LLMs. Our key idea is policy-mechanism\nseparation: LLMs focus on high-level semantic planning (policy) while GOI\nhandles low-level navigation and interaction (mechanism). GOI does not require\nmodifying the application source code or relying on application programming\ninterfaces (APIs).\n  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on\nWindows. Compared to a leading GUI-based agent baseline, GOI improves task\nsuccess rates by 67% and reduces interaction steps by 43.5%. Notably, GOI\ncompletes over 61% of successful tasks with a single LLM call."}
{"id": "2510.04609", "pdf": "https://arxiv.org/pdf/2510.04609", "abs": "https://arxiv.org/abs/2510.04609", "authors": ["Shreya Chappidi", "Jennifer Cobbe", "Chris Norval", "Anjali Mazumder", "Jatinder Singh"], "title": "Accountability Capture: How Record-Keeping to Support AI Transparency and Accountability (Re)shapes Algorithmic Oversight", "categories": ["cs.CY", "cs.AI"], "comment": "To appear at 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "Accountability regimes typically encourage record-keeping to enable the\ntransparency that supports oversight, investigation, contestation, and redress.\nHowever, implementing such record-keeping can introduce considerations, risks,\nand consequences, which so far remain under-explored. This paper examines how\nrecord-keeping practices bring algorithmic systems within accountability\nregimes, providing a basis to observe and understand their effects. For this,\nwe introduce, describe, and elaborate 'accountability capture' -- the\nre-configuration of socio-technical processes and the associated downstream\neffects relating to record-keeping for algorithmic accountability. Surveying\n100 practitioners, we evidence and characterise record-keeping issues in\npractice, identifying their alignment with accountability capture. We further\ndocument widespread record-keeping practices, tensions between internal and\nexternal accountability requirements, and evidence of employee resistance to\npractices imposed through accountability capture. We discuss these and other\neffects for surveillance, privacy, and data protection, highlighting\nconsiderations for algorithmic accountability communities. In all, we show that\nimplementing record-keeping to support transparency in algorithmic\naccountability regimes can itself bring wider implications -- an issue\nrequiring greater attention from practitioners, researchers, and policymakers\nalike."}
{"id": "2510.04615", "pdf": "https://arxiv.org/pdf/2510.04615", "abs": "https://arxiv.org/abs/2510.04615", "authors": ["X. Tao", "P. Chen", "M. Tsami", "F. Khayati", "M. Eckert"], "title": "Design Process of a Self Adaptive Smart Serious Games Ecosystem", "categories": ["eess.SY", "cs.AI", "cs.SY", "I.2.1"], "comment": null, "summary": "This paper outlines the design vision and planned evolution of Blexer v3, a\nmodular and AI-driven rehabilitation ecosystem based on serious games. Building\non insights from previous versions of the system, we propose a new architecture\nthat aims to integrate multimodal sensing, real-time reasoning, and intelligent\ncontrol. The envisioned system will include distinct modules for data\ncollection, user state inference, and gameplay adaptation. Key features such as\ndynamic difficulty adjustment (DDA) and procedural content generation (PCG) are\nalso considered to support personalized interventions. We present the complete\nconceptual framework of Blexer v3, which defines the modular structure and data\nflow of the system. This serves as the foundation for the next phase: the\ndevelopment of a functional prototype and its integration into clinical\nrehabilitation scenarios."}
{"id": "2510.04618", "pdf": "https://arxiv.org/pdf/2510.04618", "abs": "https://arxiv.org/abs/2510.04618", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead."}
{"id": "2510.04624", "pdf": "https://arxiv.org/pdf/2510.04624", "abs": "https://arxiv.org/abs/2510.04624", "authors": ["Eugene Lim", "Tzeh Yuan Neoh", "Nicholas Teh"], "title": "Fairness in Repeated Matching: A Maximin Perspective", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.MA", "econ.TH"], "comment": null, "summary": "We study a sequential decision-making model where a set of items is\nrepeatedly matched to the same set of agents over multiple rounds. The\nobjective is to determine a sequence of matchings that either maximizes the\nutility of the least advantaged agent at the end of all rounds (optimal) or at\nthe end of every individual round (anytime optimal). We investigate the\ncomputational challenges associated with finding (anytime) optimal outcomes and\ndemonstrate that these problems are generally computationally intractable.\nHowever, we provide approximation algorithms, fixed-parameter tractable\nalgorithms, and identify several special cases whereby the problem(s) can be\nsolved efficiently. Along the way, we also establish characterizations of\nPareto-optimal/maximum matchings, which may be of independent interest to works\nin matching theory and house allocation."}
{"id": "2510.04630", "pdf": "https://arxiv.org/pdf/2510.04630", "abs": "https://arxiv.org/abs/2510.04630", "authors": ["Vrushank Ahire", "Aniruddh Muley", "Shivam Zample", "Siddharth Verma", "Pranav Menon", "Surbhi Madan", "Abhinav Dhall"], "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications."}
{"id": "2510.04646", "pdf": "https://arxiv.org/pdf/2510.04646", "abs": "https://arxiv.org/abs/2510.04646", "authors": ["Johanna Sommer", "John Rachwan", "Nils Fleischmann", "Stephan Günnemann", "Bertrand Charpentier"], "title": "Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025", "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."}
{"id": "2510.04667", "pdf": "https://arxiv.org/pdf/2510.04667", "abs": "https://arxiv.org/abs/2510.04667", "authors": ["Fanzhe Fu", "Yang Yang"], "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8"], "comment": "9pages, 6 figures", "summary": "Reversible Instance Normalization (RevIN) is a key technique enabling simple\nlinear models to achieve state-of-the-art performance in time series\nforecasting. While replacing its non-robust statistics with robust counterparts\n(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal\na far more complex reality. This paper deconstructs the perplexing performance\nof various normalization strategies by identifying four underlying theoretical\ncontradictions. Our experiments provide two crucial findings: first, the\nstandard RevIN catastrophically fails on datasets with extreme outliers, where\nits MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN\nprevents this failure and unexpectedly emerges as the best overall performer,\nour adaptive model (A-IN), designed to test a diagnostics-driven heuristic,\nunexpectedly suffers a complete and systemic failure. This surprising outcome\nuncovers a critical, overlooked pitfall in time series analysis: the\ninstability introduced by a simple or counter-intuitive heuristic can be more\ndamaging than the statistical issues it aims to solve. The core contribution of\nthis work is thus a new, cautionary paradigm for time series normalization: a\nshift from a blind search for complexity to a diagnostics-driven analysis that\nreveals not only the surprising power of simple baselines but also the perilous\nnature of naive adaptation."}
{"id": "2510.04671", "pdf": "https://arxiv.org/pdf/2510.04671", "abs": "https://arxiv.org/abs/2510.04671", "authors": ["Chao Liu", "Ling Luo", "Tengxiao Lv", "Huan Zhuang", "Lejing Yu", "Jian Wang", "Hongfei Lin"], "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a regular paper at BIBM2025", "summary": "With the rapid development of online medical platforms, consumer health\nquestions (CHQs) are inefficient in diagnosis due to redundant information and\nfrequent non-professional terms. The medical question summary (MQS) task aims\nto transform CHQs into streamlined doctors' frequently asked questions (FAQs),\nbut existing methods still face challenges such as poor identification of\nquestion focus and model hallucination. This paper explores the potential of\nlarge language models (LLMs) in the MQS task and finds that direct fine-tuning\nis prone to focus identification bias and generates unfaithful content. To this\nend, we propose an optimization framework based on core focus guidance. First,\na prompt template is designed to drive the LLMs to extract the core focus from\nthe CHQs that is faithful to the original text. Then, a fine-tuning dataset is\nconstructed in combination with the original CHQ-FAQ pairs to improve the\nability to identify the focus of the question. Finally, a multi-dimensional\nquality evaluation and selection mechanism is proposed to comprehensively\nimprove the quality of the summary from multiple dimensions. We conduct\ncomprehensive experiments on two widely-adopted MQS datasets using three\nestablished evaluation metrics. The proposed framework achieves\nstate-of-the-art performance across all measures, demonstrating a significant\nboost in the model's ability to identify critical focus of questions and a\nnotable mitigation of hallucinations. The source codes are freely available at\nhttps://github.com/DUT-LiuChao/FocusMed."}
{"id": "2510.04674", "pdf": "https://arxiv.org/pdf/2510.04674", "abs": "https://arxiv.org/abs/2510.04674", "authors": ["Lorenzo Pannacci", "Simone Fiorellino", "Mario Edoardo Pandolfo", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": "Proceedings of IEEE Globecom 2025 Workshops", "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks."}
{"id": "2510.04682", "pdf": "https://arxiv.org/pdf/2510.04682", "abs": "https://arxiv.org/abs/2510.04682", "authors": ["Chanjoo Jung", "Jaehyung Kim"], "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall."}
{"id": "2510.04686", "pdf": "https://arxiv.org/pdf/2510.04686", "abs": "https://arxiv.org/abs/2510.04686", "authors": ["Chenxiang Zhang", "Alexander Theus", "Damien Teney", "Antonio Orvieto", "Jun Pang", "Sjouke Mauw"], "title": "How does the optimizer implicitly bias the model merging loss landscape?", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Model merging methods combine models with different capabilities into a\nsingle one while maintaining the same inference cost. Two popular approaches\nare linear interpolation, which linearly interpolates between model weights,\nand task arithmetic, which combines task vectors obtained by the difference\nbetween finetuned and base models. While useful in practice, what properties\nmake merging effective are poorly understood. This paper explores how the\noptimization process affects the loss landscape geometry and its impact on\nmerging success. We show that a single quantity -- the effective noise scale --\nunifies the impact of optimizer and data choices on model merging. Across\narchitectures and datasets, the effectiveness of merging success is a\nnon-monotonic function of effective noise, with a distinct optimum. Decomposing\nthis quantity, we find that larger learning rates, stronger weight decay,\nsmaller batch sizes, and data augmentation all independently modulate the\neffective noise scale, exhibiting the same qualitative trend. Unlike prior work\nthat connects optimizer noise to the flatness or generalization of individual\nminima, we show that it also affects the global loss landscape, predicting when\nindependently trained solutions can be merged. Our findings broaden the\nunderstanding of how optimization shapes the loss landscape geometry and its\ndownstream consequences for model merging, suggesting the possibility of\nfurther manipulating the training dynamics to improve merging effectiveness."}
{"id": "2510.04692", "pdf": "https://arxiv.org/pdf/2510.04692", "abs": "https://arxiv.org/abs/2510.04692", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement."}
{"id": "2510.04694", "pdf": "https://arxiv.org/pdf/2510.04694", "abs": "https://arxiv.org/abs/2510.04694", "authors": ["Lucas Bandarkar", "Chenyuan Yang", "Mohsen Fayyaz", "Junlin Hu", "Nanyun Peng"], "title": "Multilingual Routing in Mixture-of-Experts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages."}
{"id": "2510.04698", "pdf": "https://arxiv.org/pdf/2510.04698", "abs": "https://arxiv.org/abs/2510.04698", "authors": ["Xin Tong", "Thi Thu Uyen Hoang", "Xue-Xin Wei", "Michael Hahn"], "title": "The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities", "categories": ["q-bio.NC", "cs.AI", "econ.TH"], "comment": null, "summary": "Understanding the representation of probability in the human mind has been of\ngreat interest to understanding human decision making. Classical paradoxes in\ndecision making suggest that human perception distorts probability magnitudes.\nPrevious accounts postulate a Probability Weighting Function that transforms\nperceived probabilities; however, its motivation has been debated. Recent work\nhas sought to motivate this function in terms of noisy representations of\nprobabilities in the human mind. Here, we present an account of the Probability\nWeighting Function grounded in rational inference over optimal decoding from\nnoisy neural encoding of quantities. We show that our model accurately accounts\nfor behavior in a lottery task and a dot counting task. It further accounts for\nadaptation to a bimodal short-term prior. Taken together, our results provide a\nunifying account grounding the human representation of probability in rational\ninference."}
{"id": "2510.04704", "pdf": "https://arxiv.org/pdf/2510.04704", "abs": "https://arxiv.org/abs/2510.04704", "authors": ["Taoyuze Lv", "Alexander Chen", "Fengyu Xie", "Chu Wu", "Jeffrey Meng", "Dongzhan Zhou", "Bram Hoex", "Zhicheng Zhong", "Tong Xie"], "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these\nabilities can be combined for complex, domain-specific tasks. This question is\nessential in fields like materials science, where deep understanding of 3D\natomic structures is fundamental. While initial studies have successfully\napplied LLMs to tasks involving pure crystal generation or coordinate\nunderstandings, a standardized benchmark to systematically evaluate their core\nreasoning abilities across diverse atomic structures has been notably absent.\nTo address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on\ntasks based in Crystallographic Information Files (CIFs), a standard structure\nrepresentation format. These tasks, including structural editing, CIF\nperception, and property-guided modeling, reveal a critical limitation: current\nmodels, despite establishing promising baselines, consistently fail in\nstructural understanding and spatial reasoning. Our experiments show that these\nmodels make frequent errors on structure modification tasks, and even in the\nbasic CIF format understandings, potentially leading to cumulative errors in\nsubsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows."}
{"id": "2510.04716", "pdf": "https://arxiv.org/pdf/2510.04716", "abs": "https://arxiv.org/abs/2510.04716", "authors": ["Maximilian R. P. von Liechtenstein"], "title": "Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences", "categories": ["cs.LO", "cs.AI", "cs.CC", "quant-ph", "68Q17, 68Q25", "F.1.1; F.2.2; I.2.3"], "comment": "44 pages, 15 figures. Reproducible Colab notebook and params included\n  as ancillary files; all paper figures are generated by the notebook. v1", "summary": "Curved Boolean Logic (CBL) generalizes propositional logic by allowing local\ntruth assignments that do not extend to a single global valuation, analogous to\ncurvature in geometry. We give equivalent sheaf and exclusivity-graph semantics\nand a context-aware proof calculus that is conservative in the flat limit. We\nformalize CBL-SAT and basic complexity (NP-complete in general) and present\noperational operators (CBL-AC and CBL-CONS) that prune contradictions earlier\non classical hardware. We model noise with iid, AR(1)-correlated, and\nadversarial bounded perturbations and provide permutation-based significance\nwith Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files)\nregenerates all figures and statistics. We position CBL relative to KCBS, CSW,\nand sheaf frameworks and outline links to SAT/CSP and robustness/adapter\nstability in large language models."}
{"id": "2510.04738", "pdf": "https://arxiv.org/pdf/2510.04738", "abs": "https://arxiv.org/abs/2510.04738", "authors": ["Baher Mohammad", "Magauiya Zhussip", "Stamatios Lefkimmiatis"], "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and\nSynthesis), a novel autoregressive architecture for text-conditioned voice\nediting and high-fidelity text-to-speech (TTS) synthesis, built on a\ncross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in\nspeech editing and very competitive results in zero-shot TTS, while not being\nexplicitly trained on the latter task, outperforming leading autoregressive and\ndiffusion models on diverse, real-world audio. By integrating Mamba for\nefficient audio sequence modeling with cross-attention for precise\ntext-acoustic alignment, MAVE enables context-aware voice editing with\nexceptional naturalness and speaker consistency. In pairwise human evaluations\non a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%\nof listeners rated MAVE - edited speech as perceptually equal to the original,\nwhile 24.8% prefered the original and 18.0% MAVE - demonstrating that in the\nmajority of cases edits are indistinguishable from the source. MAVE compares\nfavorably with VoiceCraft and FluentSpeech both on pairwise comparisons and\nstandalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE\nexceeds VoiceCraft in both speaker similarity and naturalness, without\nrequiring multiple inference runs or post-processing. Remarkably, these quality\ngains come with a significantly lower memory cost and approximately the same\nlatency: MAVE requires ~6x less memory than VoiceCraft during inference on\nutterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch\nsize 1). Our results demonstrate that MAVE establishes a new standard for\nflexible, high-fidelity voice editing and synthesis through the synergistic\nintegration of structured state-space modeling and cross-modal attention."}
{"id": "2510.04755", "pdf": "https://arxiv.org/pdf/2510.04755", "abs": "https://arxiv.org/abs/2510.04755", "authors": ["Jason Miklian", "Kristian Hoelscher"], "title": "A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Digital technologies are transforming democratic life in conflicting ways.\nThis article bridges two perspectives to unpack these tensions. First, we\npresent an original survey of software developers in Silicon Valley,\ninterrogating how coder worldviews, ethics, and workplace cultures shape the\ndemocratic potential and social impact of the technologies they build. Results\nindicate that while most developers recognize the power of their products to\ninfluence civil liberties and political discourse, they often face ethical\ndilemmas and top-down pressures that can lead to design choices undermining\ndemocratic ideals. Second, we critically investigate these findings in the\ncontext of an emerging new digital divide, not of internet access but of\ninformation quality. We interrogate the survey findings in the context of the\nSlop Economy, in which billions of users unable to pay for high-quality content\nexperience an internet dominated by low-quality, AI-generated ad-driven\ncontent. We find a reinforcing cycle between tech creator beliefs and the\ndigital ecosystems they spawn. We discuss implications for democratic\ngovernance, arguing for more ethically informed design and policy interventions\nto help bridge the digital divide to ensure that technological innovation\nsupports rather than subverts democratic values in the next chapter of the\ndigital age."}
{"id": "2510.04759", "pdf": "https://arxiv.org/pdf/2510.04759", "abs": "https://arxiv.org/abs/2510.04759", "authors": ["Chi Yan", "Dan Xu"], "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://yanchi-3dv.github.io/PG-Occ", "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ"}
{"id": "2510.04760", "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature."}
{"id": "2510.04762", "pdf": "https://arxiv.org/pdf/2510.04762", "abs": "https://arxiv.org/abs/2510.04762", "authors": ["Thorsten Glüsenkamp"], "title": "Fisher-Bingham-like normalizing flows on the sphere", "categories": ["stat.ML", "astro-ph.IM", "cs.AI", "cs.LG"], "comment": null, "summary": "A generic D-dimensional Gaussian can be conditioned or projected onto the D-1\nunit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular\nGaussian (AG) distribution families, respectively. These are some of the most\nfundamental distributions on the sphere, yet cannot straightforwardly be\nwritten as a normalizing flow except in two special cases: the von-Mises Fisher\nin D=3 and the central angular Gaussian in any D. In this paper, we describe\nhow to generalize these special cases to a family of normalizing flows that\nbehave similarly to the full FB or AG family in any D. We call them\n\"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham\ndistribution, their composition allows to gradually add complexity as needed.\nFurthermore, they can naturally handle conditional density estimation with\ntarget distributions that vary by orders of magnitude in scale - a setting that\nis important in astronomical applications but that existing flows often\nstruggle with. A particularly useful member of the new family is the Kent\nanalogue that can cheaply upgrade any flow in this situation to yield better\nperformance."}
{"id": "2510.04769", "pdf": "https://arxiv.org/pdf/2510.04769", "abs": "https://arxiv.org/abs/2510.04769", "authors": ["Michele Caprio", "Siu Lun Chau", "Krikamol Muandet"], "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates", "categories": ["cs.LG", "cs.AI", "math.PR", "math.ST", "stat.ML", "stat.TH", "Primary: 54H25, Secondary: 68T05, 68T37"], "comment": null, "summary": "Many machine learning algorithms rely on iterative updates of uncertainty\nrepresentations, ranging from variational inference and\nexpectation-maximization, to reinforcement learning, continual learning, and\nmulti-agent learning. In the presence of imprecision and ambiguity, credal sets\n-- closed, convex sets of probability distributions -- have emerged as a\npopular framework for representing imprecise probabilistic beliefs. Under such\nimprecision, many learning problems in imprecise probabilistic machine learning\n(IPML) may be viewed as processes involving successive applications of update\nrules on credal sets. This naturally raises the question of whether this\niterative process converges to stable fixed points -- or, more generally, under\nwhat conditions on the updating mechanism such fixed points exist, and whether\nthey can be attained. We provide the first analysis of this problem and\nillustrate our findings using Credal Bayesian Deep Learning as a concrete\nexample. Our work demonstrates that incorporating imprecision into the learning\nprocess not only enriches the representation of uncertainty, but also reveals\nstructural conditions under which stability emerges, thereby offering new\ninsights into the dynamics of iterative learning under imprecision."}
{"id": "2510.04773", "pdf": "https://arxiv.org/pdf/2510.04773", "abs": "https://arxiv.org/abs/2510.04773", "authors": ["Kai Qin", "Jiaqi Wu", "Jianxiang He", "Haoyuan Sun", "Yifei Zhao", "Bin Liang", "Yongzhe Chang", "Tiantian Zhang", "Houde Liu"], "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages", "summary": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned\nfrom vast corpora, concerns regarding data privacy and safety are receiving\nincreasing attention. LLM unlearning, which aims to remove the influence of\nspecific data while preserving overall model utility, is becoming an important\nresearch area. One of the mainstream unlearning classes is optimization-based\nmethods, which achieve forgetting directly through fine-tuning, exemplified by\nNegative Preference Optimization (NPO). However, NPO's effectiveness is limited\nby its inherent lack of explicit positive preference signals. Attempts to\nintroduce such signals by constructing preferred responses often necessitate\ndomain-specific knowledge or well-designed prompts, fundamentally restricting\ntheir generalizability. In this paper, we shift the focus to the\ndistribution-level, directly targeting the next-token probability distribution\ninstead of entire responses, and derive a novel unlearning algorithm termed\n\\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show\nthat the requisite preference distribution pairs for DiPO, which are\ndistributions over the model's output tokens, can be constructed by selectively\namplifying or suppressing the model's high-confidence output logits, thereby\neffectively overcoming NPO's limitations. We theoretically prove the\nconsistency of DiPO's loss function with the desired unlearning direction.\nExtensive experiments demonstrate that DiPO achieves a strong trade-off between\nmodel utility and forget quality. Notably, DiPO attains the highest forget\nquality on the TOFU benchmark, and maintains leading scalability and\nsustainability in utility preservation on the MUSE benchmark."}
{"id": "2510.04774", "pdf": "https://arxiv.org/pdf/2510.04774", "abs": "https://arxiv.org/abs/2510.04774", "authors": ["Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate."}
{"id": "2510.04786", "pdf": "https://arxiv.org/pdf/2510.04786", "abs": "https://arxiv.org/abs/2510.04786", "authors": ["Jonas Hübotter", "Leander Diaz-Bone", "Ido Hakimi", "Andreas Krause", "Moritz Hardt"], "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Humans are good at learning on the job: We learn how to solve the tasks we\nface as we go along. Can a model do the same? We propose an agent that\nassembles a task-specific curriculum, called test-time curriculum (TTC-RL), and\napplies reinforcement learning to continue training the model for its target\ntask. The test-time curriculum avoids time-consuming human curation of datasets\nby automatically selecting the most task-relevant data from a large pool of\navailable training data. Our experiments demonstrate that reinforcement\nlearning on a test-time curriculum consistently improves the model on its\ntarget tasks, across a variety of evaluations and models. Notably, on\nchallenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B\nby approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that\nTTC-RL significantly raises the performance ceiling compared to the initial\nmodel, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to\n43%. Our findings show the potential of test-time curricula in extending the\ntest-time scaling paradigm to continual training on thousands of task-relevant\nexperiences during test-time."}
{"id": "2510.04787", "pdf": "https://arxiv.org/pdf/2510.04787", "abs": "https://arxiv.org/abs/2510.04787", "authors": ["Zifan Song", "Kaitao Song", "Guosheng Hu", "Ding Qi", "Junyao Gao", "Xiaohua Wang", "Dongsheng Li", "Cairong Zhao"], "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading", "categories": ["cs.MA", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Recent advancements in large language models (LLMs) and agentic systems have\nshown exceptional decision-making capabilities, revealing significant potential\nfor autonomic finance. Current financial trading agents predominantly simulate\nanthropomorphic roles that inadvertently introduce emotional biases and rely on\nperipheral information, while being constrained by the necessity for continuous\ninference during deployment. In this paper, we pioneer the harmonization of\nstrategic depth in agents with the mechanical rationality essential for\nquantitative trading. Consequently, we present TiMi (Trade in Minutes), a\nrationality-driven multi-agent system that architecturally decouples strategy\ndevelopment from minute-level deployment. TiMi leverages specialized LLM\ncapabilities of semantic analysis, code programming, and mathematical reasoning\nwithin a comprehensive policy-optimization-deployment chain. Specifically, we\npropose a two-tier analytical paradigm from macro patterns to micro\ncustomization, layered programming design for trading bot implementation, and\nclosed-loop optimization driven by mathematical reflection. Extensive\nevaluations across 200+ trading pairs in stock and cryptocurrency markets\nempirically validate the efficacy of TiMi in stable profitability, action\nefficiency, and risk control under volatile market dynamics."}
{"id": "2510.04797", "pdf": "https://arxiv.org/pdf/2510.04797", "abs": "https://arxiv.org/abs/2510.04797", "authors": ["Qi Li", "Shuwen Qiu", "Julien Han", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kee Kiat Koo", "Karim Bouyarmane"], "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On\n(VTO) technologies, enabling customers to realistically visualize products\noverlaid on their own images. Despite recent advances, existing VTO models face\nchallenges with fine-grained detail preservation, robustness to real-world\nimagery, efficient sampling, image editing capabilities, and generalization\nacross diverse product categories. In this paper, we present DiT-VTON, a novel\nVTO framework that leverages a Diffusion Transformer (DiT), renowned for its\nperformance on text-conditioned image generation, adapted here for the\nimage-conditioned VTO task. We systematically explore multiple DiT\nconfigurations, including in-context token concatenation, channel\nconcatenation, and ControlNet integration, to determine the best setup for VTO\nimage conditioning.\n  To enhance robustness, we train the model on an expanded dataset encompassing\nvaried backgrounds, unstructured references, and non-garment categories,\ndemonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also\nredefines the VTO task beyond garment try-on, offering a versatile Virtual\nTry-All (VTA) solution capable of handling a wide range of product categories\nand supporting advanced image editing functionalities such as pose\npreservation, localized editing, texture transfer, and object-level\ncustomization. Experimental results show that our model surpasses\nstate-of-the-art methods on VITON-HD, achieving superior detail preservation\nand robustness without reliance on additional condition encoders. It also\noutperforms models with VTA and image editing capabilities on a diverse dataset\nspanning thousands of product categories."}
{"id": "2510.04802", "pdf": "https://arxiv.org/pdf/2510.04802", "abs": "https://arxiv.org/abs/2510.04802", "authors": ["Han Zhang", "Lalithkumar Seenivasan", "Jose L. Porras", "Roger D. Soberanis-Mukul", "Hao Ding", "Hongchao Shu", "Benjamin D. Killeen", "Ankita Ghosh", "Lonny Yarmus", "Masaru Ishii", "Angela Christine Argento", "Mathias Unberath"], "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle."}
{"id": "2510.04816", "pdf": "https://arxiv.org/pdf/2510.04816", "abs": "https://arxiv.org/abs/2510.04816", "authors": ["Junhyung Ahn", "Sanghack Lee"], "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference", "categories": ["cs.LG", "cs.AI"], "comment": "This work has been accepted for publication at the IEEE International\n  Conference on Data Mining (ICDM) 2025", "summary": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities."}
{"id": "2510.04837", "pdf": "https://arxiv.org/pdf/2510.04837", "abs": "https://arxiv.org/abs/2510.04837", "authors": ["Guillaume Godin"], "title": "Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 10 figures, 1 table", "summary": "Bond Centered FingerPrint (BCFP) are a complementary, bond-centric\nalternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static\nBCFP that mirrors the bond-convolution used by directed message-passing GNNs\nlike ChemProp, and evaluate it with a fast rapid Random Forest model on\nBrain-Blood Barrier Penetration (BBBP) classification task. Across stratified\ncross-validation, concatenating ECFP with BCFP consistently improves AUROC and\nAUPRC over either descriptor alone, as confirmed by Turkey HSD\nmultiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not\nyield statistically separable gains under the same test. We further propose\nBCFP-Sort&Slice, a simple feature-combination scheme that preserves the\nout-of-vocabulary (OOV) count information native to ECFP count vectors while\nenabling compact unhashed concatenation of BCFP variants. We also outperform\nthe MGTP prediction on our BBBP evaluation, using such composite new features\nbond and atom features. These results show that lightweight, bond-centered\ndescriptors can complement atom-centered circular fingerprints and provide\nstrong, fast baselines for BBBP prediction."}
{"id": "2510.04842", "pdf": "https://arxiv.org/pdf/2510.04842", "abs": "https://arxiv.org/abs/2510.04842", "authors": ["Yorgos Felekis", "Theodoros Damoulas", "Paris Giampouras"], "title": "Distributionally Robust Causal Abstractions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification."}
{"id": "2510.04850", "pdf": "https://arxiv.org/pdf/2510.04850", "abs": "https://arxiv.org/abs/2510.04850", "authors": ["Hengxiang Zhang", "Hyeong Kyu Choi", "Yixuan Li", "Hongxin Wei"], "title": "Detecting Distillation Data from Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning distillation has emerged as an efficient and powerful paradigm for\nenhancing the reasoning capabilities of large language models. However,\nreasoning distillation may inadvertently cause benchmark contamination, where\nevaluation data included in distillation datasets can inflate performance\nmetrics of distilled models. In this work, we formally define the task of\ndistillation data detection, which is uniquely challenging due to the partial\navailability of distillation data. Then, we propose a novel and effective\nmethod Token Probability Deviation (TBD), which leverages the probability\npatterns of the generated output tokens. Our method is motivated by the\nanalysis that distilled models tend to generate near-deterministic tokens for\nseen questions, while producing more low-probability tokens for unseen\nquestions. Our key idea behind TBD is to quantify how far the generated tokens'\nprobabilities deviate from a high reference probability. In effect, our method\nachieves competitive detection performance by producing lower scores for seen\nquestions than for unseen questions. Extensive experiments demonstrate the\neffectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of\n0.470 on the S1 dataset."}
{"id": "2510.04852", "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "categories": ["cs.SE", "cs.AI"], "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization."}
{"id": "2510.04860", "pdf": "https://arxiv.org/pdf/2510.04860", "abs": "https://arxiv.org/abs/2510.04860", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP."}
{"id": "2510.04868", "pdf": "https://arxiv.org/pdf/2510.04868", "abs": "https://arxiv.org/abs/2510.04868", "authors": ["Seyed Soroush Karimi Madahi", "Kenneth Bruninx", "Bert Claessens", "Chris Develder"], "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC."}
{"id": "2510.04871", "pdf": "https://arxiv.org/pdf/2510.04871", "abs": "https://arxiv.org/abs/2510.04871", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Less is More: Recursive Reasoning with Tiny Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters."}
{"id": "2510.04888", "pdf": "https://arxiv.org/pdf/2510.04888", "abs": "https://arxiv.org/abs/2510.04888", "authors": ["Alina Ermilova", "Dmitrii Kornilov", "Sofia Samoilova", "Ekaterina Laptenkova", "Anastasia Kolesnikova", "Ekaterina Podplutova", "Senotrusova Sofya", "Maksim G. Sharaev"], "title": "Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare."}
{"id": "2510.04891", "pdf": "https://arxiv.org/pdf/2510.04891", "abs": "https://arxiv.org/abs/2510.04891", "authors": ["Punya Syon Pandey", "Hai Son Le", "Devansh Bhardwaj", "Rada Mihalcea", "Zhijing Jin"], "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench."}
{"id": "2510.04898", "pdf": "https://arxiv.org/pdf/2510.04898", "abs": "https://arxiv.org/abs/2510.04898", "authors": ["Zheng Xiong", "Kang Li", "Zilin Wang", "Matthew Jackson", "Jakob Foerster", "Shimon Whiteson"], "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA"}
{"id": "2510.04901", "pdf": "https://arxiv.org/pdf/2510.04901", "abs": "https://arxiv.org/abs/2510.04901", "authors": ["Jonathan Colaço Carr", "Qinyi Sun", "Cameron Allen"], "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects", "categories": ["cs.LG", "cs.AI"], "comment": "Reinforcement Learning Journal 2025", "summary": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks."}
{"id": "2510.04910", "pdf": "https://arxiv.org/pdf/2510.04910", "abs": "https://arxiv.org/abs/2510.04910", "authors": ["Jie Yang", "Kexin Zhang", "Guibin Zhang", "Philip S. Yu", "Kaize Ding"], "title": "Glocal Information Bottleneck for Time Series Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB."}
{"id": "2510.04919", "pdf": "https://arxiv.org/pdf/2510.04919", "abs": "https://arxiv.org/abs/2510.04919", "authors": ["Davood Rafiei", "Morgan Lindsay Heisler", "Weiwei Zhang", "Mohammadreza Pourreza", "Yong Zhang"], "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks."}
{"id": "2510.04923", "pdf": "https://arxiv.org/pdf/2510.04923", "abs": "https://arxiv.org/abs/2510.04923", "authors": ["Alec K. Peltekian", "Halil Ertugrul Aktas", "Gorkem Durak", "Kevin Grudzinski", "Bradford C. Bemiss", "Carrie Richardson", "Jane E. Dematte", "G. R. Scott Budinger", "Anthony J. Esposito", "Alexander Misharin", "Alok Choudhary", "Ankit Agrawal", "Ulas Bagci"], "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures, 2 tables", "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications."}
{"id": "2510.04927", "pdf": "https://arxiv.org/pdf/2510.04927", "abs": "https://arxiv.org/abs/2510.04927", "authors": ["Usman Akram", "Yiyue Chen", "Haris Vikalo"], "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions."}
{"id": "2510.04933", "pdf": "https://arxiv.org/pdf/2510.04933", "abs": "https://arxiv.org/abs/2510.04933", "authors": ["Amir Hameed Mir"], "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT", "68T50, 68T07, 62H30", "I.2.7; I.2.6; F.2.2; H.3.3"], "comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at:\n  https://github.com/sirraya-tech/Sirraya_LSD_Code", "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models."}
{"id": "2510.04934", "pdf": "https://arxiv.org/pdf/2510.04934", "abs": "https://arxiv.org/abs/2510.04934", "authors": ["Satvik Dixit", "Soham Deshmukh", "Bhiksha Raj"], "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation", "categories": ["eess.AS", "cs.AI"], "comment": null, "summary": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language\nModels (ALMs), yet assessing open-ended responses remains challenging. Existing\nmetrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from\nNLP and audio captioning, rely on surface similarity and fail to account for\nquestion context, reasoning, and partial correctness. To address the gap in\nliterature, we make three contributions in this work. First, we introduce\nAQEval to enable systematic benchmarking of AQA metrics. It is the first\nbenchmark of its kind, consisting of 10k model responses annotated by multiple\nhumans for their correctness and relevance. Second, we conduct a comprehensive\nanalysis of existing AQA metrics on AQEval, highlighting weak correlation with\nhuman judgment, especially for longer answers. Third, we propose a new metric -\nAURA score, to better evaluate open-ended model responses. On AQEval, AURA\nachieves state-of-the-art correlation with human ratings, significantly\noutperforming all baselines. Through this work, we aim to highlight the\nlimitations of current AQA evaluation methods and motivate better metrics. We\nrelease both the AQEval benchmark and the AURA metric to support future\nresearch in holistic AQA evaluation."}
{"id": "2510.04938", "pdf": "https://arxiv.org/pdf/2510.04938", "abs": "https://arxiv.org/abs/2510.04938", "authors": ["Shiwen Qin", "Alexander Auras", "Shay B. Cohen", "Elliot J. Crowley", "Michael Moeller", "Linus Ericsson", "Jovita Lukasik"], "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Our code is available at: https://github.com/shiwenqin/ONNX-Net", "summary": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly."}
{"id": "2510.04939", "pdf": "https://arxiv.org/pdf/2510.04939", "abs": "https://arxiv.org/abs/2510.04939", "authors": ["Yuxi Liu", "Catherine Lalman", "Yimin Yang"], "title": "Unsupervised Active Learning via Natural Feature Progressive Framework", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under review at IEEE TPAMI", "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage."}
{"id": "2510.04945", "pdf": "https://arxiv.org/pdf/2510.04945", "abs": "https://arxiv.org/abs/2510.04945", "authors": ["Juan-José Guzmán-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Ligia Quintana-Torres", "Martha-Lorena Avendaño-Garrido", "Graham Ranger"], "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 tables, 1 figure", "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required."}
{"id": "2510.04947", "pdf": "https://arxiv.org/pdf/2510.04947", "abs": "https://arxiv.org/abs/2510.04947", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "BIBM2025 accept, 8 pages, 4 figures", "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics."}
{"id": "2510.04950", "pdf": "https://arxiv.org/pdf/2510.04950", "abs": "https://arxiv.org/abs/2510.04950", "authors": ["Om Dobariya", "Akhil Kumar"], "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ME"], "comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations\n  sections; short paper under submission to Findings of ACL 2025", "summary": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction."}
{"id": "2510.04951", "pdf": "https://arxiv.org/pdf/2510.04951", "abs": "https://arxiv.org/abs/2510.04951", "authors": ["Jayanta Mandi", "Marianne Defresne", "Senne Berden", "Tias Guns"], "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When some parameters of a constrained optimization problem (COP) are\nuncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising\ntwo stages -- the prediction of the unknown parameters from contextual\ninformation and the subsequent optimization using those predicted parameters.\nDecision-focused learning (DFL) implements the first stage by training a\nmachine learning (ML) model to optimize the quality of the decisions made using\nthe predicted parameters. When parameters in the constraints of a COP are\npredicted, the predicted parameters can lead to infeasible solutions.\nTherefore, it is important to simultaneously manage both feasibility and\ndecision quality. We develop a DFL framework for predicting constraint\nparameters in a generic COP. While prior works typically assume that the\nunderlying optimization problem is a linear program (LP) or integer linear\nprogram (ILP), our approach makes no such assumption. We derive two novel loss\nfunctions based on maximum likelihood estimation (MLE): the first one penalizes\ninfeasibility (by penalizing when the predicted parameters lead to infeasible\nsolutions), and the second one penalizes suboptimal decisions (by penalizing\nwhen the true optimal solution is infeasible under the predicted parameters).\nWe introduce a single tunable parameter to form a weighted average of the two\nlosses, allowing decision-makers to balance suboptimality and feasibility. We\nexperimentally demonstrate that adjusting this parameter provides a\ndecision-maker the control over the trade-off between the two. Moreover, across\nseveral COP instances, we find that for a single value of the tunable\nparameter, our method matches the performance of the existing baselines on\nsuboptimality and feasibility."}
{"id": "2510.04956", "pdf": "https://arxiv.org/pdf/2510.04956", "abs": "https://arxiv.org/abs/2510.04956", "authors": ["Bi-Cheng Yan", "Ming-Kang Tsai", "Berlin Chen"], "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling", "categories": ["eess.AS", "cs.AI"], "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "summary": "Computer-assisted pronunciation training (CAPT) manages to facilitate\nsecond-language (L2) learners to practice pronunciation skills by offering\ntimely and instructive feedback. To examine pronunciation proficiency from\nmultiple facets, existing methods for CAPT broadly fall into two categories:\nmispronunciation detection and diagnosis (MDD) as well as automatic\npronunciation assessment (APA). The former aims to pinpoint phonetic\npronunciation errors and provide diagnostic feedback, while the latter seeks\ninstead to quantify pronunciation proficiency pertaining to various aspects.\nDespite the natural complementarity between MDD and APA, researchers and\npractitioners, however, often treat them as independent tasks with disparate\nmodeling paradigms. In light of this, we in this paper first introduce MuFFIN,\na Multi-Faceted pronunciation Feedback model with an Interactive hierarchical\nNeural architecture, to jointly address the tasks of MDD and APA. To better\ncapture the nuanced distinctions between phonemes in the feature space, a novel\nphoneme-contrastive ordinal regularization mechanism is then put forward to\noptimize the proposed model to generate more phoneme-discriminative features\nwhile factoring in the ordinality of the aspect scores. In addition, to address\nthe intricate data imbalance problem in MDD, we design a simple yet effective\ntraining objective, which is specifically tailored to perturb the outputs of a\nphoneme classifier with the phoneme-specific variations, so as to better render\nthe distribution of predicted phonemes meanwhile considering their\nmispronunciation characteristics. A series of experiments conducted on the\nSpeechocean762 benchmark dataset demonstrates the efficacy of our method in\nrelation to several cutting-edge baselines, showing state-of-the-art\nperformance on both the APA and MDD tasks."}
{"id": "2510.04966", "pdf": "https://arxiv.org/pdf/2510.04966", "abs": "https://arxiv.org/abs/2510.04966", "authors": ["Anna Chistyakova", "Mikhail Pautov"], "title": "ActiveMark: on watermarking of visual foundation models via massive activations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model."}
{"id": "2510.04970", "pdf": "https://arxiv.org/pdf/2510.04970", "abs": "https://arxiv.org/abs/2510.04970", "authors": ["Marcel Wienöbst", "Leonard Henckel", "Sebastian Weichwald"], "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "comment": null, "summary": "We present FLOP (Fast Learning of Order and Parents), a score-based causal\ndiscovery algorithm for linear models. It pairs fast parent selection with\niterative Cholesky-based score updates, cutting run-times over prior\nalgorithms. This makes it feasible to fully embrace discrete search, enabling\niterated local search with principled order initialization to find graphs with\nscores at or close to the global optimum. The resulting structures are highly\naccurate across benchmarks, with near-perfect recovery in standard settings.\nThis performance calls for revisiting discrete search over graphs as a\nreasonable approach to causal discovery."}
{"id": "2510.04983", "pdf": "https://arxiv.org/pdf/2510.04983", "abs": "https://arxiv.org/abs/2510.04983", "authors": ["Khalid Mehtab Khan", "Anagha Kulkarni"], "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative."}
{"id": "2510.04996", "pdf": "https://arxiv.org/pdf/2510.04996", "abs": "https://arxiv.org/abs/2510.04996", "authors": ["Wei Xiong", "Chenlu Ye", "Baohao Liao", "Hanze Dong", "Xinxing Xu", "Christof Monz", "Jiang Bian", "Nan Jiang", "Tong Zhang"], "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "16 pages, 6 figures", "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada."}
{"id": "2510.04997", "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis."}
{"id": "2510.04999", "pdf": "https://arxiv.org/pdf/2510.04999", "abs": "https://arxiv.org/abs/2510.04999", "authors": ["Nilay Kumar", "Priyansh Bhandari", "G. Maragatham"], "title": "Bridging Text and Video Generation: A Survey", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications."}
{"id": "2510.05003", "pdf": "https://arxiv.org/pdf/2510.05003", "abs": "https://arxiv.org/abs/2510.05003", "authors": ["Imran Mansha"], "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 2 figures. Submitted to arXiv for open access", "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems."}
{"id": "2510.05016", "pdf": "https://arxiv.org/pdf/2510.05016", "abs": "https://arxiv.org/abs/2510.05016", "authors": ["Lucas Carrit Delgado Pinheiro", "Ziru Chen", "Bruno Caixeta Piazza", "Ness Shroff", "Yingbin Liang", "Yuan-Sen Ting", "Huan Sun"], "title": "Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad", "categories": ["astro-ph.IM", "cs.AI", "cs.CL"], "comment": "18 pages, 6 figures, to be submitted, comments are welcome", "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy."}
{"id": "2510.05023", "pdf": "https://arxiv.org/pdf/2510.05023", "abs": "https://arxiv.org/abs/2510.05023", "authors": ["Weixin Wang", "Haoyang Zheng", "Guang Lin", "Wei Deng", "Pan Xu"], "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "39 pages, 3 figures, 2 tables", "summary": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed\nbandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in\neach round to sample from the posterior, relaxing the need for conjugacy\nassumptions between priors and reward distributions in vanilla TS. However,\nthey often require approximating a different posterior distribution in\ndifferent round of the bandit problem. This requires tricky, round-specific\ntuning of hyperparameters such as dynamic learning rates, causing challenges in\nboth theoretical analysis and practical implementation. To alleviate this\nnon-stationarity, we introduce TS-SA, which incorporates stochastic\napproximation (SA) within the TS framework. In each round, TS-SA constructs a\nposterior approximation only using the most recent reward(s), performs a\nLangevin Monte Carlo (LMC) update, and applies an SA step to average noisy\nproposals over time. This can be interpreted as approximating a stationary\nposterior target throughout the entire algorithm, which further yields a fixed\nstep-size, a unified convergence analysis framework, and improved posterior\nestimates through temporal averaging. We establish near-optimal regret bounds\nfor TS-SA, with a simplified and more intuitive theoretical analysis enabled by\ninterpreting the entire algorithm as a simulation of a stationary SGLD process.\nOur empirical results demonstrate that even a single-step Langevin update with\ncertain warm-up outperforms existing methods substantially on bandit tasks."}
{"id": "2510.05025", "pdf": "https://arxiv.org/pdf/2510.05025", "abs": "https://arxiv.org/abs/2510.05025", "authors": ["Kuofeng Gao", "Yiming Li", "Chao Du", "Xin Wang", "Xingjun Ma", "Shu-Tao Xia", "Tianyu Pang"], "title": "Imperceptible Jailbreaking against Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks."}
{"id": "2510.05036", "pdf": "https://arxiv.org/pdf/2510.05036", "abs": "https://arxiv.org/abs/2510.05036", "authors": ["Sergio Rozada", "Vimal K. B.", "Andrea Cavallo", "Antonio G. Marques", "Hadi Jamali-Rad", "Elvin Isufi"], "title": "Graph-Aware Diffusion for Signal Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the problem of generating graph signals from unknown distributions\ndefined over given graphs, relevant to domains such as recommender systems or\nsensor networks. Our approach builds on generative diffusion models, which are\nwell established in vision and graph generation but remain underexplored for\ngraph signals. Existing methods lack generality, either ignoring the graph\nstructure in the forward process or designing graph-aware mechanisms tailored\nto specific domains. We adopt a forward process that incorporates the graph\nthrough the heat equation. Rather than relying on the standard formulation, we\nconsider a time-warped coefficient to mitigate the exponential decay of the\ndrift term, yielding a graph-aware generative diffusion model (GAD). We analyze\nits forward dynamics, proving convergence to a Gaussian Markov random field\nwith covariance parametrized by the graph Laplacian, and interpret the backward\ndynamics as a sequence of graph-signal denoising problems. Finally, we\ndemonstrate the advantages of GAD on synthetic data, real traffic speed\nmeasurements, and a temperature sensor network."}
{"id": "2510.05040", "pdf": "https://arxiv.org/pdf/2510.05040", "abs": "https://arxiv.org/abs/2510.05040", "authors": ["Jihoon Lee", "Hoyeon Moon", "Kevin Zhai", "Arun Kumar Chithanar", "Anit Kumar Sahu", "Soummya Kar", "Chul Lee", "Souradip Chakraborty", "Amrit Singh Bedi"], "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference."}
{"id": "2510.05054", "pdf": "https://arxiv.org/pdf/2510.05054", "abs": "https://arxiv.org/abs/2510.05054", "authors": ["Peter Van Katwyk", "Karianne J. Bergen"], "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model", "categories": ["cs.LG", "cs.AI"], "comment": "Reviewed and published in TMLR at\n  https://openreview.net/forum?id=xRiEdSyVjY", "summary": "Uncertainty quantification is critical for ensuring robustness in high-stakes\nmachine learning applications. We introduce HybridFlow, a modular hybrid\narchitecture that unifies the modeling of aleatoric and epistemic uncertainty\nby combining a Conditional Masked Autoregressive normalizing flow for\nestimating aleatoric uncertainty with a flexible probabilistic predictor for\nepistemic uncertainty. The framework supports integration with any\nprobabilistic model class, allowing users to easily adapt HybridFlow to\nexisting architectures without sacrificing predictive performance. HybridFlow\nimproves upon previous uncertainty quantification frameworks across a range of\nregression tasks, such as depth estimation, a collection of regression\nbenchmarks, and a scientific case study of ice sheet emulation. We also provide\nempirical results of the quantified uncertainty, showing that the uncertainty\nquantified by HybridFlow is calibrated and better aligns with model error than\nexisting methods for quantifying aleatoric and epistemic uncertainty.\nHybridFlow addresses a key challenge in Bayesian deep learning, unifying\naleatoric and epistemic uncertainty modeling in a single robust framework."}
{"id": "2510.05069", "pdf": "https://arxiv.org/pdf/2510.05069", "abs": "https://arxiv.org/abs/2510.05069", "authors": ["Dachuan Shi", "Abedelkadir Asi", "Keying Li", "Xiangchi Yuan", "Leyan Pan", "Wenke Lee", "Wen Xiao"], "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/", "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten."}
{"id": "2510.05077", "pdf": "https://arxiv.org/pdf/2510.05077", "abs": "https://arxiv.org/abs/2510.05077", "authors": ["Chenyu Wang", "Zishen Wan", "Hao Kang", "Emma Chen", "Zhiqiang Xie", "Tushar Krishna", "Vijay Janapa Reddi", "Yilun Du"], "title": "Slm-mux: Orchestrating small language models for reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach."}
{"id": "2510.05081", "pdf": "https://arxiv.org/pdf/2510.05081", "abs": "https://arxiv.org/abs/2510.05081", "authors": ["Ronen Kamenetsky", "Sara Dorfman", "Daniel Garibi", "Roni Paiss", "Or Patashnik", "Daniel Cohen-Or"], "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page at: https://ronen94.github.io/SAEdit/", "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains."}
{"id": "2510.05087", "pdf": "https://arxiv.org/pdf/2510.05087", "abs": "https://arxiv.org/abs/2510.05087", "authors": ["Janos Perczel", "Jin Chow", "Dorottya Demszky"], "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data", "categories": ["cs.CL", "cs.AI"], "comment": "28 pages, 9 figures", "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction."}
{"id": "2510.05090", "pdf": "https://arxiv.org/pdf/2510.05090", "abs": "https://arxiv.org/abs/2510.05090", "authors": ["Runchu Tian", "Junxia Cui", "Xueqiang Xu", "Feng Yao", "Jingbo Shang"], "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures. Work in progress", "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available."}
{"id": "2510.05092", "pdf": "https://arxiv.org/pdf/2510.05092", "abs": "https://arxiv.org/abs/2510.05092", "authors": ["Avichal Goel", "Yoon Kim", "Nir Shavit", "Tony T. Wang"], "title": "Learning to Interpret Weight Differences in Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The weight diffs and DIT adapters trained in the paper can be found\n  at https://huggingface.co/diff-interpretation-tuning/loras", "summary": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions."}
{"id": "2510.05095", "pdf": "https://arxiv.org/pdf/2510.05095", "abs": "https://arxiv.org/abs/2510.05095", "authors": ["Mingkang Zhu", "Xi Chen", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance."}
{"id": "2510.05096", "pdf": "https://arxiv.org/pdf/2510.05096", "abs": "https://arxiv.org/abs/2510.05096", "authors": ["Zeyu Zhu", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Paper2Video: Automatic Video Generation from Scientific Papers", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA", "cs.MM"], "comment": "20 pages, 8 figures", "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video."}
{"id": "2510.05102", "pdf": "https://arxiv.org/pdf/2510.05102", "abs": "https://arxiv.org/abs/2510.05102", "authors": ["Cheng Xin", "Fan Xu", "Xin Ding", "Jie Gao", "Jiaxin Ding"], "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration", "categories": ["cs.LG", "cs.AI", "cs.CG", "math.AT", "stat.ML", "55N31, 68T05, 62R40, 05C, 68R05", "I.2.6; G.2.2; I.5.1"], "comment": "submitted to ICML 2025", "summary": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality."}
